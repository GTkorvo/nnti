> Referee: 1 Comments to the Author 

> I have a few minor comments about the paper, which I hope are taken
> positively.  Overall. this paper is well worth publishing.  A uniform
> interface to the many sparse direct solvers available is a worthwhile goal.
> 
> Minor comments: page 3, you should also cite Gould, Hu, and Scott's comparison
> of different sparse direct solvers for symmetric pos. def. matrices and indef.
> matrices.

Done.

> page 3, re: "It should not be much different in a production code".  I would
> prefer you state "It should not be much different in a non-MATLAB code", or
> something similar.  MATLAB is used in many production settings, and x=A\b also
> has little overhead compared with the sparse solvers.

Done

> page 13, figure 4.  I don't think this figure is useful in its current state.
> What if one solver, for one particular matrix, is twice as slow as the other,
> but the overhead in absolute time is the same?  Then the slower solver has a
> lower % overhead.  That's a little misleading.  You should either just scrap
> the figure, and give a few example data points (discussed in the text), or
> give a table of the absolute times (overhead and solve time) for both UMFPACK
> and SuperLU.

We split the data in two figures: one with SuperLU, showing the dependence of
the matrix size n, and another one for UMFPACK, with respect to the number of
nonzeros nnz. This should avoid (implicit) comparisons between the two solvers
from the reader. Note that we did not report the absolute timings exactly not
to suggest that this solver is better than another one --- the goal of Amesos
is to let users decide on their problems.

> Regarding overhead for the solvers, it would be very interesting to compare
> the overhead with x=A\b in MATLAB.  That comparison would be slightly unfair,
> since MATLAB also takes time to select the appropriate solver (see
> http://www.mathworks.com/access/helpdesk/help/techdoc/ref/mldivide.html ).
> But it would be very interesting, particularly since you state as one of your
> goals the simplicity of x=A\b.  This is an optional suggestion.

Mike, Ken??

> A more important question I have is extensiblity.  How easy is it to extend
> the interface, particularly for exploiting features unique to particular
> solvers?  For example, SuperLU has some powerful equilibration scaling
> methods.  KLU has a permutation to block triangular form.  Each method has
> various ordering strategies.  Can you mix and match?  Suppose I want to use
> SuperLU's scaling, then KLU's permutation to block triangular form, and then
> use your favorite solver on each block (perhaps using one solver for one
> block, and another for a different block, depending on how sparse or how large
> the matrix is)?  How easy would it be to extend Amesos to handle new
> operations that are supported by one, or just a few, of the solvers?  And to
> mix and match where appropriate?  You would end up, perhaps, with more of a
> meta-solver than a solver interface.  I'm not suggesting you add this to the
> code, but it would be very interesting to have a paragraph or two discussing
> how extensible this software architecture is.  You do state on page 4 that
> extensibility is a benefit of abstraction.

Mike, Ken??

-------------------------------------------------------------------------------

> Referee: 2 Comments to the Author 
> 
> The paper describes  an approach to
> designing interfaces for sparse direct solvers; the aim of the interface
> design is to enable application programmers to easily incorporate new software
> and new solution techniques as they become available.  The paper is well
> organized and addresses the relevant issues. The references cover most of the
> direct solvers of which I am aware.  The design presented is actually
> implemented in the Trilinos software available on the Internet, which I
> downladed and played with .  I have a few comments on the subject: 

> 1. The steps of instantiating, creating and finalizing a sparse matrix are
> characteristics of any sparse matrix application; they are largely independent
> of the back-end solver, except with respect to the specific storage format
> that the solver (may) impose, and indeed are also covered by the standard
> efforts of Duff et al which are mainly about iterative solvers.  
> 
> 2. The actual overhead of the package is likely to depend not only on the size
> N but also on the number of nonzeros NNZ. The number of nonzeroes NNZ is
> rightly mentioned in the caption to Fig. 4,  yet nnz does not appear in the
> graph itself; the graph should be augmented/complemented with overhead vs.
> nnz. As a short demonstration, the data set chosen may be sufficient. However
> it would be interesting to know how dependent the overhead is on the platform
> and/or compiler (xl compiler on G4 and gcc/Intel  on the Intel or AMD
> processors immediately come to mind) . Moreover it is relevant in this context
> to know about any possible storage format conversion depending on the
> underlying solver, since they would affect the time and space overhead.  

We replaced the data in the figure with two figures: one contains the
dependency on the matrix size n, and the other on the nonzeros nnz. This also
avoids comparisons between UMFPACK and SuperLU, that are not the goal of this
paper, and should be conducted by the authors of these packages.

About the compilers, we don't have quantitative data on the overhead. However,
Trilinos (and therefore Amesos) is compiled every night during the test
harness on several platform, using Linux, Mac, SGI, SUN, DEC, and other
compilers. In our experience, the compilers have a much greater effect on the
performances of the underlying solvers more than on the interfaces (whose
overhead is quite limited). Therefore, gathering data corresponding to several
compilers might steer the comparison to the solvers --- and we'd like to avoid
this. Comparing solver, although extremely interesting, is not the focus of
our contribution. 

> 3. In my opinion the major problem is one that the authors touch on briefly on
> page 15. viz.  the handling of error conditions; indeed,  in an application
> development context  where the developer is trying out new alternatives the
> availability of a good error handling strategy may be essential in allowing
> rapid prototyping. Could the author expand a little on what they are doing in
> this specific respect?  

An explanation has been added. We agree with the referee that this is one of
the most difficult and important part. 

> 4. The importance of the actual space overhead is dependent on the usage of
> the library. In my opinion the real reason why the storage conversion is not
> often  a first order concern is that most of the space requirements come (most
> of the time) from the fill-in process, which is not (very) dependent on the
> initial storage choice. Whether or not this is acceptable in  a given
> application is something beyond the library control.
> 
> 5. Again with respect to space, the situation mentioned in the paper  in sec.
> 6.1 of using a sparse factorization in a multilevel preconditioner is in some
> ways the best possible scenario, in that the complete factorization is only
> performed on the coarse matrix at the second (last) level of a 2 (multi) level
> scheme, therefore the size is already substantially reduced with respect to
> the size of the original problem. In a context where a solution to the base
> system is sought, the overhead might be much more relevant.

> A few typos: on page 6, "our aim is to standardized"

Done

> on page 15, 4th item in list: "...names usually do not change": surely the
> authors mean that names  DO change?

We clarify the sentence --- at least we hope. The problem is that some
versions of widely used algorithms (colamd, for example) might require
different arguments from one version ot the next, for the _same_ function.
This means that if two solvers use two, incompatible versions, the linker
can only link with one function, and one solver will provide the wrong
parameters. 

> Figure4: There is something strange with the graph, because it is perfectly
> readable on screen using Adobe Acrobat 7.0 under Linux, yet when printed on
> paper the labels and legend turn into total gibberish, at least on my system.

The graph was produced with OmniGraffle, and sometime strange occurs from time
to time. We are trying to understand what's happening...

-----------------------------------------------------------------------------

> Referee: 3 Comments to the Author 

> This paper has great potential, but the interface design and choices need to
> be better explained for it to be recommended for publication.  The first two
> sections provide an excellent overview of the basic interface issues for
> sparse direct solvers;  they promise a clear, consistent, general, and
> easy-to-use interface, but unfortunately the writing in the remainder of the
> paper is not as strong and therefore  it is not clear if these goals are met.
> This manuscript requires a significant rewrite to support these claims.   The
> general idea of trying to present a single interface for several sparse direct
> solvers is a good one, and indeed the proposed design may be a fine solution,
> but there just isn't enough information in the paper to justify that
> conclusion.

> The basic problem is that the description of the interface is incomplete and
> relies too much on other software packages to be meaningful.  There is no
> mention of how one creates vectors and matrices to interact with this
> interface, which is a fundamental challenge.  Most importantly,  there is no
> real example of how this interface is used in practice.  The sole C++ code
> example (Fig. 2) shows only a high level skeleton code that is so abstract, it
> is difficult to extract any meaning.    (In fact, if it didn't say ?umfpack?
> near the middle no one would little idea what this code is doing.)  The code
> also seems quite dependent on other packages (Epetra, Teuchos) that are not
> really described and therefore not clear if the ideas expressed in this
> interface are complete.  In short, the code  may seem familiar to a Trillinos
> expert, but general readers will find it counter-intuitive and difficult to
> understand.   If this is the example used to convince programmers to use this
> interface,  it may not be the most effective.

We replaced the code with a compilable example, now included in the
distribution. The linear system matrix is very simple, only diagonal, because
anything more advanced would be too long to be published. Indeed, the Amesos
distribution contains several working examples, for example to read an
Harwell/Boeing matrix, or to generate matrices. 

> Any complex application can be made to look simple just replacing the code
> with ?init() / solve()? rouitnes that merely move the code complexity
> somewhere.   To demonstrate that   this is not the case here, a more concrete
> description of the underlying specification is needed to let readers see how
> one interacts with such an interface.   In Figure 2, for example, it is not
> clear what the matrix is, or how it was specified, or what options are being
> set to UMFPACK, or how the various phases of  symbolic and numeric
> factorization are being controlled.  Is the matrix symmetric?  Is it complex?
> What is its sparsity pattern?  Do index values begin at 1, or 0?  UMFPACK
> expects compressed column, but the interface support only row-ordered storage,
> so where does the conversion take place, and how it is controlled for
> efficiency?  Is this a parallel code?  (It sure looks like it.)  How is the
> matrix distributed?  What does the sequential version look like?  Is it
> supposed to appear the same?  If no, then how is it different?  And if yes,
> then why does one have to worry about MPI and Maps in sequential codes?
> Also, why is ?umfpack? hard-coded in the source?  If the interface is truly
> general, wouldn't make more sense to specify this at link time?

The new example has been improved following your suggestions. The solver is
specified from the command line (error checks are skipped), so any supported
solver can be used. The index values begin at 0, and a short description has
been added on this subject. The code is both serial and parallel, depending on
the variable HAVE_MPI. For parallel codes, the matrix is distributed, as
specified by Interface 3.3.

>  I would suggest the authors pick a simple example (say a small 5x5  matrix
> and its right-hand side) and show the **complete**  code used to solve this
> linear system using the proposed  C++ interface, including specifying the
> matrix.  This should take about 10 lines or less.   Show it  once using
> UMFPACK as the underlying solver, and once using another library (say LAPACK).
> If the interface is truly general, then these two solution should look very
> similar.  Now compare those codes to the actual C or Fortran counterparts that
> call these libraries directly and show how vastly superior the proposed C++
> interface is.  Repeat the process for distributed matrix (using say UMFPACK
> and ScaLAPACK) and drive the point home for the parallel case.   That would
> make for a compelling argument that the design and C++ interface is truly
> better than explicitly calling these libraries from C.

Since we'd like to have a serial/parallel example running with any number of
processors, we opted for a diagonal matrix. Changing this code for a more
general matrix is not difficult, and referenced to the Trilinos tutorial are
now provided in the text. Umfpack could be replaced by LAPACK or ScaLAPACK at
the command line.

> One point that was rather confusing was the format of the matrix used in the
> linear solver. It was repeatedly stated as completely ?general? without any
> imposed format, but then later described that it must be basically
> row-oriented (or compressed-row) storage (Section 3, p. 7).   This needs to be
> clarified.
> 
> It was repeatedly declared as a completely ?general? format and stated ?we do
> not impose any matrix format.?  (Section 3 , p. 7 )was that the interface did
> not impose any sparse matrix format

We clarified the discussion before Interface 3.3. It is there clearly stated
that any matrix format that provides a quick access to row elements can be
wrapped to satisfy Int 3.3. This requirement is followed by most
high-performance linear algebra packages and matrix formats, and it should be
too restrictive. In our opinion, it is a good compromise between the
generality of the approach and its performances. Note also that the GetRow()
function should be written by the user, so no matrix storage is forced by the
package.


> Minor Editorial Notes:

> (*) Sections (1) and (2):  These sections are well written and provide an
> excellent overview.   In Section 2, though, I'm not sure why the various
> phases of direct solvers are given cryptic mnemonics (S0, S1, and so on).  Why
> not just call the preordering phase  ?Preordering,? the analysis phase
> ?Analysis?, and so on?  These are well-known terms and are much less
> confusing.   The same goes for the A1, A2, A3 tags.

Our goal with these acronyms was to avoid specific terms, since what is
actually done by these phases may change a bit from application to
application, and from solver to solver. 

> (*) Section 3:  Project Design:  the usage of design patterns is interesting,
> but you should describe **why** you are employing this approach.  That is,what
> is the benefit?  Most numerical and scientific programmers are not well-versed
> in design patters, and to them this just make the code look foreign, so a few
> words of justification would be helpful.

Mike, could you add something here and possibly in the text?

> (*) Section 3, Interface 3.1, 3.2. & 3.3:  These sections could written
> better.  Listing the public methods for Map, before describing what this class
> actually does  (p.  6 ? 7) is poor organization.  It would make more sense to
> start with the Vector and Matrix classes and then add Map when discussing
> parallelism.  Also, all of these interfaces do not include any constructors,
> which are critical to the design.  (Minor point:  calling a processor
> distribution a ?Map? in this interface is unfortunate, as ANSI C++ already
> defines a ?map? container as part of the STL.)  Also what is the semantics of
> vectors and matrices? That is, what does the copy constructor for these
> classes do?  Copy by value, or by reference?

As suggested, we changed the description of the Map class, by first describing
what it does, then listing the methods. A footnote is added to warn the reader
that a Map has nothing or few in common with std::map. It is difficult to
describe distributed vectors and matrices without having Map's, and this is
why Map is the first object.

We cannot report constructors, since these classes are pure virtual. A
description of their implementation would be in our opinion outside the main
goal if this paper; so we decided to refer to the Epetra documentation for
more details.

>        (*)  why aren't these classes templated?  In particular, how does one
> specify complex matrices when ?Vector? and ?Matrix? return only pointers to
> doubles?

We added a comment on this subject in Section 3, to expand what said in the
conclusions. It is very easy to write down a general design, and much more
difficult to write the corresponding code. Our approach in this paper was
to describe a design we have already implemented, and this explains why no
support for float, complex or a type T is here given.

>        (*)  are GetValues() and GetMap() the only methods available for Vector
> objects? What about a const version of GetValues()?

We added the const version. 

> (*) Section 3, Interface 3.5: The ?Solver? interface description needs to
> greatly expanded.  ?void SetParamers(List) // specifies all the parameters for
> the solver? yields little insight into how this is actually used.  Some
> examples here would be useful.

Ken, could you add something here?

>        (*) How does one solve A'x = b using this interface?

Ken, could you add something here?

>  (*) Section 6.1, Figure 2: the labels are not readable in the pdf file I
> looked at.  Please check this.

> -----------------------------------------------------------------------------

> Referee: 4 Comments to the Author 
> 
> This article describes an OO interface for easy access to many publically
> available sparse direct solvers. The authors first describe the design
> philosophy and the interface issues, then present a C++ implementation of this
> design. The authors show that the interface incurs little overhead, especially
> for large problems.
> 
> In my opinion, this software framework is valuable for the user community as
> well as for the algorithm research community. A unified interface provides
> users with the capability of using state-of-the-art solvers in an application
> with minimum amount of re-coding. The solver developers can benefit from this
> when they need to compare different algorithms. The article is easily
> readable, and is worth publishing after moderate revision.
> 
> Here are my suggestions for revision.
> 
> 1. The interface implementation uses several "design patterns" developed in
> reference [Gamma et al.].  These mechanisms do not  seem to be in standard
> C++, or any OO language, then you should not assume the readers would know
> this.  You need to describe them   in an understandable way, and explain why
> using them simply your interface implementation.

Mike: Can you comment here?

> 2. P.6 to P.7, "Map" interface seems to be used for both vector and matrix
> interface. Does that mean the distribution of the vectors must be conforming
> with the distribution of the matrix (by rows)? Or at least they have to be
> conforming when setting up the LinearProblem using Inferface 3.4 ? Please
> clarify these.

A note has been added for this important point at page 8.

> 3. Is there any plan to include Fortran wrapper for this?   Interfacing C++
> code with Fortran code is actually not straightforward. Please comment whether
> there is any hurdle in doing that. For example, I see that you use string
> variables in specfifying parameter list. This can be ugly if you want to
> provide a portable Fortran interface.  Why not use enumerate constants?

A comment has been added at page 17. Currently, there are no FORTRAN wrappers
because it is not straighforward to derive pure virtual classes from C++ in
FORTRAN and vice-versa. Most of Trilinos is in C++ (with Python wrappers), and
there are no plans at this point to push the FORTRAN wrappers. Said that, we
agree that it would be great to have FORTRAN interfaces.

> 4. P.5, line 12, "this a solve phase" -> "this phase".
> 
> 5. P.5, line -5, "few reasons" -> "a few reasons".
> 
> 6. P.6, line -15, "to standardized" -> "to standardize".
> 
> 7. P.8, middle, "Interface 3.4 can be easily extended ... reordering". Please
> comment more on this. When performing reordering for a distributed matrix,
> communication is involved. How will you facilitate that?
> 
> 8. P.11, line 4, "provides and abstraction" -> "provides an abstraction".

Done

> 9. P.13, caption of Fig. 4, "nnz" does not appear in the plot, and should be
> removed in the caption.

We changed the figure, so that the UMFPACK data are plotted with respect to
nnz.

> 10. P.15, line -4, "one of our goal" -> "one of our goals".

Done

\documentclass[10pt,relax]{PetraObjectModel}
% ---------------------------------------------------------------------------- %
%
% Set the title, author, and date
%
\title{The Petra Object Model for Parallel Linear Algebra Computations}
\SANDsubtitle{}

\author{Michael A.~Heroux, Robert J. Hoekstra and Alan Williams \\
       Sandia National Laboratories\\
       P.O. Box 5800\\
       Albuquerque, NM 87185-1110
     }

% There is a "Printed" date on the title page of a SAND report, so
% the generic \date should generally be empty.
\date{}


\SANDnum{SAND2005-xxxx} \SANDprintDate{June 2005}
\SANDauthor{Michael A.~Heroux \\
Computational Mathematics and Algorithms Department \\
 \\
Robert Hoekstra \\
Computational Sciences Department \\
 \\
Alan Williams \\
High Performance Computing and Networking Department \\
 \\
Sandia National Laboratories \\
P.O. Box 5800 \\
Albuquerque, NM 87185-1110}



\SANDreleaseType{Unlimited Release}


\begin{document}
\maketitle

\begin{abstract}
This paper describes the design and implementation of a parallel,
distributed memory, object oriented model for basic
matrix and vector services required for the solution of linear,
non-linear and evolutionary problems which arise in scientific
and engineering applications.  We first provide an overview of
the basic object model using the Unified Modeling Language (UML),
and then discuss in detail several implementations.
\end{abstract}


\clearpage
\section*{Acknowledgement}
The authors would like to acknowledge the support of the ASCI and LDRD programs
that funded development of Trilinos.

\newpage
\
\vspace{3.5in}
\begin{center}Intentionally Left Blank\end{center}
\clearpage
\tableofcontents
\listoffigures

\clearpage
\addcontentsline{toc}{section}{Nomenclature}
\section*{Nomenclature}
\begin{itemize}
\item[Processor] A computing element that can execute compiled
code and directly access some physical memory resource.  Typically
a processor is one of the commonly available commodity microprocessors.

\item[Process] An executable file submitted to the operating system.
A process must be assigned to a processor in order to execute.  At
any given time, it may be running on a processor or sitting idle in
memory.

\item[Node] A computing element that has a single, usually
hardware-supported, address space.  In other words, a processor
on a node can directly address any memory location on the node.
A node has one or more processors, all of which have access to the
same shared memory.  Typically a node is a single integrated hardware
unit that, in other customer situations, would be sold as a
stand-alone
system.

\item[Memory Image]  Some portion of memory existing on a single node.
This memory is associated with one or more processes.


\item[Parallel Machine] For our purposes a parallel machine is a
collection of $l$ nodes, where the $i^{th}$ node has $k_i$ processors that have
shared memory.  The total number of processors is
\begin{equation}
p = \sum_{i=0}^{l-1} k_i
\end{equation}

\end{itemize}

\newpage

\section{Introduction}


Many numerical algorithms for the solution of problems in
scientific and engineering applications use vectors and
matrices as basic elements.   Generally, the exact details
of how these mathematical objects are stored, or how computations
are performed using them, is not of primary importance for the
algorithm designer.  Because of this, using abstract object models
that describe the attributes and methods associated with each
object class, and defining the relationship between classes, is an
effective way to separate algorithm design and implementation from
vector and matrix design.

The practical importance of this separation comes from the fact
that there are many different data structures used to store
vectors and matrices and correspondingly many different ways
to implement basic matrix and vector operations.  The variety
of vector matrix data structures and implementations is, to
some extent, a result of arbitrary choices made by application
designers.  However, often there are sound reasons for choosing
particular data structures, either because of the intended
computer architecture or because of the specifics of the
application.  Abstraction of the vector and matrix objects
leverages the investment in sophisticated algorithm implementations
across many different vector and matrix implementations and
provides a convenient generic programming framework for new
algorithm development.

In this paper we present a detailed object model for vectors
and matrices that are intended for use in a parallel, distributed
memory computing environment.  We present a brief overview of
the classes, followed by a UML object model, in the next section.
In subsequent sections we present a collection of pure virtual
classes, two C++ implementations of this model and a Java
implementation.

\section{Sample Use Cases}

To illustrate the types of problems we are addressing and to motivate
our design, we begin with a discussion of several examples.
Specifically, we discuss:
\begin{enumerate}
\item Sparse matrix-vector multiplication,
\item Overlapping Schwarz preconditioning,
\item Migration of matrix and vector data for direct solvers.
\end{enumerate}

\subsection{Sparse Matrix-vector Multiplication}

Sparse matrix-vector multiplication is one of the most important parallel
distributed memory kernel for a broad set of applications.  For
unstructured problems, communication patterns for this kernel are
determined at run-time by the nonzero pattern of the sparse matrix.
For many PDE applications and sparse iterative solver libraries, a
row-based partitioning has been commonly used.  For example,
PETSc~\cite{petsc-home-page,petsc-manual,petsc-efficient} and
Aztec~\cite{Aztec2.1} both use this distribution by default for
general sparse matrices.  Figure~\ref{fig:MatrixPartitions}(a) shows
this partitioning for a 4-by-4 matrix $A$ on two processors.  In this case
the first two rows are stored on the local memory of the first
processor while the last two rows are stored on the second processor.
In contrast, sparse direct
solvers are primarily based on column partitionings as shown in
Figure~\ref{fig:MatrixPartitions}(b), storing the first two columns of
the matrix on the first processor and the last two rows on the second processor.
\begin{figure}
\begin{center}
\begin{equation}
A=\left[
\begin{array}{cccc}
a_{11} & a_{12}&      0 &      0\\
a_{21} & a_{22}& a_{23} &      0\\\hline
     0 &     0 & a_{33} & a_{34}\\
a_{41} &     0 & a_{43} & a_{44}\\
\end{array}
\right]
\hspace{.3in}
A=\left[
\begin{array}{cc|cc}
a_{11} & a_{12}&      0 &      0\\
a_{21} & a_{22}& a_{23} &      0\\
     0 &     0 & a_{33} & a_{34}\\
a_{41} &     0 & a_{43} & a_{44}\\
\end{array}
\right]
\end{equation}
\end{center}
\caption{\label{fig:MatrixPartitions} (a) Row-partitioned matrix $A$ (b)
Column-partitioned matrix $A$.}
\end{figure}

Regardless of how the matrix is partitioned, vectors are typically
partitioned to match the row or column distribution.
Figure~\ref{fig:VectorPartitions} shows how 4-by-1 vectors $x$ and $y$
would typically be stored to be compatible with $A$ in
Figure~\ref{fig:MatrixPartitions}.
\begin{figure}
\begin{center}
\begin{equation}
x=\left[
\begin{array}{l}
x_{1}\\
x_{2}\\\hline
x_{3}\\
x_{4}\\
\end{array}
\right]
\hspace{.8in}
y=\left[
\begin{array}{l}
y_{1}\\
y_{2}\\\hline
y_{3}\\
y_{4}\\
\end{array}
\right]
\end{equation}
\end{center}
\caption{\label{fig:VectorPartitions} Partitioned vectors $x$ and $y$.}
\end{figure}

\subsubsection{Row-oriented multiplication}
Assuming our matrix $A$ is partitioned as in
Figure~\ref{fig:MatrixPartitions}(a), to compute $y=Ax$ each
processor needs to first obtain value of $x$ that are not
local\footnote{The astute reader will notice that we can attempt to
simultaneously compute the local contribution while obtaining
off-processor elements if $x$ as long as asynchronous communication
is supported.  For example, on the first processor, we can compute
the portions of $y_1$ and $y_2$ that depend on $x_1$ and $x_2$.
Practically speaking, the extra complexity of this approach
typically does not improve performance, because local memory
bandwidth is required by both the local computation and
processor-to-processor communication. Since bandwidth is the typical
bottleneck for most sparse matrix computations, there is no
effective speedup.  In fact, we have observed reduced performance on
some computer systems}. We call this type of data transfer an {\it
import} operation, where we know what we want to receive. For our
specific example, the first processor needs $x_3$ and the second
processor needs $x_1$. Once the off-processor $x$ values are
obtained the matrix vector multiplication can proceed without
further communication since elements of $y$ are stored on the same
processors as rows of $A$.

\subsubsection{Column-oriented multiplication}
Assuming our matrix $A$ is partitioned as in
Figure~\ref{fig:MatrixPartitions}(b), we can begin computing $y=Ax$
without any communication.  However, after this step each processor
will have contributions to $y$ that must be sent to to the other
processor for summation.  We call this type of
data transfer an {\it export} operation, where we know what we want to
send.  For our specific example, the first processor must send its
portion of $y_4$ to the second processor and the second processor
must send it portion of $y_2$ to the first processor.  As each
processor receives data from other processors it must sum the partial
results to obtain the final result for $y$.

\framebox{\begin{minipage}[c]{\textwidth}{ {\bf Definitions:} In the
Petra Object Model, an \textit{\textbf{import}} object is used when
a given processor knows the information it wants to receive from one
or more other processors. The complement of import is
\textit{\textbf{export}}, where a processor holds information that
should be sent to another processor. }\end{minipage}}


\subsection{Overlapping Schwarz preconditioning}

Schwarz methods are a family of domain decomposition
methods that introduce coarse-grain parallelism by
partitioning the problem domain into subdomains.  Assuming one or more
subdomains is completely assigned to a processor, on each subdomain
the majority of computation can be performed without communication.
Overlapping Schwarz methods assign parts of subdomains to more that
one processor.  Without overlap, preconditioners based on Schwarz
methods tend to lose robustness as the number of subdomains increases.
With overlap this problem still exists, but is less pronounced.  In
many practical settings, even though overlap introduces redundant
work, there is an overall improvement in performance.

The typical way to implement overlapping Schwarz preconditioners is to
create a submatrix for each
overlapped subdomain.  Given this matrix, we can choose one of many
serial preconditioners and apply it to the submatrix as though it were
the full matrix.  Common submatrix preconditioners are Gauss-Seidel,
Incomplete Cholesky or Incomplete LU.

\subsubsection{Determining the overlap}

Although Overlapping Schwarz methods originated in the context of
PDEs on continuous domains, it is well-known that they can be
applied to sparse matrices that are similar in nature to PDEs.  In
particular, level-based overlap approaches are simple to define.
Consider the row-oriented matrix $A$ in
Figure~\ref{fig:MatrixPartitions}(a). Level-1 overlap is obtained by
determining which columns have non-zero entries on a processor and
then including the corresponding rows in the overlap.  For the first
(last) two rows of our matrix $A$, there is a non-zero entry in
column 3 (1). The level-one overlap matrices are shown in
Figure~\ref{fig:OverlapMatrixPartitions}.
\begin{figure}
\begin{center}
\begin{equation}
\textrm{Overlapped} A_1=\left[
\begin{array}{cccc}
a_{11} & a_{12}&      0 &      0\\
a_{21} & a_{22}& a_{23} &      0\\
     0 &     0 & a_{33} & a_{34}\\
\end{array}
\right] \hspace{.3in} \textrm{Overlapped} A_2 =\left[
\begin{array}{cccc}
a_{11} & a_{12}&      0 &      0\\
     0 &     0 & a_{33} & a_{34}\\
a_{41} &     0 & a_{43} & a_{44}\\
\end{array}
\right]
\end{equation}
\end{center}
\caption{\label{fig:OverlapMatrixPartitions} Overlapped matrix $A$
on (a) first processor and (b) second processor.}
\end{figure}

\subsubsection{Factoring the overlapped preconditioner}

Note that we can view the construction of the overlap matrix as an
import operation.  On each processor, we scan the columns of the
owned rows, looking for entries whose column number has no
corresponding row number on the processor.  The import step then
gathers the off-processor rows, typically ignoring any column
entries that do not correspond to rows in the overlapped matrix.
After the import step, any serial preconditioner can be used as-is,
since all data is local and the matrix is square.


\subsubsection{Applying the overlapped preconditioner}

In order to apply the preconditioner, say $z = M^{-1}r$ as part of a
preconditioned iterative method, the same basic import pattern is
needed to get off-processor entries of $r$ as we used for creating
the overlap matrix.  Similarly, we will have extra values of $z$ on
each processor that must be resolved.  These can be combined as an
average using an export pattern that is the reverse of the import,
or can simply be set to zero if the entry is does not correspond to
an owned row of the original matrix.

\subsection{Migration of matrix and vector data for direct solvers}

Although iterative methods are attractive for distributed memory
parallel execution, frequently a sparse direct solver can be faster,
especially for one and two-dimensional problems and problems where
the matrix is especially sparse.  At the same time, sparse direct
solvers typically do not scale well beyond several tens of
processors.  Therefore, even if an application is running on a large
number of processors, constructing matrices and vectors that are
distributed across all processors, it often makes sense to migrate
the matrix and vectors to fewer processors (even down to one
processor in some instances) for best performance from the direct
sparse solver.  This type of operation can also be viewed as an
import.  In the simplest case, where all data will be gathered onto
a single processor, the import involves gathering all matrix
coefficients from remote processors to one.

Note that this migration can be just as easily viewed as an export,
where all processor will send their portion of the matrix to a
single processor.  We also note that, once the solution is computed
on a single processor, a reverse operation using the same import (or
export) pattern will place the solution values back onto the
processors in the original application distribution.  In this way,
the solver does not even need to know that the data migration
occurred.


\section{Overview of Primary Petra Classes}

In this section we present in logical order the basic object classes
needed for parallel vectors and matrices.  Each of the object
classes represents a family of classes with one base class and
specializations of the base class.

\paragraph{Comm, Distributor and Directory Classes:  Parallel Machine
 Description}

We begin with the Comm class that describes parallel machines. This
class is an attribute in most other classes since it provides
essential information and services needed for operations across the
parallel machine.  The Comm class provides information about the
number of  logical processors on the parallel machine and processor
identification. It also provides basic services such as global
synchronization and collective communications. The Distributor and
Directory classes encapsulate information necessary for performing
import/export operations and referencing of non-local data.

\paragraph{Platform: Factory for Parallel Machine Description classes}

The Platform interface defines a factory for creating instances of the Comm,
Distributor and Directory classes. Separate implementations of the Platform
interface are required for different machines and environments (serial versus
distributed-memory, etc.).

\paragraph{ElementSpace, BlockElementSpace Classes: Object Distribution Specification}

Next we present the ElementSpace and BlockElementSpace classes.  These classes
are a specification of the distribution of a collection of elements, represented
by global identifiers (GIDs). BlockElementSpace is a generalization of
ElementSpace, allowing for elements with more than 1 associated scalar point.
 GIDs are integer values and need not be uniquely owned by a
processor or cover a contiguous range of integer values.  An object is
distributed across the parallel machine by associating subsets of the
object data with the GIDs.  Each processor of the parallel machine will be
assigned zero or more GIDs.  Example 1 illustrates this concept for the
simple case of a distributed vector.   Each ElementSpace object requires a
Platform object to be passed in at construction.

\paragraph{VectorSpace Class: Mathematical Vector-Space Object}

A VectorSpace object provides a set of indices for addressing and
manipulating Vector objects. A VectorSpace can be initialized from
either an ElementSpace or a BlockElementSpace.  A VectorSpace
instance can generate a Vector or MultiVector instance.  VectorSpace
objects are also used to test the compatibility of two distributed
mathematical objects. The Vector, MultiVector, CisGraph, CisMatrix,
Import and Export classes each have one or more VectorSpace objects
as attributes to encode data distribution and to test
compatibilities.

\paragraph{Vector Class:  Mathematical Vector Object}

This class allows construction and manipulation of vectors, and
supports operations involving vectors.  Basic vector operations
include norms, dot products, vector updates, etc.  The interface to
the Vector constructors and Vector methods is independent of the
specifics of the parallel machine and the Vector distribution since
this information is encapsulated in the VectorSpace object passed in
during construction.  Vectors can be generated from a given
VectorSpace object and, once instantiated, a Vector can provide
access to the VectorSpace used to create it.

\paragraph{MultiVector Class:  Generalization of a Dense Matrix}

This class allows the construction and manipulation of a collection
of Vectors that have the same size and distribution, i.e., the same
VectorSpace. A MultiVector is a generalization of a dense matrix.
There are methods in the class that provide results for each Vector
in the MultiVector and methods that view the MultiVector as a
matrix. In a practical implementation, one should note that the
Vector class can usually be implemented as a specialization of the
MultiVector class (a MultiVector with one Vector).

\paragraph{Compressed-Index Graph (CISGraph) Class:  Connectivity Graph Object}

A graph object specifies a set of vertices and the connecting edge
pairs.  It can also be represented as a binary-valued matrix where
entries are one or zero.  A value of one at entry (i,j) in the graph
indicates that a (directed) edge exists from node i to node j in the
graph.  A graph is constructed across a parallel machine by
declaring either a row or column orientation and then associating
each row (column) of the graph with a GID and assigning the entire
row (column) of the graph to the processor(s) that owns the GID. In
the Petra object model, CISGraph objects are held as attributes by
matrix classes, and are used to define and describe the structure of
the matrix (location of nonzero entries).

\paragraph{Operator Class: Linear Operator Object}

The Operator interface is the highest-level abstraction for all
linear operator objects, in particular matrix objects, representing
mathematical operations that require only the \textit{action} of the
operator on a Vector or MultiVector and specifically avoiding issues
relating to storage formats and implementation details. The Operator
interface is inherited by the CISMatrix interface and implemented by
all of our 'concrete' matrix classes, so that any CISMatrix
\textit{isa} Operator and any Petra matrix \textit{isa} Operator.
Algorithms which are written to use these abstract Operators rather
than specific matrix class types may make use of any matrix type
that implements the Operator interface.

\paragraph{Compressed-Index Matrix (CISMatrix) Class: Sparse Matrix Object}

A compressed-index matrix is very similar to a compressed-index
graph. Additionally it contains matrix values and has methods to
compute matrix-vector and matrix-multivector multiplications.  It
has methods to provide certain matrix norms and scalings.  It can
also be queried for matrix entries, which are usually returned a row
at a time or a column at a time. As will be discussed in a later
section, Petra implementations include both point-entry and
block-entry matrix classes.


\paragraph{Import and Export Classes:  Parallel Data Redistribution}

Given a vector, multivector, graph or matrix distributed across processors
via an ElementSpace (call it a source ElementSpace), it is possible to
redistribute any of these
objects by creating a new ElementSpace (a target ElementSpace) and then
creating an import or
an export object to redistribute the object to the layout specified by the
target ElementSpace.

An import object is used in the situation where a target ElementSpace is
 constructed
using GIDs that the calling processor wants to obtain from the source
 ElementSpace
layout.  An export object is used when the target ElementSpace is constructed
so that each GID in the source ElementSpace is uniquely owned by one processor
 in the target ElementSpace, effectively asserting that each calling processor
 wants to export anything
that is in its source ElementSpace but not in its target ElementSpace.
  Import and export
objects form the basis for all data communication on the parallel machine.

\paragraph{Serial Dense Matrix and Vector Classes:  Serial Dense Linear Algebra Objects}

Because there are many good implementations of serial dense linear algebra
operations, e.g. optimized BLAS on many computers, we provide an abstract
class hierarchy to define dense matrix and vector objects.  The default
implementation of these classes uses the standard Fortran BLAS and LAPACK
interfaces~\cite{BLAS1,BLAS2,BLAS3,LAPACK}, but other similar libraries could
be used, including shared memory parallel implementations of the same
libraries.

\paragraph{Linear Problem Class:  Composite Class for Defining a Linear Problem}

One of the most common uses for matrix and vector classes is solving a linear
system of equations, $AX = B$, where $A$ is a known (sparse) matrix, $B$ is a
known set of one or more right hand side vectors and $X$ is the corresponding
set of initial solution guesses (if any).  To facilitate this process, we
define a composite linear problem class that contains an abstract matrix object
(the base class of our compressed-index and block compressed-index matrix
classes), a multivector (which could be a vector since multivector is the base
class for vectors) containing the right hand side and another multivector
containing the initial guess.

\subsection{UML Depiction of Petra Object Model}

The static structure diagrams in figures~\ref{matvecmap} and \ref{distobj}
show inheritance (solid lines) and dependencies or collaborations (dotted
lines) in the Petra object model.
\begin{figure}[ht]
\begin{center}
\epsfig{file=PetraMVM.eps, scale=1.0}
\caption{Relationships among primary Petra classes.}
\label{matvecmap}
\end{center}
\end{figure}
Figure~\ref{matvecmap} shows relationships among the primary classes that users
interact with directly.  This
diagram shows base classes in the Petra object model. In an implementation of
this model, such as the Epetra package, there are also concrete matrix classes,
etc.

Figure~\ref{distobj} shows those aspects of the object model that enable
\begin{figure}[ht]
\begin{center}
\epsfig{file=DistObject.eps, scale=1.0}
\caption{DistObject is the base for objects such as matrix and vector that can be re-distributed by the user.}
\label{distobj}
\end{center}
\end{figure}
parallel data redistribution. Each of the classes that can be redistributed
inherit the DistObject class. DistObject provides the Import() and Export()
methods, and declares other methods that are implemented by derived classes for
packing and unpacking data which is communicated among processors. Parallel
data redistribution will be discussed in more detail in a later section.

\section{Parallel Machine Interfaces and Implementations}

\subsection{The Comm and Related Interface}

\subsubsection{The Comm Interface}
\subsubsection{The Directory Interface}
\subsubsection{The Distributor Interface}

\subsection{Parallel Machine Implementations}
Figure~\ref{commfigure} shows the relationship between the Comm interface and
the implementations for MPI and Serial environments, as well as the dependence
\begin{figure}[ht]
\begin{center}
\epsfig{file=Comm.eps, scale=1.0}
\caption{Comm interface, MpiComm and SerialComm implementations, and data objects.}
\label{commfigure}
\end{center}
\end{figure}
between the implementation classes and their associated reference-counted
data objects.

\subsubsection{Serial Implementation}
\subsubsection{MPI Implementation}
\subsubsection{LBComm Implementation}
\subsubsection{MPI-SMP Implementation}
\subsubsection{Future Implementations}


\section{Index Spaces}
We define a vector to be a list of elements, with each element being a
dense block of one or more real-valued numbers. The set of indices used to
address the elements in a vector is referred to as an index space. Matrices
in general have two index spaces, one for the domain and another for the
range. Matrices and vectors may be used together in operations (such as
matrix-vector products) if they share compatible index spaces. Matrices
and vectors which are distributed across multiple processors must have
distributed index spaces.

\subsection{Defining Index Spaces}
In the Petra object model, index spaces are defined and described using the
ElementSpace class. The ElementSpace class describes not only global properties
such as total
number of elements, but also distribution and partitioning properties such as
number of local elements (local to a particular processor), etc.

There are a number of ways to define a distributed index space using the Petra
ElementSpace. Depending on how the indices are assigned to processors, the
distribution
may be described as uniform or non-uniform, and linear or general.

\subsubsection{Uniform vs Non-Uniform Index Space Distributions}
A uniformly distributed index space has an equal number of indices assigned
to each processor. (In cases where the number of indices is not an integer
multiple of the number of processors, then some processors may have 1 fewer
indices than other processors.) A non-uniformly distributed index space has
arbitrarily varying numbers of indices assigned to each processor at
the discretion of the user.

\subsubsection{Linear vs General Index Space Distributions}
An index space has a linear distribution if indices are distributed across
processors in contiguous ordered groups. In this case, the first global index
on the processor with rank $p$ is one greater than the last global index on
processor $p-1$. An index space with
a general distribution has indices in arbitrary order, as defined by the user.

\subsection{Block Index Spaces}
In cases where the corresponding vector or matrix has block-elements (elements
which consist of more than one point-entry) we describe the index space as a
block index space, and the corresponding Petra class is called a
BlockElementSpace. The BlockElementSpace class is a generalization of the
ElementSpace class (an ElementSpace is equivalent to a
BlockElementSpace with all element-sizes equal to 1). A BlockElementSpace
 allows the user to
specify the element-sizes at construction, and provides query methods for
obtaining element-sizes corresponding to given elements referred to using
global identifiers (GIDs).

\section{Vectors and MultiVectors}
In the Petra object model, the Vector class is defined to have dense,
real-valued entries or elements. Each Vector object has a VectorSpace object
as an attribute, and VectorSpaces can be initialized from either an
ElementSpace or a BlockElementSpace.  Vector objects are distributed across the
parallel machine at construction by passing the VectorSpace into the
 constructor.
The ElementSpace determines the global length of the vector
and the list of GIDs on each processor
determines the segment of the vector stored on the processor.

\subsection{MultiVectors as Distributed Dense Matrices}
A Petra MultiVector is a collection of Vectors. A MultiVector is a
generalization of the Vector class and supports the
same operations as a Vector, such as norms, dot-products etc. A MultiVector
allocates memory for its collection of vectors using a contiguous array which
is identical to a two-dimensional Fortran array. This provides significant
advantages in providing access to selected dense BLAS and LAPACK operations.

\section{Compressed Index Sparse Graphs}
A Petra Graph is used to define the nonzero structure of Petra matrices. The
matrix classes each hold a Graph object as an attribute.
In general, our graphs are directed acyclic graphs (DAGs), so that one must
specify the direction of the connectivity between two vertices.  A graph is
often used to describe the connectivity of a sparse matrix, independent of
the values of the matrix, such that the connectivity graph of the matrix has
a directed edge from vertex $i$ to vertex $j$ if matrix $a_{ij}$ is nonzero.

Once a graph is constructed and filled, it can be used to determine
communication patterns and perform any setup procedures that would improve
the performance of basic sparse matrix operations for matrices that become
associated with the graph.  As part of this setup, the graph will construct
an import ElementSpace and an import object (described below), that allows for
efficient communication during a sparse matrix vector multiplication.

A graph requires an ElementSpace to be passed in at construction time.
In addition to passing in an ElementSpace, one may provide
information about how many nonzero entries will be inserted into each row
of the graph on each processor.  This information can be helpful for
performance, but is not required, nor is it a fatal error if the estimate
is incorrect.  One should note that a row may be owned by more than one node.

\subsection{Row vs Column Orientation}
A row-oriented graph can be thought of as a collection of rows, where each row
contains a list of column-indices. Each row is wholly owned by the local
processor. A row-oriented graph is distributed across processors by assigning
sets of complete rows to the various processors. Correspondingly, a
column-oriented graph is a collection of columns which are lists of row-indices,
and the distribution across processors is accomplished by assigning sets of
columns to the various processors.
A row-oriented graph can also be queried for graph entries a row at a time,
whereas a column-oriented graph is queried a column at a time.

\subsection{Shared and Implicitly Constructed Graphs}
All Petra matrix objects have an attribute which is the Graph object that
describes the nonzero structure of the matrix. Matrix objects may be constructed
without a Graph (construction arguments must still include an ElementSpace to
define the
distribution of the matrix), in which case the matrix will internally create
and define the graph as the user inserts matrix entries. Alternatively, a
predefined graph
may be given to the matrix at construction time, and that will set the
structure to be used for the matrix. In this way matrices with the same
structure may share a single graph object.

\section{Compressed Index Sparse and Block-Sparse Matrices}
Petra matrix implementations include CRS (compressed row sparse) and
VBR (variable block row) matrix classes. Both of these matrix implementations
make use of the above-described graph objects to define structure.
In the case of the VBR matrix, the entries of the sparse
matrix are matrices also, and in particular dense.  We provide this class
to address numerous situations where vertices can be grouped such that
vertices within one group are fully interconnected, or nearly so.  In this
case, it is beneficial to use a smaller graph that merges these fully
interconnected vertices into a single vertex, creating a sparse matrix of
dense matrices.  Most methods in the block-entry matrix class mirror the
methods in the point-entry matrix class.

It is worth mentioning that both of the above matrix classes inherit
a common abstract row-biased matrix interface.  This allows us to use the
abstract base class for operations where the details of the matrix entries
are not of interest, such as matrix-vector multiplication.



\section{Parallel Data Redistribution}
For our purposes, parallel data redistribution concerns the redistribution of
data on a parallel distributed memory computer, where all processors on the
machine are participating in the operation, even though some processors may
send or receive no data.  The data is assumed to be partitioned across the
machine in some form already, including the case where one processor owns all
of the data.


\section{Basic Parallel Data Redistribution Operations}

An essential design issue in the development of distributed memory parallel programs is the
distribution of data across the memory of the computer.  Optimal distribution of data and work
is an often a multiply-constrained optimization problem with the goal of balancing the work and
data distribution across all nodes of the parallel machine and minimizing the cost of
communicating remote data to processors as needed as the computation proceeds.
Except for embarrassingly parallel programs, access to remote data is required even if
the initial distribution of work and data is optimal.  In the single program multiple data
(SPMD) model, accessing remote data typically involves all processors, even if some processors
are not sending or receiving data.  We use the term {\it parallel data
redistribution (PDR)} to refer to the simultaneous exchange of distributed data as it occurs during
the execution of an SPMD program.  The focus of the paper is to develop an object-oriented
model for PDR.  These ideas have been used in the Epetra~\cite{Epetra-Ref-Manual} and
Tpetra~\cite{Tpetra-User-Guide} software packages.

Two of the most common classes of PDR operations are:
\begin{enumerate}
\item Collective operations, such as the dot
product of two distributed vectors, and
\item Sparse all-to-all operations, where each processor will
communicate with some, but typically not all, other processors.
\end{enumerate}


\subsection{Regular pattern and collective operations}

\subsection{Sparse Matrix Vector Multiplication}

One of the most common pattern-dependent distributed memory
kernels is sparse matrix vector multiplication.
This type of kernel appears in many types of applications but
is often implemented assuming a
one-dimensional partitioning of the rows (or columns) of the
matrix such that each row is uniquely and
completely assigned to one processor.  Also, in the square
matrix case, the vectors are usually distributed
with a conformal partitioning.  Figure~\ref{MatrixVectorDistribution}
illustrates this case for a 4-by-4
problem on two processors.  The first (last) two rows of
the matrix and the first (last) two elements of
both vectors are assigned to PE 0 (PE 1).  Given this
distribution, the non-zero pattern of the matrix stored
on each processor determines which elements of $x$ are
required to compute $w$.  On PE 0, there are no
non-zero entries in the third column, so that column is
not present on PE 0, and the third element of $x$
(stored on PE 1) does not need to be sent to PE 0.
Therefore, locally, the fourth
column of the global matrix become the third column on
PE 0, and the fourth global element of $x$ is labeled
as the third elements locally on PE 0, as illustrated
in Figure~\ref{PE1MatrixVectorDistribution}.

On PE1, global

\begin{figure}
\begin{center}
\includegraphics[height=3in]{TwoPESpMV}
\end{center}
\label{MatrixVectorDistribution}
\caption{Matrix and vector assigments on two memory images}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=3in]{PE0SpMV}
\end{center}
\label{PE0MatrixVectorDistribution}
\caption{Contents of memory image on PE 0 after distribution}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=3in]{PE1SpMV}
\end{center}
\label{PE1MatrixVectorDistribution}
\caption{Contents of memory image on PE 1 after distribution}
\end{figure}


\section{PDR Terms and Concepts}
\label{sect:concepts}

\subsection{Distributed Objects}

\subsection{Element Spaces and Global IDs}

Given an existing distribution of data, we want a formal mechanism for describing
how the data should be redistributed.  To accomplish this, we define an
{\it element space} as a collection of labeled elements.  The exact meaning of
an element is determined by the type of object we are redistributing.
The label associated with each element is a signed integer value which we refer to as
the {\it global identifier} or {\it GID} of that element.  If there are no
repeated GIDs associated with an element space, the element space is said to
have the one-to-one property.  For many of the redistribution operations, the
one-to-one property is required for one or both of the element spaces involved
in the redistribution operation.

An element space is itself a distributed object.  An Espace is constructed by
having each processor call the espace constructor, passing in the number of
elements that should be assigned to the processor and a list of related GIDs.

\subsection{Import and Export Operations}

\section{Important PDR Classes}

Figure~\ref{importfigure} shows inheritance and dependence relationships among
several important PDR classes.
\begin{figure}[ht]
\begin{center}
\epsfig{file=ImportExport.eps, scale=1.0}
\caption{Several important PDR classes and interfaces.}
\label{importfigure}
\end{center}
\end{figure}

\subsection{Parallel Machine Class}

\subsection{Element Space Class}

\subsection{Distributed Object Base Class}

\subsection{User Oriented Distributed Object Classes}

\subsection{Import and Export Classes}

\subsubsection{Classification of Global IDs}


\section{Using Imports and Exports for Common PDR Operations}

\subsection{Sparse Matrix Vector Multiplication}

\subsection{Optimal Data Distributions}

\section{Conclusions}


% ---------------------------------------------------------------------- %
% References
%
\clearpage
\bibliographystyle{plain}
\bibliography{../CommonFiles/TrilinosBibliography}
\addcontentsline{toc}{section}{References}


\end{document}

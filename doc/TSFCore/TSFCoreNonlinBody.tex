\section{Introduction}

There is a reasonably large overlap in the requirements and
functionality needed from applications (i.e.~partial differential
equations (PDEs), differential algebraic equations (DAEs), nonlinear
equations (NLEs) etc.) by different types of abstract numerical
algorithms (ANAs) such as iterative linear equation solvers, nonlinear
equation solves (for both single and coupled sets of equations),
stability and bifurcation methods, uncertainty quantification methods
and nonlinear programming solvers for simultaneous analysis and design
(SAND) optimization.  In this paper we propose a set of extensions to
TSFCore \cite{ref:TSFCore}, called TSFCore::Nonlin, for use as the
common interface for the development of nonlinear abstract numerical
algorithms (ANAs) and their integration to applications and linear
algebra libraries.  By agreeing on a simple minimal common interface
layer such as TSFCore::Nonlin, we get around the many-to-many problem
of nonlinear ANA/application interfaces.

It is difficult to describe a set of numerical interfaces outside of
the context of some class of numerical problems.  For this purpose, we
will consider numerical algorithms where it is possible to implement
all of the required operations through only well defined interfaces to
vectors, vector spaces and linear operators.  Here we consider only
the type of functionality as described in
\cite{ref:opt_ctrl_itfc}.

To motivate TSFCore::Nonlin, we present a mathematical formulation in
Section \ref{tsfcorenonlin:sec:math} for a basic set of nonlinear
equations and responses which can be mapped into many different types
numerical problems.  This is followed by an overview of the
TSFCore::Nonlin ANA/application interface in Section
\ref{tsfcorenonlin:sec:TSFCoreNonlin}.
A more detailed discussion of the design of the proposed
TSFCore::Nonlin ANA/application interface is given with examples in
Section \ref{tsfcorenonlin:sec:TSFCoreNonlin_Details}.  Examples of
concrete implementations of the proposed nonlinear ANA/application
interfaces are described in Section
\ref{tsfcorenonlin:sec:example_nps}.  Finally, a simple, but complete,
Newton-based nonlinear equation solver ANA is described in Section
\ref{tsfcorenonlin:sec:example_ana} which solves equations that are
specified by the proposed interfaces.

%
\section{Mathematical Formulation and Requirements}
\label{tsfcorenonlin:sec:math}
%

In this section, we describe a mathematical formulation for many
different classes of nonlinear problems and describe the requirements
for ANAs that solve these problems.

%
\subsection{Mathematical formulation}
\label{tsfcorenonlin:sec:math_form}
%

Before describing the common basic requirements needed by ANAs we
present a generic mathematical formulation of the underlying problem
which is comprised of a set of finite-dimensional nonlinear equations
and a set of response functions:

{\bsinglespace
\begin{eqnarray}
& c(y,\{u_l\})  = 0 & \label{tsfcorenonlin:equ:c} \\
& g_L \leq g(y,\{u_l\}) \leq g_U \label{tsfcorenonlin:equ:g} \\
& y_L \leq y \leq y_U \label{tsfcorenonlin:equ:y_bnds} \\
& \{u_{L,l}\} \leq \{u_l\} \leq \{u_{U,l}\} \label{tsfcorenonlin:equ:u_bnds}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$y \:\in\:\mathcal{Y}$ : Vector of state variables \\
\>	$u_l \:\in\:\mathcal{U}_l$ : Sub-vector $l$ of auxiliary variables, $l = 1 \ldots N_u$ \\
\>	$\{u_l\} = \{u_1,u_2,\ldots,u_{N_u}\}$ : Set of auxiliary variable sub-vectors ($N_u = |\{u_l\}|$) \\
\>	$c(y,\{u_l\}) : \:\mathcal{Y} \times \mathcal{U}_1 \times \mathcal{U}_2 \times \ldots \times \mathcal{U}_{N_u}
                 \rightarrow \mathcal{C}$ : State constraint functions \\
\>	$g(y,\{u_l\}) : \:\mathcal{Y} \times \mathcal{U}_1 \times \mathcal{U}_2 \times \ldots \times \mathcal{U}_{N_u}
                 \rightarrow \mathcal{G}$ : Auxiliary response functions \\
\>	$g_L, g_U \:\in\:\mathcal{G}$ : Lower and upper bounds on response functions $g(y,\{u_l\})$ \\
\>	$y_L, y_U \:\in\:\mathcal{G}$ : Lower and upper bounds on state variables $y$ \\
\>	$u_{L,l}, u_{U,l} \:\in\:\mathcal{U}_l$ : Lower and upper bounds on auxiliary variables $u_l$, $l = 1 \ldots N_u$ \\
\>	$\mathcal{Y} \:\subseteq\:\RE^{n_y}$: Space of the state variables $y$ \\
\>	$\mathcal{U}_l \:\subseteq\:\RE^{n_{u,l}}$: Space of the auxiliary variable sub-vector $u_l$, $l = 1 \ldots N_u$ \\
\>	$\mathcal{C} \:\subseteq\:\RE^{n_y}$: Space of the state constraints $c$ \\
\>	$\mathcal{G} \:\subseteq\:\RE^{n_g}$: Space of the auxiliary responses $g$
\end{tabbing}
\esinglespace}

Equation (\ref{tsfcorenonlin:equ:c}) represents a set of large-scale
nonlinear simulation equations which we refer to as the \textit{state
constraints}.  In this notation, $c(y,\{u_l\})$ is a vector function
where each component $c_j(y,\{u_l\})$, for $j = 1 \ldots n_y$,
represents a nonlinear scalar function of the variables $y$ and
$\{u_l\}$.  The variables $y$ are often called the {\em state
variables}.  These variables are also known as the simulation, or
solution variables.  The set of variables $\{u_l\}$ may represent
different types of variables depending on the type of mathematical
problem being solved.  One could imagine several different categories
of variables in $\{u_l\}$ (where $N_u = |\{u_l\}|$) that would
represent a composite mathematical problem to be solved.  For example,
with $N_u = 3$ one could have a combined optimization problem what
includes embedded stability analysis and uncertainty quantification
(i.e.~using SFE \cite{ref:sfe}) subproblems.  In this example, $u_1$
could represent optimization design (or control or inversion)
variables, $u_2$ could be the continuation variables (in the stability
analysis subproblem), and $u_3$ could be the uncertain variables (in
an SFE method).  In order to be neutral with respect to the specific
mathematical problem being solved, we will call the set of variables
$\{u_l\}$ the {\em auxiliary variables}.  The number of state $y$ and
auxiliary $u_l$ variables are $n_y$ and $n_{u_l}$ respectively.  The
basic assumption is that the size of the state space $n_y =
|\mathcal{Y}| = |\mathcal{C}|$ may be very large (e.g.~$n_y \ge
10^{6}$).  It is also assumed that while the total number of the
auxiliary variables $n_u =
\sum^{N_u}_{l=1} n_{u,l}$ may be very small, it could also be very
large (i.e.~$n_u = O(n_y)$ for some types of optimization problems).

A typical simulation code requires that the user specify the auxiliary
variables $\{u_l\}$ up front and then the square set of equations
$c(y,\{u_l\})=0$ is solved for $y$.  It is this square problem that is
solved by a nonlinear equation solver such as NOX.  An example of such
an nonlinear equation solver is described in Section
\ref{tsfcorenonlin:sec:example_ana}.  This type of nonlinear
elimination of the state equations $c(y,\{u_l\})=0$ to yield the
solution of the state variables $y = y(\{u_l\})$ is the basis of all
so called black-box, or nested analysis and design (NAND)
\cite[Chapter 2]{ref:PDELDRD}, approaches such as are superbly
supported by DAKOTA \cite{ref:eldred_dakota_2001}.

The auxiliary variables $\{u_l\}$ could also represent coupling
variables in a multi-disciplinary analysis (MDA) problem where multiple
sets of state constraints are coupled and solved simultaneously.  The
interfaces described here can be used to support a fully implicit
(i.e.~Newton) solution procedure for sets of coupled MDA equations.
This type of MDA problem is discussed in \cite{ref:mds03,ref:mda_aiaa98}.

In addition to the state constraints in (\ref{tsfcorenonlin:equ:c}), a
set of general {\em response} functions $g(y,\{u_l\})$ is shown in
(\ref{tsfcorenonlin:equ:g}).  This set of response functions is
exactly analogous to the response functions that are modeled in
DAKOTA.  In DAKOTA, the response functions can be interpreted
differently based on the type of problem being solved.  In a
constrained optimization problem, for example, with $n_g = 3$,
$g_1(y,\{u_l\})$ could represent the objective function,
$g_2(y,\{u_l\})$ could represent an auxiliary nonlinear equality
constraint (i.e.~with $(g_L)_{(2)} = (g_U)_{(2)} = 0$) and
$g_3(y,\{u_l\})$ could represent an auxiliary inequality constraint
(i.e.~with $(g_L)_{(3)} < (g_U)_{(3)}$).  As an example application,
consider an aerodynamics simulation problem $c(y,\{u_l\}) = 0$
\cite{AJameson_1988} with the associated response functions of lift
$g_1(y,\{u_l\})$ and drag $g_2(y,\{u_l\})$.  These responses could be
mapped into equality constraints $g_1(y,\{u_l\}) = (g_L)_{(1)} =
(g_U)_{(1)}$ and $g_2(y,\{u_l\}) = (g_L)_{(2)} = (g_U)_{(2)}$ and
solved with the state constraints $c(y,\{u_l\}) = 0$ as one set of
nonlinear equations (here $n_u$ must be $2$ in order to yield a square
set of equations).  Or, if there are more unknowns in $\{u_l\}$ than
responses functions, the nonlinear problem in
(\ref{tsfcorenonlin:equ:c})--(\ref{tsfcorenonlin:equ:u_bnds}) can be
mapped into an optimization problem where the objective could be to
minimize drag $g_2(y,\{u_l\})$ subject to a fixed lift $g_1(y,\{u_l\})
= (g_L)_{(1)} = (g_U)_{(1)}$.  Different subsets of the auxiliary
variables $\{u_l\}$ could be included or excluded in order to come up
with different types of formulations.  Several other permutations of
optimization or other numerical problems can be developed based on
this set of response functions.

The analogy to DAKOTA's handling of response functions was made but
the difference in DAKOTA, however, is that the state equations
$c(y,\{u_l\}) = 0$ and state variables $y$ are first nonlinearly
eliminated (using a nonlinear solver such as NOX) and these variables
and constraints are never seen by DAKOTA.  Only the reduced response
functions are seen (see \cite[Charter 2]{ref:PDELDRD}).

The basic assumption is that while the size of the state space $n_y$
may be very large, the size of the space for the auxiliary response
functions $n_g = |\mathcal{G}|$ will be very small in comparison.  For
example, $n_g$ may only be $O(10)$ or less in every relevant
application that will be considered but there may be some special
exceptions where larger $n_g$ may be manageable.  Because of the large
difference in the sizes of $n_y$ and $n_g$, the functions
$c(y,\{u_l\})$ and $g(y,\{u_l\})$ (and their gradients) are handled in
very different ways as described later.

Here we identify the mathematical entities that are required by an ANA
which are revealed through first-order Taylor expansions.

First, consider the Taylor expansion of $c(y,\{u_l\})$ about
$c(y_k,\{u_l\}_k)$

\begin{equation}
c(y,\{u_l\}) \approx c(y_k,\{u_l\}_k) + \frac{\partial c}{\partial y} \delta y
+ \sum_{l=1}^{N_u} \frac{\partial c}{\partial u_l} \delta u_l
\label{tsfcorenonlin:equ:c_taylor}
\end{equation}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$\frac{\partial c}{\partial y}$ is a square, nonsingular $\RE^{n_y}$-by-$\RE^{n_y}$ Jacobian matrix
	evaluated at $(y_k,\{u_l\}_k)$\\
\>	$\frac{\partial c}{\partial u_l}$ is a rectangular $\RE^{n_y}$-by-$\RE^{n_{u,l}}$ Jacobian matrix
	evaluated at $(y_k,\{u_l\}_k)$.
\end{tabbing}

We use the notation where the Jacobian sub-matrix $\frac{\partial
c}{\partial y}$ is defined element-wise as $\left( \frac{\partial
c}{\partial y} \right)_{(j,l)} = \frac{\partial c_j}{\partial y_l}$ ,
for $j = 1 \ldots n_y$, $l = 1 \ldots n_y$.

In some applications, the matrix $\frac{\partial c}{\partial y}$ is
always nonsingular in regions of interest.  In any case, we will
assume for the remainder of this discussion that for the given
selection of state variables that the matrix $\frac{\partial
c}{\partial y}$ is nonsingular for every point $(y,\{u_l\})$
considered by an ANA (or may be nearly singular for a stability
analysis method at bifurcation points).  The requirement of
nonsingularity of $\frac{\partial c}{\partial y}$ may be relaxed in
some special cases.

Note that if there are no auxiliary variables (i.e.~$N_u = 0$) then
(\ref{tsfcorenonlin:equ:c_taylor}) reduces to the Newton system where
$\frac{\partial c}{\partial y}$ can be inverted to solve for $\delta
y$.  The nonsingularity of $\frac{\partial c}{\partial y}$ is one of
the core building blocks of all of the ANAs being considered here.
Note that applications that do have rank deficient state Jacobians may
also be able to be handled if a partitioning scheme, such as is
described in \cite{ref:moochouserguide}, is used to form a (small)
nonsingular sub-Jacobian object.  In some types of applications this
is not possible however.

Next, consider the Taylor expansion of $g(y,\{u_l\})$ about
$g(y_k,\{u_l\}_k)$

\begin{equation}
g(y,\{u_l\}) \approx g(y_k,\{u_l\}_k) + \frac{\partial g}{\partial y} \delta y
+ \sum_{l=1}^{N_u} \frac{\partial g}{\partial u_l} \delta u_l
\label{tsfcorenonlin:equ:g_taylor}
\end{equation}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$\frac{\partial g}{\partial y}$ is a short, long rectangular $\RE^{n_g}$-by-$\RE^{n_y}$ Jacobian matrix
	evaluated at $(y_k,u_k)$ \\
\>	$\frac{\partial g}{\partial u_l}$ is a small rectangular $\RE^{n_g}$-by-$\RE^{n_{u,l}}$ Jacobian matrix
	evaluated at $(y_k,\{u_l\}_k)$.
\end{tabbing}

Because of the large difference in the size of $n_g$ verses $n_y$, the
Jacobian matrices $\frac{\partial g}{\partial y}$ and
$\left\{\frac{\partial g}{\partial u_l}\right\}$ are handled and
represented very differently from the Jacobian matrices $\frac{\partial
c}{\partial y}$ and $\left\{\frac{\partial c}{\partial u_l}\right\}$.
This will be brought out in the requirements discussed in
section \ref{tsfcorenonlin:sec:requirements}.

%
\subsection{Requirements}
\label{tsfcorenonlin:sec:requirements}
%

The requirements for the basic linear algebra objects identified in
(\ref{tsfcorenonlin:equ:c_taylor}) and
(\ref{tsfcorenonlin:equ:g_taylor}), which are needed in implementing
various ANAs, are shown in Table \ref{tsfcorenonlin:tbl:requirements}.
The most basic set of requirements shown involve the evaluation of the
constraint and auxiliary functions in Requirement
\ref{tsfcorenonlin:req:funcs}.
%
\begin{table}
\begin{center}
\fbox{
\begin{minipage}{\textwidth}
{\bsinglespace
%\hspace{4ex}
\begin{enumerate}
\item\label{tsfcorenonlin:req:funcs}
Function evaluations:
  \begin{enumerate}
  \item\label{tsfcorenonlin:req:c}
  Evaluation of constraint residual: $(y_k,\{u_l\}_k) \rightarrow c \in \mathcal{C}$
  \item\label{tsfcorenonlin:req:g}
  Evaluation of response functions: $(y_k,\{u_l\}_k) \rightarrow g \in \mathcal{G}$
  \end{enumerate}
\item\label{tsfcorenonlin:req:mat-vecs}
Matrix-vector products with gradients evaluated at $(y_k,\{u_l\}_k)$:
  \begin{enumerate}
  \item\label{tsfcorenonlin:req:DcDy}
  $t = \frac{\partial c}{\partial y} p,\;\mbox{[optional]}\;p = \frac{\partial c}{\partial y}^T t,
  \;\mbox{where:}\; t\in\mathcal{C}, \; p\in\mathcal{Y}$
  \item\label{tsfcorenonlin:req:DcDu}
  $t = \frac{\partial c}{\partial u_l} p,\;\mbox{[optional]}\;p = \frac{\partial c}{\partial u_l}^T t,
  \;\mbox{where:}\; t\in\mathcal{C}, \; p\in\mathcal{U}_l$, for $l = 1 \ldots N_u$
  \item\label{tsfcorenonlin:req:DgDy}
  $t = \frac{\partial g_{(j_1:j_2)}}{\partial y} p, \; p = \frac{\partial g_{(j_1:j_2)}}{\partial y}^T t,
  \;\mbox{where:}\; t\in\RE^{j_2-j_1+1}, \; p\in\mathcal{Y}, \; [j_1,j_2] \in [1,n_g]$
  \item\label{tsfcorenonlin:req:DgDu}
  $t = \frac{\partial g_{(j_1:j_2)}}{\partial u_l} p, \; p = \frac{\partial g_{(j_1:j_2)}}{\partial u_l}^T t,
  \;\mbox{where:}\; t\in\RE^{j_2-j_1+1}, \; p\in\mathcal{Y}, \; [j_1,j_2] \in [1,n_g]$
  \end{enumerate}
\item\label{tsfcorenonlin:req:solves}
Solutions of linear systems with $op\left(\frac{\partial c}{\partial y}\right)$ given a definitions of the norms
$||.||_{\mathcal{C}}$ and $||.||_{\mathcal{Y}}$:
  \begin{enumerate}
  \item\label{tsfcorenonlin:req:DcDy_solve}
  Solve for $p$ s.t.~$\frac{||\frac{\partial c}{\partial y} p - r||_{\mathcal{C}}}{O(||r||_{\mathcal{C}})} \le \eta,
  \; \mbox{where:}\; \eta\in\RE, \; p\in\mathcal{Y}, \; r\in\mathcal{C}$
  \item\label{tsfcorenonlin:req:DcDy_solve_adj}
  [optional] Solve for $p$ s.t.~$\frac{||\frac{\partial c}{\partial y}^T p - r||_{\mathcal{Y}}}{O(||r||_{\mathcal{Y}})} \le \eta,
  \; \mbox{where:}\; \eta\in\RE, \; p\in\mathcal{C}, \; r\in\mathcal{Y}$
  \end{enumerate}
\item\label{tsfcorenonlin:req:precond}
[optional] Access to preconditioner $op\left(\tilde{M}\right)$
for $op\left(\frac{\partial c}{\partial y}\right)$
  \begin{enumerate}
  \item\label{tsfcorenonlin:req:precond_solve}
  $p = \tilde{M} r, \; \mbox{where:}\; \eta\in\RE, \; p\in\mathcal{Y}, \; r\in\mathcal{C}$
  \item\label{tsfcorenonlin:req:precond_solve_adj}
  [optional] $p = \tilde{M}^T r, \; \mbox{where:}\; \eta\in\RE, \; p\in\mathcal{C}, \; r\in\mathcal{Y}$
  \end{enumerate}
\end{enumerate}
%\hspace{4ex}
\esinglespace}
\end{minipage}
}%fbox
\end{center}
\caption{
\label{tsfcorenonlin:tbl:requirements}
Requirements for constraint and response functions by nonlinear ANAs.
}
\end{table}
%
All of the requirements in Requirement \ref{tsfcorenonlin:req:mat-vecs} involve
matrix-vector products with the Jacobian matrices $\frac{\partial
c}{\partial y}$, $\left\{\frac{\partial c}{\partial u_l}\right\}$ and
the Jacobian sub-matrices $\frac{\partial g_{(j_1:j_2)}}{\partial y}$,
$\left\{\frac{\partial g_{(j_1:j_2)}}{\partial u_l}\right\}$.  The
Jacobian sub-matrices for the auxiliary functions for contiguous sets
of functions $(j_1:j_2)$ can be accessed independently.  In practice,
each $\frac{\partial g_{j}}{\partial u_l}^T$ will be accessible as a
column vector.  

The requirements for linear solves with the nonsingular state
constraint Jacobian $\frac{\partial c}{\partial y}$ are given in
Requirement \ref{tsfcorenonlin:req:solves}.  Here, the relative solution
tolerance $\eta\in\RE$ is a scalar that is selected by the ANA, while
the definitions of the norms $||.||_{\mathcal{C}}$ and
$||.||_{\mathcal{Y}}$ may be determined by the application.  For fast
convergence in a truncated Newton method for instance, the relative
tolerance $\eta < 1$ will generally be selected to be
$O(||r||_{\mathcal{C}})$ where $r$ in this case will be the residual
$r = c$.  Note that the most natural definition of a norm for
vectors $x\in\mathcal{S}$ on a vector space $\mathcal{S}$ is
$||x||_{\mathcal{S}} = \sqrt{<x,x>_{\mathcal{S}}}$ where
$<x,x>_{\mathcal{S}}$ is the scalar product that defines the vector
space $\mathcal{S}$.  Also, the linear system can be solved to some
default tight tolerance without the ANA having to specifically pick a
relative tolerance $\eta$.  Or, more specialized convergence criteria
can also be selected by the application and/or the ANA (see Section
\ref{tsfcorenonlin:sec:conv_tests}).  Some rare types of ANAs also
perform more efficiently if access to a preconditioner $\tilde{M}$ for
$\frac{\partial c}{\partial y}$ is also granted.  This is shown as
Requirement \ref{tsfcorenonlin:req:precond}.  All of the nuances of
the specification for linear solvers is described in more detail in
Section \ref{tsfcorenonlin:sec:linear_op}.

Note that the requirements on the Jacobian matrices involve both
non-transposed and (optional) transposed operations.  While basic
nonlinear equation solvers generally do not require the transposed (or
adjoint) operations such as in Requirement
\ref{tsfcorenonlin:req:DcDy_solve_adj}, many optimization algorithms do.
However, trust-region methods for nonlinear equations and several
types of iterative methods for linear equations do require the
transposed matrix-vector products.  The requirements in Table 
\ref{tsfcorenonlin:tbl:requirements} without adjoints satisfy the requirements
for {\em direct SAND} (or level-4 \cite{ref:PDELDRD}) optimization
methods and these requirements with adjoints satisfies the
requirements for {\em adjoint SAND} (or level-5 \cite{ref:PDELDRD})
optimization methods.

In the next section, the TSFCore::Nonlin abstractions for the
nonlinear problem specified in
(\ref{tsfcorenonlin:equ:c})--(\ref{tsfcorenonlin:equ:u_bnds}) and the
mathematical objects in Table \ref{tsfcorenonlin:tbl:requirements} are
described.

%
\section{TSFCore::Nonlin Abstractions: Overview}
\label{tsfcorenonlin:sec:TSFCoreNonlin}
%

The basic TSFCore linear algebra interfaces where described in
\cite{ref:TSFCore}.  These base class interfaces are shown in Figure
\ref{tsfcorenonlin:fig:tsfcore}.
%
{\bsinglespace
\begin{figure}[t]
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 3.3in 4.4in,scale=0.40
]{UML1}
%}%fbox
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 6.55in 4.4in,scale=0.70
]{TSFCore}
%}%fbox
\end{center}
\caption{
\label{tsfcorenonlin:fig:tsfcore}
UML class diagram : Core components of the TSF
interface to linear algebra
}
\end{figure}
\esinglespace}
%
Here, we describe object-oriented interfaces for abstracting
nonlinear problems of the form in
(\ref{tsfcorenonlin:equ:c})--(\ref{tsfcorenonlin:equ:u_bnds}) that
extend the core TSFCore interfaces.  Figure
\ref{tsfcorenonlin:fig:NonlinearProblem} shows interfaces for abstracting the
nonlinear problem in (\ref{tsfcorenonlin:equ:c})--(\ref{tsfcorenonlin:equ:u_bnds}).
%
{\bsinglespace
\begin{figure}
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 7.55in 7.0in,angle=0,scale=0.65
]{TSFCoreNonlin}
%}%fbox
\end{center}
\caption{
\label{tsfcorenonlin:fig:NonlinearProblem}
UML \cite{ref:booch_et_al_1999} class diagram : \textbf{TSFCore::Nonlin}:
Interfaces for nonlinear problems for the formulation in
(\ref{tsfcorenonlin:equ:c})--(\ref{tsfcorenonlin:equ:u_bnds}).
}
\end{figure}
\esinglespace}
%
The namespace TSFCore::Nonlin extends the basic linear algebra abstractions
in TSFCore with the addition of two new linear operator interfaces:
\texttt{\textit{Linear\-Solve\-Op}} and \texttt{\textit{Linear\-Op\-With\-Solve}}
The \texttt{\textit{Linear\-Solve\-Op}} interface provides the ability
to solve for linear systems with an operator but not perform
matrix-vector multiplications.  This interface does not represent a
complete mathematical abstraction but instead is an implementation
artifact.  The \texttt{\textit{Linear\-Op\-With\-Solve}} interface
represents a complete abstraction for a linear operator that also has
the ability to be used in the solution of linear systems.

The \texttt{\textit{Linear\-Solve\-Op}} interface allows the client
(i.e.~an ANA algorithm) to carefully specify how tightly a linear
system should be solved and is represented mathematically as

\begin{equation}
\mbox{Compute} \, x, \, \mbox{s.t.} \; \frac{||op(M) \, x - y||}{O(||y||)} \le \eta
\label{tsfcorenonlin:equ:apply_inverse_vec}
\end{equation}

where the norm $||.||$ can be defined by an abstract object of type
\texttt{Norm} and the relative tolerance $\eta$ is a \texttt{Scalar}.
The definition of the relative error represented in
(\ref{tsfcorenonlin:equ:apply_inverse_vec}) may be somewhat solver
dependent and it may be wise for an ANA to not explicitly define the
relative error but instead just a relative tolerance $\eta$ (see
Section \ref{tsfcorenonlin:sec:conv_tests} for an expanded
discussion).  In general, however, the ANA and/or the client can
specify more specialized convergence criteria through an abstract
interface \texttt{\textit{ConvergenceTester}}.  An
\texttt{\textit{ConvergenceTester}} object extracts the state of the
linear solver through the abstract solver interface
\texttt{\textit{Solver\-State}} and then determines if the linear
system(s) have been solved accurately enough. The details of this
approach are discussed in Section \ref{tsfcorenonlin:sec:conv_tests}.  If the
ANA chooses, it can specify the exact meaning of the
\texttt{\textit{Norm}} interface which has a virtual method
\texttt{\textit{norm(x)}} that computes the norm for a vector $x$.
Since the natural norm for a vector space $\mathcal{S}$ is defined as
$||x||_{\mathcal{S}} = \sqrt{<x,x>_{\mathcal{S}}}$, the method
\texttt{Scalar Norm::norm(const Vector\& x)} has a default
implementation based on the method
\texttt{\textit{VectorSpace::scalarProd(...)}}  (which is accessed as
\texttt{x.space()->scalarProd(x,x)}).

In addition to being able to solve single linear systems, a
\texttt{\textit{Linear\-Solve\-Op}} object can also be used to
solve sets of linear systems

\begin{equation}
\mbox{Compute} \, X, \, \mbox{s.t.} \;
\frac{||\frac{1}{\alpha}\,op(M)\,X_{(:,j)} - Y_{(:,j)}||}{O(||Y_{(:,j)}||)} \le \eta_{(j)},
\;\mbox{for}\;j=1 \ldots |domain(Y)|
\label{tsfcorenonlin:equ:apply_inverse_multi_vec}
\end{equation}

where $X$ and $Y$ are \texttt{\textit{MultiVector}} objects, $\alpha$
is a \texttt{Scalar} constant and $\eta$ is an array of
\texttt{Scalar} values for the relative
tolerances for each linear system.  Again, just as in the
\texttt{\textit{Vector}} version, a \texttt{\textit{Norm}} object can
be provided by the ANA.  There is a \texttt{\textit{MultiVector}}
version \texttt{\textit{Norm\-::norms(...)}} (not shown in the figure)
that computes the norm for each column in the multi-vector in a near
optimal manner.  The reason for the inclusion of the scalar multiplier
$\frac{1}{\alpha}$ is that this form of the linear solve is the form
directly supported in the BLAS and LAPACK.  The direct correspondence
to BLAS and LAPACK routines is also the motivation behind the form for
(\ref{tsfcorenonlin:equ:apply_inverse_vec}).  Note that the relative error in
(\ref{tsfcorenonlin:equ:apply_inverse_multi_vec}) is more-or-less invariant to
whether the solver defines the residuals $R_{(:,j)}$ as $R_{(:,j)} =
\frac{1}{\alpha}\,op(M)\,X_{(:,j)} - Y_{(:,j)}$ or $R_{(:,j)} =
op(M)\,X_{(:,j)} - \alpha\,Y_{(:,j)}$ (with $O(\alpha||Y_{(:,j)}||)$ in
the denominator).  This is convenient with respect to several issues.
Just as with the \texttt{\textit{MultiVector}} version of
\texttt{\textit{LinearOp\-::apply(...)}}, the
\texttt{\textit{MultiVector}} version of the
\texttt{\textit{solve(...)}} method also has a default implementation based
on the \texttt{\textit{Vector}} version of
\texttt{\textit{solve(...)}}.  Also, just as with
\texttt{\textit{LinearOp\-::apply(...)}}, adjoints may not be supported.

The specification for the \texttt{\textit{solve(...)}} methods
and optional access to a preconditioner object are designed to address
issues for both direct and iterative solvers in a uniform way and
these issues are discussed in more detail in Section
\ref{tsfcorenonlin:sec:linear_op}.

The interface \texttt{\textit{Linear\-Op\-With\-Solve}} is a specialization
of the \texttt{\textit{LinearOp}} and \texttt{\textit{Linear\-Solve\-Op}}
interfaces that is used to abstract complete nonsingular linear
operators that can be used to solve for linear systems through the
\texttt{\textit{solve(...)}} methods.  The non-singular
Jacobian matrices $\frac{\partial c}{\partial y}$ are represented as
\texttt{\textit{Linear\-Op\-With\-Solve}} objects.
A \texttt{\textit{Linear\-Op\-With\-Solve}} object optionally may also be
able to give access to a preconditioner object (what supports the
interface \texttt{\textit{LinearOp}}), which is shown in Figure
\ref{tsfcorenonlin:fig:NonlinearProblem} as the  association with the role name
\texttt{preconditioner} and multiplicity \texttt{0..1}.  As mentioned
earlier, access to preconditioners is a desired feature for a few types
of specialized nonlinear ANAs.

Before describing the other interfaces it is important to recognize
that in addition to the mathematical requirements shown in Table
\ref{tsfcorenonlin:tbl:requirements}, a major extra requirement is the need to
maintain multiple objects for the Jacobian matrices $\frac{\partial
c}{\partial y}$ and $\frac{\partial c}{\partial u_l}$ which are
represented through the interfaces
\texttt{\textit{Linear\-Op\-With\-Solve}} and
\texttt{\textit{LinearOp}} respectively.  Note that maintaining
multiple \texttt{\textit{Linear\-Op\-With\-Solve}} objects for
$\frac{\partial c}{\partial y}$ also implies the need to maintain
multiple copies of factorizations (for direct linear solvers) and
preconditioners (for iterative linear solvers).  This capability is
needed for certain types of optimization methods (e.g.~multi-period
design \cite{ref:varvarezos_1994}) and SFE.

The interface \texttt{\textit{Nonlinear\-Problem}} defines the basic
problem and allows the computation of zero-order quantities (i.e.~the
calculation of $c(y,\{u_l\})$ and $g(y,\{u_l\})$).  The
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} interface
specialized the \texttt{\textit{Nonlinear\-Problem}} interface by
adding the ability to compute the first-order Jacobian quantities
shown in (\ref{tsfcorenonlin:equ:c_taylor}) and (\ref{tsfcorenonlin:equ:g_taylor}).

The interface \texttt{\textit{Nonlinear\-Problem}} exposes
\texttt{\textit{VectorSpace}} objects
\texttt{space\_y}, \texttt{space\_u(l)} (\texttt{l = 1...Nu}),
\texttt{space\_c} and \texttt{space\_g} which represent the vector
spaces $\mathcal{Y}$, $\mathcal{U}_l$ ($l = 1 \ldots N_u$),
$\mathcal{C}$ and $\mathcal{G}$ respectively.  These objects are
exposed through the methods with the same names and are shown in
Figure \ref{tsfcorenonlin:fig:NonlinearProblem} as the associations from
\texttt{\textit{Nonlinear\-Problem}} to
\texttt{\textit{VectorSpace}}. \texttt{\textit{Vector}} objects are
exposed for the lower and upper bounds and for the initial guesses for
the variables and are shown as attributes in Figure
\ref{tsfcorenonlin:fig:NonlinearProblem}.  The constraint residual
$c(y,\{u_l\})$ and the auxiliary functions $g(y,\{u_l\})$ are computed
using the methods
\texttt{\textit{calc\_c(...)}} and
\texttt{\textit{calc\_g(...)}}  respectively.  These functions
update the  \texttt{c} and \texttt{g} which are first set using
the methods \texttt{\textit{set\_c(...)}} and
\texttt{\textit{set\_g(...)}} respectively.  These vectors that
are set and updated are created using the
\texttt{\textit{VectorSpace}} objects \texttt{space\_c} and
\texttt{space\_g} respectively.

The \texttt{\textit{Nonlinear\-Problem\-First\-Order}} interface allows
the calculation of the Jacobian matrices \texttt{DcDy}
($\frac{\partial c}{\partial y}$), \texttt{DcDu\_l} ($\frac{\partial
c}{\partial u_l}$), \texttt{DgDy} ($\frac{\partial g}{\partial y}$)
and \texttt{DgDu\_l} ($\frac{\partial g}{\partial u_l}$), which are
represented as \texttt{\textit{Linear\-Op\-With\-Solve}},
\texttt{\textit{LinearOp}}, \texttt{\textit{MultiVector}}
and \texttt{\textit{MultiVector}} objects respectively.  The Jacobian
matrix objects are set using the
\texttt{\textit{set\_D\-...(...)}} methods and computed using the
\texttt{\textit{calc\_D\-...(...)}} methods.  The \texttt{\textit{Linear\-Op\-With\-Solve}}
object for \texttt{DcDy} and the \texttt{\textit{LinearOp}} object for
\texttt{DcDu\_l} are created using the exposed
\texttt{\textit{AbstractFactory<>}} objects \texttt{factory\_DcDy} and
\texttt{factory\_DcDu(l)}.  The \texttt{\textit{MultiVector}} objects
for \texttt{DgDy} and \texttt{DgDu\_l} are created using the
\texttt{\textit{VectorSpace}} objects \texttt{space\_y} and \texttt{space\_u(l)}
and the method \texttt{\textit{Vector\-Space\-::create\-Members(...)}}.

The purpose of the separate \texttt{\textit{set\_...(...)}} and
\texttt{\textit{calc\_...(...)}} methods is that, with the proper use,
the subclass can perform calculations of multiple quantities at once
in a transparent way (see Section \ref{tsfcorenonlin:sec:TSFCoreNonlin_Details}).

If adjoints are supported by the objects \texttt{DcDy} and
\texttt{DcDu\_l} then the method \texttt{\textit{adjoint\-Supported()}}
will return \texttt{true}.

The handling of auxiliary variables and response functions is optional
in \texttt{\textit{Nonlinear\-Problem}} and
\texttt{\textit{Nonlinear\-Problem\-First\-Order}}.  In fact, all of the
methods associated with auxiliary variables and response functions
functions have default implementations that reflect this.  This allows
subclasses for simulation-only problems $c(y) = 0$ to be created more
easily (see the example subclass \texttt{NP2DSim} in Section 
\ref{tsfcorenonlin:sec:np2dsim}).

A more detailed discussion of the \texttt{\textit{Nonlinear\-Problem}} and
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} interfaces along with
examples are provided in Section \ref{tsfcorenonlin:sec:TSFCoreNonlin_Details}

%
\section{TSFCore::Nonlin Abstractions: Details and Examples}
\label{tsfcorenonlin:sec:TSFCoreNonlin_Details}
%

In Section \ref{tsfcorenonlin:sec:TSFCoreNonlin}, the basics of object-oriented
interfaces for nonlinear problems of the form in
(\ref{tsfcorenonlin:equ:c})--(\ref{tsfcorenonlin:equ:u_bnds}) were discussed.  In the
following sections, these interfaces are described in more detail and
include numerous code examples.

%
\subsection{\texttt{\textit{Linear\-Solve\-Op}} and \texttt{\textit{LinearOpWithSolve}}}
\label{tsfcorenonlin:sec:linear_op}
%

This section continues the discussion started in Section
\ref{tsfcorenonlin:sec:TSFCoreNonlin} for the \texttt{\textit{Linear\-Solve\-Op}}
and \texttt{\textit{Linear\-Op\-With\-Solve}} interfaces and includes some examples.

%
\subsubsection{\texttt{\textit{LinearSolveOp::solve(...)}}}
\label{tsfcorenonlin:sec:linear_op_apply_inv}
%

The \texttt{\textit{solve(...)}} methods in the
\texttt{\textit{Linear\-Op\-With\-Solve}} interface are designed to
deal with several different types of issues associated with direct and
iterative linear solvers and their use in nonlinear ANAs.
The exact prototype for the \texttt{\textit{Vector}} version of
\texttt{\textit{solve(...)}} is shown below (the prototype of
the \texttt{MultiVector} version is similar):

{\scriptsize\begin{verbatim}
namespace TSFCore {
template<class Scalar>
class LinearSolveOp: virtual public LinearOp<Scalar> {
public:
    ...
    virtual void solve(
        ETransp M_trans, const Vector<Scalar> &y, Vector<Scalar> *x
        ,ConvergenceTester<Scalar>* convTester = NULL
        ) const = 0;
    ...
};
} // namespace TSFCore
\end{verbatim}}

There are three different types of use cases supported by this method.
These use cases, along with the argument values of \texttt{convTester} for
each, are described below.

\begin{enumerate}
\item\label{tsfcorenonlin:lso:full_solves}
Fully solve the linear system(s) to some predefined tight tolerance\\
$\Rightarrow$ Use the default value of \texttt{convTester==NULL}
\item\label{tsfcorenonlin:lso:inexact_solves}
Solve the linear system(s) to a relative tolerance defined by the
ANA\\ $\Rightarrow$ Use \texttt{convTester!=NULL} pointing to a
\texttt{NormedConvergenceTester} object (see Section \ref{tsfcorenonlin:sec:conv_tests})
\item\label{tsfcorenonlin:lso:specialized_solves}
Solve the linear system(s) using some more specialized
application-dependent criteria\\ $\Rightarrow$ Use
\texttt{convTester!=NULL} pointing to a specialized
\texttt{ConvergenceTester} object (see Section \ref{tsfcorenonlin:sec:conv_tests})
\end{enumerate}

The first use case is for algorithms where the theory assumes exact
solves of linear systems (see the ANA in Section
\ref{tsfcorenonlin:sec:example_ana} for example).  Traditionally, when
direct solvers are used, there is little reason to allow for inexact
solves since the cost for solving the linear system is fixed once the
proper factorizations have been formed.  However, even in the case of
direct solvers this approach may expend unnecessary effort in
performing iterative refinement
\cite{ref:golub_van_loan_1996} which may not be necessary until the
later iterations when close to the solution.  This brings us to the
next use case.

The second use case is where the ANA defines a relaxed relative tolerance
$\eta$ that is only tightened down as necessary.  The main
motivation for this use case is to reduce the computational effort to
solve linear systems but to do so in a way that maintains the
theoretical convergence properties of the nonlinear ANA.  Examples of
the theory behind such algorithms are found in
\cite{JEDennis_MHeinkenschloss_LNVicente_1998}.  First, it is
important to note that independent of how the linear systems are
solved, exact matrix-vector products must be performed with the exact
matrices when solving non-transposed or transposed linear systems
respectively in order to insure that conditions similar to
(\ref{tsfcorenonlin:equ:apply_inverse_vec}) or
(\ref{tsfcorenonlin:equ:apply_inverse_multi_vec}) are satisfied.  Even
if matrix-free methods are used to solve the linear systems, the
matrix-vector products can be performed cheaply using algorithmic
(automatic) differentiation in many cases \cite{ref:adolc_1996}.  By
allowing a relaxed solution tolerance, much can be done to make the
solution of the linear system cheaper.  In iterative solvers this
obviously means having to perform less iterations.  In a direct
solver, several approaches can be used make the solution of the linear
systems cheaper.  First, in an iterative nonlinear ANA the Jacobian
matrices may not change much from one iteration to the next.  This may
allow the factorization of a Jacobian matrix computed at one iteration
to be used in several later successive iterations and still be able to
satisfy the conditions like
(\ref{tsfcorenonlin:equ:apply_inverse_vec}) and
(\ref{tsfcorenonlin:equ:apply_inverse_multi_vec}).  Second, the
decision of whether to apply iterative refinement or not, and if so
how many refinement iterations to perform, can also be directed by the
conditions like in (\ref{tsfcorenonlin:equ:apply_inverse_vec}) and
(\ref{tsfcorenonlin:equ:apply_inverse_multi_vec}).

%
\subsubsection{Optional access to preconditioners}
\label{tsfcorenonlin:sec:preconditioners}
%

As mentioned in Section \ref{tsfcorenonlin:sec:TSFCoreNonlin}, the
\texttt{\textit{Linear\-Solve\-Op}} interface also provides
optional access to a preconditioner object which is represented
through the \texttt{\textit{LinearOp}} interface.  The
method \texttt{pre\-conditioner()} returns a
\texttt{RefCountPtr<const LinearOp>} object which may
point to a null object if the \texttt{\textit{Linear\-Solve\-Op}}
object can not return a meaningful preconditioner.

Some types of ANAs can be constructed to be more efficient if access
can be granted to a preconditioner for $\frac{\partial c}{\partial
y}$.  For example, the LNKS method for optimization described in
\cite{GBiros_OGhattas_1_2000} requires access to the state Jacobian's
preconditioner in coming up with a preconditioner for the full KKT
matrix.

From a conceptual mathematical perspective, a preconditioner
$\tilde{M}$ is simply an approximation to the inverse of the exact
operator $M$ where it is assumed that applying the approximate inverse
operator $\tilde{M}$ is cheaper that solving linear systems with
the exact operator $M$ and that $\kappa(\tilde{M} M)
\le \kappa(M)$, where $\kappa(.)$ is a condition number in some norm.
While the concept of an approximate inverse operator is common to
iterative linear solvers this concept may not seem, at first glance,
to apply to direct linear solvers.  However, when iterative refinement
and incomplete factorizations are considered, the line between direct
and iterative methods becomes more blurred.  Therefore, by taking this
perspective, in the case of a direct linear solver the preconditioner
returned should be a \texttt{\textit{LinearOp}} object that represents
a solve with the original (possibly incomplete) factorization of the
original operator $M$.  This gives the ideal behavior in the type of
use cases such as described above (i.e.~LNKS).  Of course, when the
underlying implementation is based on a traditional iterative method
\cite{ref:tmpls_for_iter_systems}, the actual preconditioner object
that is used in the iterative method should be returned; and for an
unpreconditioned iterative method, no preconditioner object should be
returned (i.e.~a null object).

%
\subsubsection{Convergence tests for linear solves}
\label{tsfcorenonlin:sec:conv_tests}
%

An ANA can specify any arbitrary type of convergence criteria for the
solution of linear systems of the form
(\ref{tsfcorenonlin:equ:apply_inverse_vec}) and
(\ref{tsfcorenonlin:equ:apply_inverse_multi_vec}).  Figure
\ref{tsfcorenonlin:fig:ConvergenceTester} shows a UML class diagram for the basics
of the design.  The way that this works is that all linear solvers
expose an interface called
\texttt{\textit{Solver\-State}} which represents the state of a
(block) linear solver and then an ANA can specify an arbitrary
convergence test through the interface \texttt{\textit{ConvergenceTester}}
with abstracts all types of convergence testers.  The
\texttt{\textit{Solver\-State}} interface is designed for only one
purpose -- to provide information needed for a convergence check.
This base interface supports only basic information that all linear
solvers (both iterative and direct) can provide.  The methods on
\texttt{\textit{Solver\-State}} only make since during a solve and
not at any other time.  These abstractions are designed to work for
general block linear solvers (both iterative and direct) and are
designed to take into account compression (i.e.~linear systems that are
made ``inactive'' after they have been determined to be solved and are
removed from the ``active'' set).  These interfaces are designed to
put more of a burden on the
\texttt{\textit{Solver\-State}} interface and less on the
\texttt{\textit{ConvergenceTester}} interface so that developing concrete
\texttt{\textit{ConvergenceTester}} subclasses is as easy as possible
(since the ANA and/or user has to develop
\texttt{\textit{ConvergenceTester}} subclasses).

{\bsinglespace
\begin{figure}[t]
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 8.55in 3.6in,angle=0,scale=0.70
]{TSFCoreSolvers}
%}%fbox
\end{center}
\caption{
\label{tsfcorenonlin:fig:ConvergenceTester}
UML class diagram : Generalized convergence testing for (block) linear solves.
}
\end{figure}
\esinglespace}

This design is best described using an example scenario such as the
one shown in Figure \ref{tsfcorenonlin:fig:ConvergenceTesterSeqNormed}.  This
scenario uses a normed-based convergence tester subclass called
\texttt{NormedConvergenceTester} which is applicable to both direct and
iterative solvers and relies on just the base
\texttt{\textit{Solver\-State}} interface.  The mechanisms of
this design will be made clear and are easily generalized to other
types of more specialized linear solver and convergence tester
subclasses.  This scenario involves a use case where a nonlinear ANA
initiates the solve of a set of 10 linear systems which is implemented
using a BiCG iterative linear solver object (of type
\texttt{BiCG\-Solver}, see \cite{ref:TSFCore}, named \texttt{solver}).

{\bsinglespace
\begin{figure}[t]
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 7.0in 5.15in,angle=0,scale=0.70
]{ConvergenceTesterSeqNormed}
%}%fbox
\end{center}
\caption{
\label{tsfcorenonlin:fig:ConvergenceTesterSeqNormed}
UML sequence diagram : Example scenario of using a normed convergence test with a block BiCG solver
}
\end{figure}
\esinglespace}

To begin the scenario, the ANA first initializes a
\texttt{NormedConvergenceTester} object called \texttt{ct}, passing in an
array of relative tolerances
\texttt{tols[]} of dimension \texttt{totalNumSystems=10}.
Following the initialization of the \texttt{ct} object, \texttt{ct} is
then passed to the \texttt{solver} object through its
\texttt{\textit{solve(...)}} method.  Upon entering the the
\texttt{\textit{solve(...)}} method, the \texttt{solver} object
extracts the definition of norm in the form of a
\texttt{\textit{Norm}} object \texttt{norm} using the method call
\texttt{ct.norm()}.  This \texttt{norm} object is used to compute
norms that are returned to the \texttt{ct} object later.

In the next step in the scenario, the \texttt{solver} object performs
the first iteration (\texttt{do\-Iteration(\-...)}) of the block BiCG method
and then calls the \texttt{\textit{convStatus(...)}} method on the
\texttt{ct} object.  The method \texttt{\textit{convStatus(...)}}
takes as its first input parameter a reference to the \texttt{solver}
object itself (\texttt{*this}) and as the second input parameter
\texttt{currNumSystems} (called
\texttt{m} for short in Figure
\ref{tsfcorenonlin:fig:ConvergenceTesterSeqNormed}) which gives the current number
of active linear systems.  For this first call, all 10 the linear
systems are active (\texttt{m=10} in this case obviously).  Once
control has been given over to the \texttt{ct} object it is given free
reign to access all of the public member functions on the
\texttt{solver} object (through its \texttt{\textit{Solver\-State}}
interface, or a derived interface).  In this case, \texttt{ct} is an
\texttt{NormedConvergenceTester} object which only considers relative
norms of the residuals in determining convergence.  The \texttt{ct}
object only compares the relative tolerances in \texttt{ct.tols[]} to
solver-defined estimates of the relative error in the residuals of the
linear system.  In order to interpret these relative errors, the method
\texttt{\textit{currActiveSystems(...)}} is first called back on the
\texttt{solver} object which returns an array \texttt{activeSystems[1..m]}
which gives the indexes of the currently ``active'' linear systems.
At this stage, all of the linear systems are active so
\texttt{activeSystems[j-1] == j}, for
\texttt{j=1..m}.  The next step is to request estimates of the
relative errors in the residuals of the current active linear systems
which is performed through a call back to the method
\texttt{\textit{curr\-Est\-Rel\-Residual\-Norms(...)}} on the \texttt{solver} object
which returns an array \texttt{norms[1..m]}.  The specification of the
\texttt{\textit{curr\-Est\-Rel\-Residual\-Norms(...)}}  method allows the
\texttt{solver} object great flexibility in determining how the
residual is defined (e.g.~scaled and/or transformed by the
preconditioner in an iterative solver and how the
relative errors are computed (i.e.~the exact form of the denominators
in (\ref{tsfcorenonlin:equ:apply_inverse_vec}) and
(\ref{tsfcorenonlin:equ:apply_inverse_multi_vec})).  The concepts
behind the \texttt{\textit{curr\-Est\-Rel\-Residual\-Norms(...)}}
method are discussed in more detail starting on page
\pageref{tsfcorenonlin:sec:solver_defined_rel_err}.  This \texttt{ct}
object then sets the convergence status of each linear system as
\texttt{isConverged[k] = (norms[k] <= tols[ active\-Systems[k]-1 ])},
for \texttt{k=0...m-1}.  The array \texttt{isConverged[1..m]} is then
returned from the \texttt{\textit{convStatus(...)}} method back to the
\texttt{solver} object.

Once the \texttt{\textit{convStatus(...)}} method returns, the
\texttt{solver} object then performs a compression (\texttt{compress(...)})
where linear systems \texttt{j} (where \texttt{isConverged[k] == true
\&\& activeSystems[k] == j}) are removed from the active-set.  The
removal of systems that are solved helps make the overall solution
process more efficient and helps to avoid numerical problems of
over convergence.

This process of performing an iteration, calling the
\texttt{\textit{convStatus(...)}} method on the \texttt{ct}
object and compression is continued until all of the linear systems are
solved (as determined by the \texttt{ct} object).  The last 
iteration of the block BiCG method is shown in Figure 
\ref{tsfcorenonlin:fig:ConvergenceTesterSeqNormed} where only one (\texttt{m=1})
active linear system remains.  In this last call to
\texttt{\textit{convStatus(...)}}, \texttt{isConverged[0]==true}
is returned and then the \texttt{solver} object assembles the final
(unscaled) solution \texttt{\textit{MultiVector}} object \texttt{X}.
The solution \texttt{X} is then returned from the
\texttt{\textit{solve(...)}} method back to the nonlinear ANA client.

A few issues and variations on the basic scenario shown in Figure
\ref{tsfcorenonlin:fig:ConvergenceTesterSeqNormed} are discussed next.

%
\subsubsection*{Specialized solvers and convergence tests}
%

\texttt{\textit{ConvergenceTester}} subclasses that need more information
than what is presented in the \texttt{\textit{Solver\-State}}
interface must perform dynamic casting (i.e.~downcasting) to more
specific interfaces and therefore become less general.  For example,
suppose that there is some type of very specialized linear solver
subclass named \texttt{My\-Specialized\-Eqn\-Solver} as shown in Figure
\ref{tsfcorenonlin:fig:ConvergenceTesterSeqNormed}.  This solver
may have some very specialized properties that can be exploited to
come up with very effective convergence testing criteria.  This
specialized convergence criteria would be implemented in a subclass
such as \texttt{My\-Specialized\-Convergence\-Tester}, also shown in
Figure \ref{tsfcorenonlin:fig:ConvergenceTesterSeqNormed}.  A
\texttt{My\-Specialized\-Convergence\-Tester} object would access the
full state of a \texttt{My\-Specialized\-Eqn\-Solver} object as shown below.

{\scriptsize\begin{verbatim}
void TSFCore::MySpecializedConvergenceTester::convStatus( const EqnSolverBase& solver, Index currNumSystems
                                                          ,bool isConverged[] )
{
    const MySpecializedEqnSolver *specialized_solver = dynamic_cast<const MySpecializedEqnSolver*>(&solver);
    if( !specialized_solver ) {
        // Error! this is not the solver type that I was expecting.  Do something else?
        ...
    }
    else {
        // Great! Now we can extract all the info we need through the MySpecializedEqnSolver interface!
        ...
    }
}
\end{verbatim}}

This type of specialized \texttt{\textit{ConvergenceTester}} object could
be supplied to a nonlinear ANA by the application is some manner along
with the specialized linear solver.

%
\subsubsection*{Solver-defined relative error}
\label{tsfcorenonlin:sec:solver_defined_rel_err}
%

The last issue to discuss with respect to the design shown in Figure
\ref{tsfcorenonlin:fig:ConvergenceTester} and described in the scenario shown in
Figure \ref{tsfcorenonlin:fig:ConvergenceTesterSeqNormed} is the specification of
the method
\texttt{\textit{SolverState\-::curr\-Est\-Rel\-Residual\-Norms(...)}}.  The
specification of this method allows the solver to define any type of
estimate of the relative error which is more or less similar to the
relative error defined in (\ref{tsfcorenonlin:equ:apply_inverse_vec}) and
(\ref{tsfcorenonlin:equ:apply_inverse_multi_vec}).  The reason that this method
does not strictly define the exact form of this relative error is that
to do so may require a lot of extra computation on the part of the
linear solver.  For example, in a preconditioned iterative linear
solver, the preconditioned residual

\[
R = \tilde{M}^{-1} \left( \frac{1}{\alpha}\,op(M)\,X - Y \right)
\]

or

\[
R = \tilde{M}^{-1} \left( op(M)\,X - \alpha\,Y \right)
\]

may be readily available, but not the unpreconditioned residual

\[
R = \frac{1}{\alpha}\,op(M)\,X - Y.
\]

From the standpoint of some nonlinear ANAs, the exact form of the
relative error is not important.  What is important is that the
residual of the linear system be reduced by some relative amount in
order to achieve global convergence of the nonlinear ANA method.  For
example, a preconditioned iterative linear solver might choose to
define the normed relative error in the $j^{\scriptsize\mbox{th}}$
linear system as

\begin{equation}
\mbox{\texttt{norm[j-1]}} = \frac{ || \tilde{M}^{-1} \left( op(M)\,X_{(:,j)} - \alpha\,Y_{(:,j)} ) \right) || }
{ |\alpha|\, ||\tilde{M}^{-1}\,Y_{(:,j)}|| + constant}
\label{tsfcorenonlin:equ:relative_error_1}
\end{equation}

where $||.||$ is defined by the \texttt{Norm} object that is returned by the
\texttt{\textit{ConvergenceTester\-::norm()}} method.  Note the form of the denominator
in (\ref{tsfcorenonlin:equ:relative_error_1}) is consistent with the
definition of the residual in the numerator.  The practice of adding a
constant (such as one) to the denominator is a common to avoid
numerical problems when the right-hand-side becomes very small but
there are better ways to deal with these issues.

%
\subsection{\texttt{\textit{NonlinearProblem}} $\:$ and
\texttt{\textit{NonlinearProblemFirstOrder}}}
%

The base interface to nonlinear problems of the form shown in
(\ref{tsfcorenonlin:equ:c})--(\ref{tsfcorenonlin:equ:u_bnds}) is
\texttt{\textit{Nonlinear\-Problem}} (which is in the
namespace \texttt{TSFCore::Nonlin}) and was first introduced in
Section \ref{tsfcorenonlin:sec:TSFCoreNonlin}.  This interface gives
client ANAs access to the initial guess for $y$ and $u_l$ (for $l = 1
\ldots N_u$), the lower and upper bounds for variables $y_L$, $y_U$,
$u_{L,l}$, $u_{U,l}$ (for $l = 1 \ldots N_u$), the lower and upper
bounds for auxiliary response functions $g_L$ and $g_U$ and provides
the ability to compute the zero-order state constraints $c(y,\{u_l\})$
and auxiliary response functions $g(y,\{u_l\})$.  In addition,
\texttt{\textit{Vector\-Space}} objects are exposed for the spaces
$\mathcal{Y}$, $\mathcal{U}_l$ (for $l = 1 \ldots N_u$), $\mathcal{C}$
and $\mathcal{G}$.  This interface does not really provide all of the
functionality needed to satisfy the requirements in Table
\ref{tsfcorenonlin:tbl:requirements} but in a pinch could be used to
fake it (i.e.~using directional finite-differences for Jacobian-vector
products as part of an iterative solver for linear systems with
$\frac{\partial c}{\partial y}$).

The interface \texttt{\textit{Nonlinear\-Problem\-First\-Order}}
derives from \texttt{\textit{Nonlinear\-Problem}} and provides for the
creation and calculation of the Jacobian (i.e.~gradient) objects for
$\frac{\partial c}{\partial y}$, $\frac{\partial c}{\partial u_l}$,
$\frac{\partial g}{\partial y}$ and $\frac{\partial g}{\partial u_l}$.

Two important aspects of these interfaces are memory management and
multiple simultaneous calculations which are discussed next.

%
\subsubsection{Memory management}
%

Memory management of variables, computed functions and computed
gradients (Jacobians) is handled and controlled by the client ANA
(through \texttt{RefCountPtr<>} objects), not by the
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} object.  These
interfaces update objects that are controlled by the ANA which opens up
support for many different types specialized ANAs (e.g.~SFE
\cite{ref:sfe} and various types of optimization algorithms).  The
\texttt{\textit{Vector\-Space}} objects returned from the methods
\texttt{space\_y()}, \texttt{space\_u(l)}, \texttt{space\_c()} and
\texttt{space\_g()} are used to create all of the \texttt{\textit{Vector}}
and \texttt{\textit{Multi\-Vector}} objects that are computed by a
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} object and
are used in intermediate calculations (e.g.~search directions etc.) by
an ANA.

The following code snippet shows how all of the quantities to be
computed by a \texttt{\textit{Nonlinear\-Problem\-First\-Order}}
are created.

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::Nonlin::foo( NonlinearProblemFirstOrder<Scalar>* np )
{
    np->initialize();
    const int                                                Nu        = np->Nu();
    const int                                                nrf       = np->numResponseFunctions();
    Teuchos::RefCountPtr<Vector<Scalar> >                    c         = np->space_c()->createMember();
    Teuchos::RefCountPtr<Vector<Scalar> >                    g;
    if(nrf)                                                  g         = np->space_g()->createMember();
    Teuchos::RefCountPtr<LinearOpWithSolve<Scalar> >         DcDy      = np->factory_DcDy()->create();
    std::vector<Teuchos::RefCountPtr<LinearOp<Scalar> > >    DcDu(Nu);
    for( l = 1; l <= Nu; ++l )                               DcDu[l-l] = np->factory_DcDu(l)->create();
    Teuchos::RefCountPtr<MultiVector<Scalar> >               DgDy;
    if(nrf)                                                  DgDy      = np->space_y()->createMembers(nrf);
    std::vector<Teuchos::RefCountPtr<MultiVector<Scalar> > > DgDu(Nu);
    if(nrf) for( l = 1; l <= Nu; ++l )                       DgDu[l-l] = np->space_u(l)->createMembers(nrf);
    ...
}
\end{verbatim}}

Note that in the above code snippet that the
\texttt{RefCountPtr<>} objects for \texttt{g}, \texttt{DcDu},
\texttt{DgDy} and \texttt{DgDu[]} are only initialized if these quantities
exist since the presence of auxiliary variables and auxiliary response
functions are optional.

%
\subsubsection{Multiple simultaneous calculations}
%

These interfaces allow for fairly transparent multiple simultaneous
calculations of quantities when possible.  For example, it is simple
for an ANA to use a \texttt{\textit{Nonlinear\-Problem\-First\-Order}}
object in such a way that all of the functions and gradients can be
computed simultaneously when it is efficient for the underlying
implementation to do so.  This is facilitated through separate
\texttt{\textit{set...(...)}} and \texttt{\textit{calc...(...)}} methods.
If an ANA wants to allow for the multiple simultaneous calculations of
several quantities at the same point, then the following procedure
must be followed.

\begin{enumerate}
\item Set pointers to all of the quantities to be computed at the point
using the \texttt{\textit{set...(...)}} methods.
\item Call the \texttt{\textit{calc...(...)}} methods for these quantities
starting with first-order quantities (e.g.~$\frac{\partial c}{\partial
y}$) followed by zero-order quantities (e.g.~$c$).  In the first call
to \texttt{\textit{calc...(...)}}, set \texttt{newPoint=true} (the default) and
in all the later calls to \texttt{\textit{calc...(...)}} methods use
\texttt{newPoint=false}.
\end{enumerate}

Figure \ref{tsfcore:nonlin:fig:foo} shows the completion of the above
\texttt{foo(...)} function which includes code for calculating all of the
quantities at the initial point supplied by the
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} object.
%
{\bsinglespace
\begin{figure}
\begin{minipage}{\textwidth}
{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::Nonlin::foo( NonlinearProblemFirstOrder<Scalar>* np )
{
    np->initialize();
    // Basic info
    const int                                                Nu        = np->Nu();
    const int                                                nrf       = np->numResponseFunctions();
    // Create storage
    Teuchos::RefCountPtr<Vector<Scalar> >                    c         = np->space_c()->createMember();
    Teuchos::RefCountPtr<Vector<Scalar> >                    g;
    if(nrf)                                                  g         = np->space_g()->createMember();
    Teuchos::RefCountPtr<LinearOpWithSolve<Scalar> >         DcDy      = np->factory_DcDy()->create();
    std::vector<Teuchos::RefCountPtr<LinearOp<Scalar> > >    DcDu(Nu);
    for( l = 1; l <= Nu; ++l )                               DcDu[l-l] = np->factory_DcDu(l)->create();
    Teuchos::RefCountPtr<MultiVector<Scalar> >               DgDy;
    if(nrf)                                                  DgDy      = np->space_y()->createMembers(nrf);
    std::vector<Teuchos::RefCountPtr<MultiVector<Scalar> > > DgDu(Nu);
    if(nrf) for( l = 1; l <= Nu; ++l )                       DgDu[l-l] = np->space_u(l)->createMembers(nrf);
    // Setup point
    const Vector<Scalar>                &y0 = np->y0();
    std::vector<const Vector<Scalar>*>  u0_store(Nu); for(l=1;l<=Nu;++l) u0_store[l-1] = &np->u0(l);
    const Vector<Scalar>                **u0 = NULL; if(Nu) u0 = &u0_store[0];
    // Set the quantities to be computed
    prob->set_c(c.get()); prob->set_DcDy(DcDy.get());
    for( l = 1; l <= Nu; ++l ) prob->set_DcDu(l,DcDu[l-1].get());
    if(nrf) {
        prob->set_g(g.get());
        prob->set_DgDy(DgDy.get());
        for( l = 1; l <= Nu; ++l ) prob->set_DgDu(l,DgDu[l-1].get());
    }
    // Calculate the quantities
    np->calc_DcDy(y0,u0,true); for( l = 1; l <= Nu; ++l ) np->calc_DcDu(l,y0,u0,false);
    if(nrf) { np->calc_DgDy(y0,u0,false); for( l = 1; l <= Nu; ++l ) np->calc_DgDu(l,y0,u0,false); }
    np->calc_c(y0,u0,false); if(nrf) np->calc_g(y0,u0,false);
    // Unset the pointers to the quantities
    np->unsetQuantities();
}
\end{verbatim}}
\end{minipage}
\caption{
\label{tsfcore:nonlin:fig:foo}
Example code for creating calculation quantities and then computing
them with possible simultaneous calculations.
}
\end{figure}
\esinglespace}
%
The order in which the \texttt{\textit{set...(...)}} methods are
called is not important but the order and the way that the
\texttt{\textit{calc...(...)}} methods are called is very critical to
allow for the most efficient computations possible.  In the code
example in Figure \ref{tsfcore:nonlin:fig:foo} it is possible, for a
very specialized implementation, that all of the quantities that are
set with the \texttt{\textit{set...(...)}} methods may be computed
with the single call to \texttt{np->calc\_DcDy( y0, u0, true )}.  All
of the following calls to \texttt{\textit{calc...(...)}} methods may
perform no computations but must be called anyway in order to insure
that these quantities get computed (if they have not been already).
The order in which calls to \texttt{\textit{calc...(...)}} methods for
first-order quantities is usually not critical but it is critical that
all calls to \texttt{\textit{calc...(...)}}  methods for first-order
quantities be completed before calls to \texttt{\textit{calc...(...)}}
methods for zero-order quantities are invoked.  For example, if
\texttt{np->calc\_c( y0, u0, true )} was called before
\texttt{np->calc\_DcDy( y0, u0, false )}, then the specification of these
interfaces guarantees that the object \texttt{DcDy} will not be
computed in the call to \texttt{np->calc\_c( y0, u0, true )} but will be
computed in the call to \texttt{np->calc\_DcDy( y0, u0, false )} (which
may recompute the residual again if automatic differentiation is
used).  See the Doxygen documentation for more details about these
interfaces.

%
\subsubsection{Reuse of initialization and preprocessing computations}
%

The final issues to discuss with respect to the
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} interface are issues
primarily associated with the
\texttt{\textit{Linear\-Op\-With\-Solve}} objects for the (nonsingular)
Jacobian $\frac{\partial c}{\partial y}$.  This object is different
than all of the other calculation objects because of the functionality
of linear solves.  When a direct linear solver is used to implement
the \texttt{\textit{Linear\-Op\-With\-Solve}} interface an initial
analyze and factorization computation will be performed the first time
a sparse Jacobian is computed [???].  This operation is usually much
more expensive than subsequent refactorizations (which uses the same
pivot sequence and fill-in pattern).  In the case of an iterative
linear solver, an incomplete factorization may be used form a
preconditioner an therefore the same issues apply as for a direct
linear solver.  In addition, in distributed parallel applications
certain preprocessing (possibly involving communication) may be
performed to determine the load balancing for Jacobian entries.  The
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} and
\texttt{\textit{Linear\-Op\-With\-Solve}} interfaces allow
for the efficient reuse of all kinds of initialization and
preprocessing computations through intelligent implementations of
subclasses.

%
\section{Example \texttt{\textit{NonlinearProblemFirstOrder}} Subclasses}
\label{tsfcorenonlin:sec:example_nps}
%

In this section we describe two simple (serial)
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} subclasses.  These
subclasses model very small and simple problems so that the reader can
focus on the software structure and not get lost in complex numerics.
With the exception of parallel communication and sparse matrices,
these example problems have all of the features of a larger-scale
application.  The first subclass \texttt{NP2DSim} represents a simple
2-D nonlinear set of equations which does not include any auxiliary
variables or auxiliary response functions.  This is the type of
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} subclass that would
be developed for a nonlinear solver such as NOX.  In fact, an example
test program is included that calls a simple Newton-based equation
solver described in Section
\ref{tsfcorenonlin:sec:example_ana} to solve this set of equations.
The second subclass, \texttt{NP4DOpt}, builds on the \texttt{NP2DSim}
subclass (using delegation), by adding auxiliary variables and
auxiliary response functions.  These auxiliary variables and response
functions are then used to formulate an optimization problem that is
solved by MOOCHO.

%
\subsection{\texttt{NP2DSim}}
\label{tsfcorenonlin:sec:np2dsim}
%

The \texttt{\textit{Nonlinear\-Problem\-First\-Order}} subclass
\texttt{NP2DSim} models the set of nonlinear equations
%
\begin{equation}
c(y) = {\bmat{c} y_1 + y_2^2 - a \\ d (y_1^2 - y_2 - b) \emat} = 0
\label{tsfcorenonlin:equ:np2dsim}
\end{equation}
%
where $a$, $b$ and $d$ are adjustable constants.  These constraints
only have one unique solution $(1,1)$ if $a = 2$ and $b = 0$ (which is
the default).

The Jacobian of the above equations is
%
\begin{equation}
\frac{\partial c}{\partial y} =
{\bmat{cc}
  1        &  2 y_2 \\
  2 d y_1  &   -d
\emat}.
\end{equation}
%
This Jacobian is nonsingular for every point except
$(\frac{1}{2d},-\frac{1}{2})$ and $(-\frac{1}{2d},\frac{1}{2})$.

Linear systems with this Jacobian are solved using a BiCG iterative
linear solver.  The preconditioner used for this Jacobian is
%
\begin{equation}
\tilde{M} =
{\bmat{cc}
  1        &        \\
            &   -d
\emat}.
\end{equation}
%
which is created and initialized only once.

\texttt{Serial\-Vector\-Space} is used as the default
for \texttt{\textit{space\_y()}} and \texttt{\textit{space\_c()}} and
\texttt{\textit{Multi\-Vector}} is used for the basic
\texttt{\textit{LinearOp}} objects for the Jacobian \texttt{DcDy} and
its preconditioner $\tilde{M}$.  Objects of type
\texttt{Linear\-Op\-With\-Solve\-Iter} (see Figure
\ref{tsfcorenonlin:fig:LinearOpWithSolve}) are used for the
nonsingular Jacobian object \texttt{DcDy} and the subclass
\texttt{BiCG\-Solver} is used to solve the linear systems (using the
preconditioner $\tilde{M}$).  Each \texttt{DcDy} object uses a
\texttt{Normed\-Convergence\-Tester} object to specify the tolerance
for solving linear systems (the default scaled relative tolerance of
$1\times{}10^{-12}$) with the BiCG iterative linear solver.  Figure
\ref{tsfcorenonlin:fig:NP2DSim_obj} shows an object diagram for a
\texttt{NP2DSim} object along with a Jacobian object for
\texttt{DcDy} (and its aggregate objects).

%
{\bsinglespace
\begin{figure}
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 5.7in 3.8in,angle=0,scale=0.70
]{LinearOpWithSolveIter}
%}%fbox
\end{center}
\caption{
\label{tsfcorenonlin:fig:LinearOpWithSolve}
UML \cite{ref:booch_et_al_1999} class diagram:
\texttt{Linear\-Op\-With\-Solve\-Iter}, Implementation
of the \texttt{\textit{Linear\-Op\-With\-Solve}} interface using a
\texttt{\textit{Iterative\-Linear\-Solver}} object.
}
\end{figure}
\esinglespace}
%

%
{\bsinglespace
\begin{figure}
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 5.9in 2.0in,angle=0,scale=0.70
]{NP2DSim_obj}
%}%fbox
\end{center}
\caption{
\label{tsfcorenonlin:fig:NP2DSim_obj}
UML \cite{ref:booch_et_al_1999} object diagram: A \texttt{NP2DSim} object and a Jacobian
\texttt{Linear\-Op\-With\-Solve\-Iter} object with \texttt{BiCG\-Solver} object.
}
\end{figure}
\esinglespace}
%

All of the default implementations for methods that deal with
auxiliary variables and auxiliary response functions are left alone
(i.e.~not overridden) for this subclass.

A test driver program for this subclass in contained in the file
\texttt{TSFCore\-Nonlin\-NP2DSim\-Test\-Main.cpp}.  This program
includes the use of a simple Newton-based nonlinear equation solver
described in Section \ref{tsfcorenonlin:sec:example_ana} to solve the
set of equations.

%
\subsection{\texttt{NP4DOpt}}
%

\texttt{NP4DOpt} is a subclass that builds on \texttt{NP2DSim}
and adds auxiliary variables and auxiliary response functions
to give
%
\begin{equation}
c(y,u) = {\bmat{c} y_1 + y_2^2 - u_1 \\ d ( y_1^2 - y_2 - u_2 ) \emat} = 0
\label{tsfcorenonlin:equ:np4dopt_c}
\end{equation}
\begin{equation}
-10 \le g(y,u) = {\bmat{c} y_1 - \hat{y}_1 \\ y_2 - \hat{y}_2 \\ u_1 - \hat{u}_1 \\ u_2 - \hat{u}_2 \emat} \le +10.
\label{tsfcorenonlin:equ:np4dopt_g}
\end{equation}
%
The equations $c(y)$ implemented in \texttt{NP2DSim} are used to
implement the equations $c(y,u)$ in \texttt{NP4DOpt} using delegation
\cite{ref:gama_et_al_1995} as shown in Figure
\ref{tsfcorenonlin:fig:NP4DOpt}.  In this implementation, there
is only one set of auxiliary variables (i.e.~$N_u = 1$).

%
{\bsinglespace
\begin{figure}
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 2.6in 1.2in,angle=0,scale=0.70
]{NP4DOpt}
%}%fbox
\end{center}
\caption{
\label{tsfcorenonlin:fig:NP4DOpt}
UML \cite{ref:booch_et_al_1999} class diagram: \texttt{NP4DOpt} extending
\texttt{NP2DSim} using delegation.
}
\end{figure}
\esinglespace}
%

The equations and response functions as stated in
(\ref{tsfcorenonlin:equ:np4dopt_c})--(\ref{tsfcorenonlin:equ:np4dopt_g})
represent a valid feasibility problem but these response functions
$g(y,u)$ can be reinterpreted in various ways.  For example, these
response functions can be used to formulate the following least
squares objective function for a constrained optimization problem that
is solved by MOOCHO.

\begin{equation}
f(y,u) = \myonehalf \left( g_1(y,u)^2 + g_2(y,u)^2 + g_3(y,u)^2 + g_4(y,u)^2 \right)
\label{tsfcorenonlin:equ:np4dopt_obj}
\end{equation}

There is an adapter class called \texttt{NLPTSFCoreNP} that implements
the
\texttt{\textit{NLP\-Interface\-Pack\-::NLP\-First\-Order}} interface
(see \cite{ref:moochouserguide}) using a
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} object and can be
used to build aggregate objective functions such as shown in
(\ref{tsfcorenonlin:equ:np4dopt_obj}).  The MOOCHO example project
\texttt{NLP\-TSFCore\-NP\-4D\-Opt} uses MOOCHO to solve the
optimization problem represented by the minimization of
(\ref{tsfcorenonlin:equ:np4dopt_obj}) subject to
(\ref{tsfcorenonlin:equ:np4dopt_c}) and can include bounds on the
variables.

A test driver program for this subclass in contained in the file
\texttt{TSFCore\-Nonlin\-NP4DOpt\-Test\-Main.cpp}.  This program
includes the use of a simple Newton-based nonlinear equation solver
solver described in Section \ref{tsfcorenonlin:sec:example_ana} to
solve the set of equations $c(y)=0$ (with $u$ fixed at its initial
value $u_0$).  This example shows how trivially easy it is to use any
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} object to solve a
simulation problem by simply ignoring the auxiliary variables and
auxiliary response functions.

%
\section{Example Nonlinear ANA: Simple Newton-based Equation Solver}
\label{tsfcorenonlin:sec:example_ana}
%

In this section we describe a simple Newton-based nonlinear equation
solver class called \texttt{Simple\-Newton\-Solver}.  The main purpose
in looking at this ANA is to discuss some of the issues involved in
working with the \texttt{\textit{Nonlinear\-Problem\-First\-Order}}
interface in order to implement an ANA in the context of a real
algorithm.  However, this example only deals with state constraints
$c(y)$ and state variables $y$ and does not deal with issues related
to auxiliary response functions $g(y,\{u_l\})$ or auxiliary variables
$\{u_l\}$, although one should be able to generalize for the
latter case.

The concrete class \texttt{Simple\-Newton\-Solver} implements a
straightforward Newton method where each Newton step $\delta y$ is
computed (nearly) exactly as

\begin{equation}
\mbox{solve:}\;\;\;\frac{\partial c}{\partial y} \delta y = -c.
\label{tsfcorenonlin:equ:newton_step}
\end{equation}

Globalization is provided using a simple (but effective) backtracking
line search method where the acceptance of the step length $\alpha$ is
based on an Armijo condition \cite{ref:dennis_schnabel_1996}
%
\begin{equation}
\phi(y_k + \alpha \delta y) \le \phi(y_k)
+ \alpha \eta \left.\frac{d(\phi(y_k + \alpha \delta y))}{d(\alpha)}\right|_{\alpha=0}
\label{tsfcorenonlin:equ:armijo_condition}
\end{equation}
%
where $\eta < 1$ is some constant (the default is $1\times{}10^{-4}$)
and $\phi(y)$, in this case, is the merit function 
%
\begin{equation}
\phi(y) = \frac{1}{2 n} c(y)^T c(y)
\end{equation}
%
with $n = |\mathcal{Y}|$.  Note that this merit function is mesh
independent (i.e.~doubling the number of mesh points in a discretized
PDE will yield the same merit function value).  Also note that the
value of $\left.\frac{d(\phi(y_k + \alpha \delta
y))}{d(\alpha)}\right|_{\alpha=0}$ for this merit function is
%
\begin{equation}
\left.\frac{d(\phi(y_k + \alpha \delta y))}{d(\alpha)}\right|_{\alpha=0} = -\frac{1}{n} c(y_k)^T c(y_k) = -2 \phi(y_k)
\label{tsfcorenonlin:equ:newton_merit_func_direc_deriv}
\end{equation}
%
due to (\ref{tsfcorenonlin:equ:newton_step}).  If the Armijo condition
in (\ref{tsfcorenonlin:equ:armijo_condition}) is not satisfied, then
$\alpha$ is halved and the next trial point is evaluated.  This is
basically all there is to the mathematical background behind this
simple nonlinear equation solver.

Figure \ref{tsfcore:nonlin:fig:newton_solver_code} shows the C++ code for the
\texttt{Simple\-Newton\-Solver\-::solve(...)} method.
%
{\bsinglespace
\begin{figure}
\begin{minipage}{\textwidth}
{\tiny\begin{verbatim}
 00032 template<class Scalar>
 00033 Solvers::SolveReturn
 00034 SimpleNewtonSolver<Scalar>::solve( NonlinearProblemFirstOrder<Scalar> *np
 00035                                    ,Vector<Scalar> *y_inout, std::ostream *out, bool dumpAll ) const
 00036 {
 00039     // Create the data for the problem
 00040     const Index                                       n        = np->space_c()->dim();
 00041     Teuchos::RefCountPtr<Vector<Scalar> >             y        = Teuchos::rcp(y_inout,false);       // Initial guess/solution
 00042     Teuchos::RefCountPtr<Vector<Scalar> >             c        = np->space_c()->createMember(); // Residual
 00043     Teuchos::RefCountPtr<LinearOpWithSolve<Scalar> >  DcDy     = np->factory_DcDy()->create();  // Jacobian object
 00044     ETransp                                           opDcDy   = np->opDcDy();                  // Transpose argument for DcDy
 00045     Teuchos::RefCountPtr<Vector<Scalar> >             dy       = np->space_y()->createMember(); // Newton step for y
 00046     Teuchos::RefCountPtr<Vector<Scalar> >             y_new    = np->space_y()->createMember(); // Trial point for y
 00047     // Compute the initial starting point
 00048     np->unsetQuantities(); np->set_c(c.get()); np->set_DcDy(DcDy.get()); // These pointers will be maintained throughout
 00049     np->calc_DcDy(*y,NULL); np->calc_c(*y,NULL,false);
 00057     // Peform the Newton iterations
 00058     int newtonIter;
 00059     for( newtonIter = 1; newtonIter <= maxNewtonIter(); ++newtonIter ) {
 00061         // Check convergence
 00063         const Scalar phi = 0.5*dot(*c,*c)/n, sqrt_phi = sqrt(phi); // merit function: phi(c) = 0.5*dot(c,c)/n
 00064         const bool isConverged = sqrt_phi <= tol();
 00066         if(isConverged) {
 00067             np->unsetQuantities();
 00068             if(y_inout != y.get()) assign( y_inout, *y );  // Assign the solution if we have to
 00075             return Solvers::SolveReturn(Solvers::SOLVED_TO_TOL,newtonIter);
 00076         }
 00078         // Compute the newton step: dy = -inv(DcDy)*c
 00080         assign( dy.get(), 0.0 );         // Initial guess for the linear solve
 00081         DcDy->solve(opDcDy,*c,dy.get()); // Solve: DcDy*dy = c
 00082         Vt_S( dy.get(), -1.0 );          // dy *= -1.0
 00085         // Perform backtracking armijo line search
 00087         const Scalar Dphi = -2.0*phi; // D(phi(y+alpha*dy))/D(alpha) at alpha=0.0 => -dot(c,c)/n: where dy = -inv(DcDy)*c
 00088         Scalar alpha = 1.0; // Try a full step initially since it will eventually be accepted near solution
 00089         int lineSearchIter;
 00090         for( lineSearchIter = 1; lineSearchIter <= maxLineSearchIter(); ++lineSearchIter ) {
 00092             // y_new = y + alpha*dy
 00093             assign( y_new.get(), *y ); Vp_StV( y_new.get(), alpha, *dy );
 00096             // Compute the residual at the updated point
 00097             np->calc_c(*y_new,NULL);
 00099             const Scalar phi_new = 0.5*dot(*c,*c)/n, phi_frac = phi + alpha * eta() * Dphi;
 00100             const bool acceptPoint = (phi_new <= phi_frac);
 00105             if( acceptPoint ) {
 00107                 break;
 00108             }
 00109             // Backtrack
 00110             alpha *= 0.5;
 00111         }
 00121         // Take the Newton step
 00122         std::swap<Teuchos::RefCountPtr<Vector<Scalar> > >( y_new, y ); // Swap y_new and y
 00123         // Update the Jacobian
 00124         np->calc_DcDy(*y,NULL,false);
 00128     }
 00129     np->unsetQuantities();
 00130     // Failure!
 00133     return Solvers::SolveReturn(Solvers::MAX_ITER_EXCEEDED,newtonIter);
 00134 }
\end{verbatim}}
\end{minipage}
\caption{
\label{tsfcore:nonlin:fig:newton_solver_code}
Simple implementation of a Newton-based nonlinear equation solver complete with
backtracking line search for globalization.
}
\end{figure}
\esinglespace}
%
This method takes a \texttt{\textit{Nonlinear\-Problem\-First\-Order}}
object \texttt{np} and an input/output vector \texttt{y\_inout} that
specifies the initial guess on input and returns the final (or
partial) solution on output.  This member function also takes the
arguments \texttt{out} and \texttt{dumpAll} which is used to produce
output (the source code for which has been omitted from Figure
\ref{tsfcore:nonlin:fig:newton_solver_code} for the sake of
space clarity).

All of the objects that are used as part of this algorithm are
allocated on lines 42--46.  Note that the Jacobian object
\texttt{DcDy} allocated on line 43 should be considered to be
uninitialized upon creation.  The use of factories to create all of
the objects (who's lifetime is controlled by the client ANA, not the
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} object) that will
be used to store computed function and gradient values and other
associated internal objects (e.g.~preconditioners) is what makes the
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} interface so
flexible.

Pointers to the residual vector \texttt{c} and the (nonsingular)
Jacobian object \texttt{DcDy} are set to the \texttt{np} object on
line 48 shown below.
%
{\scriptsize\begin{verbatim}
    np->unsetQuantities(); np->set_c(c.get()); np->set_DcDy(DcDy.get());
\end{verbatim}}
%
These pointers are maintained throughout the algorithm and the objects
they point to are calculated on command.  The call
\texttt{np->unsetQuantities()} is used to wipe out any potential
lingering pointer references that the caller may have forgot to unset
prior to this \texttt{solve(...)}  method being called.

The Jacobian and the residual vector at the input initial guess
are computed on line 49 shown below.
%
{\scriptsize\begin{verbatim}
    np->calc_DcDy(*y,NULL); np->calc_c(*y,NULL,false);
\end{verbatim}}
%
In these calls, \texttt{NULL} is passed in for the array of pointers to
the auxiliary variables.  This is flag to the \texttt{np} object to
use the default values for \texttt{u} (see the Doxygen documentation).
Note that the order and the way that these calculation functions are
called is very important.  The specification of the
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} interface
states that the implementation may perform multiple calculations
simultaneously for all quantities with pointers set.  Therefore, when
the Jacobian \texttt{DcDy} is computed in
\texttt{np->calc\_DcDy(*y,NULL)}, the residual vector \texttt{c} may
also be computed at the same time.  This, for example, will be the
case when automatic differentiation is used.  The following call
\texttt{np->calc\_c(*y,NULL,false)} specifies the input parameter
\texttt{newPoint=false} that lets the
\texttt{np} object know that this is the same point that was
passed into the last call to a \texttt{calc\_...(...)} method.
Therefore, if \texttt{c} was already computed as a cheap side-effect
in \texttt{np->calc\_DcDy(*y,NULL)}, then the call
\texttt{np->calc\_c(*y,NULL,false)} will not compute anything.  If,
however, \texttt{c} has not been computed yet, this call will insure
that \texttt{c} is computed at the given point.  Also note that this
first call to \texttt{np->calc\_DcDy(...)} will likely involve a great
deal of initialization and preprocessing associated with the Jacobian
matrix object itself and perhaps for its direct factorization (if a
direct solver is used) or for its preconditioner (if ILU is used with
an iterative solver).  This preprocessing need not be repeated during
subsequent calls to \texttt{np->calc\_DcDy(...)} (see the implementation
of the \texttt{NP2DSim} class as an example).

After the storage for the algorithm is allocated and the initial point
is evaluated, the loop for the Newton iterations begins on line 59.
Upon entering the loop, the first computation is the convergence check
on lines 63--76 which is this case is based on
%
\begin{equation}
\phi(y)^{\myonehalf} \le \epsilon
\label{tsfcorenonlin:equ:newton_conv_test}
\end{equation}
%
where $\epsilon$ is the tolerance for the nonlinear residual and is
given the identifier name \texttt{tol}.  If the condition in
(\ref{tsfcorenonlin:equ:newton_conv_test}) (line 64) is satisfied,
then the algorithm is terminated and the solution is returned in
\texttt{y\_inout}.  The assignment on line 68
%
{\scriptsize\begin{verbatim}
    if(y_inout != y.get()) assign( y_inout, *y );  // Assign the solution if we have to
\end{verbatim}}
%
\noindent{}may be needed due to the swap that takes place on line 122.

If convergence has not been achieved, this is followed by the
computation of the Newton step (\ref{tsfcorenonlin:equ:newton_step})
on lines 80--82 shown below.
%
{\scriptsize\begin{verbatim}
    assign( dy.get(), 0.0 );         // Initial guess for the linear solve
    DcDy->solve(opDcDy,*c,dy.get()); // Solve: DcDy*dy = c
    Vt_S( dy.get(), -1.0 );          // dy *= -1.0
\end{verbatim}}
%
A few issues need to be mentioned about the above code.  First, an
initial guess for the solution of a linear system must be supplied
which is performed by \texttt{assign( dy.get(), 0.0 )} which sets the
initial guess to zero.  Without more information, this is the best
initial guess that we can give.  Failure to provide a meaningful
initial guess (i.e.~by passing in uninitialized memory) could cause an
iterative solver to fail.  The solve performed in
\texttt{DcDy->solve( opDcDy, *c,dy.get() )} will be converged to some
default (presumably tight) tolerance since an explicit
\texttt{\textit{Convergence\-Tester}} object is not supplied for
the \texttt{convTester} argument in this function call.  If this where
a truncated Newton method, then a \texttt{Normed\-Convergence\-Tester}
object would be supplied for the \texttt{convTester} argument.  The
last statement \texttt{Vt\_S( dy.get(), -1.0 )} negates the solution
to form the final Newton step.

After the Newton step is computed, the Armijo-based line search
procedure is applied on lines 87--111.  First, the directional
derivative of the merit function is computed at the current iterate is
as per (\ref{tsfcorenonlin:equ:newton_merit_func_direc_deriv}) on line
87
%
{\scriptsize\begin{verbatim}
    const Scalar Dphi = -2.0*phi;
\end{verbatim}}
%
\noindent{}which is followed by setting up the initial step length $\alpha = 1$
on line 88 shown below.
%
{\scriptsize\begin{verbatim}
    Scalar alpha = 1.0;
\end{verbatim}}
%
\noindent{}Within each line-search iteration (which begins on line 90) the
trial point $y_{+} = y + \alpha \delta y$ is computed on line 93
%
{\scriptsize\begin{verbatim}
    assign( y_new.get(), *y ); Vp_StV( y_new.get(), alpha, *dy );
\end{verbatim}}
%
\noindent{}followed by the evaluation of the residual $c_{+} = c(y_{+})$
at the trial point on line 97
%
{\scriptsize\begin{verbatim}
    np->calc_c(*y_new,NULL);
\end{verbatim}}
%
\noindent{}which is stored in the vector who's pointer was set back in line 48.
It is important to note that this call to \texttt{np->calc\_c(...)}
will not result in the Jacobian also being evaluated as a side-effect
even though a pointer to the Jacobian object \texttt{DcDy} is still
set (i.e.~\texttt{np->get\_DcDy()!=NULL}).  This is because the
\texttt{\textit{Nonlinear\-Problem}} interface specifies that only
zero-order quantities (i.e.~$c(y,\{u_l\})$ and perhaps $g(y,\{u_l\})$)
can be computed as side-effects when other zero-order quantities are
computed.  In the case of this Newton solver, the Jacobian is not
needed unless the trial point is accepted but acceptance can not be
known until the zero-order residual $c(y_{+})$ is evaluated.  This is
a typical problem the comes up in various types of nonlinear ANAs.

The merit function value is at the trial point \texttt{phi\_new} along
with the fractional extrapolation of the merit function
\texttt{phi\_frac} (used in the Armijo test) are computed on line 99
%
{\scriptsize\begin{verbatim}
    const Scalar phi_new = 0.5*dot(*c,*c)/n, phi_frac = phi + alpha * eta() * Dphi;
\end{verbatim}}
%
\noindent{}which is followed by the evaluation of the Armijo condition
(\ref{tsfcorenonlin:equ:armijo_condition}) on line 100.
%
{\scriptsize\begin{verbatim}
    const bool acceptPoint = (phi_new <= phi_frac);
\end{verbatim}}
%
If \texttt{acceptPoint==true} then the line search is exited on line 107 and if not
then the step length is halved (i.e.~$\alpha = \myonehalf \alpha$) on line
110
%
{\scriptsize\begin{verbatim}
    alpha *= 0.5;
\end{verbatim}}
%
\noindent{}and the loop executes for the next trial point.  These
line-search iterations continue until the Armijo condition is accepted
(which is guaranteed assuming the functions are smooth and
differentiable and the Jacobian is not too ill conditioned and that
the exact Newton step is computed).  A line search failure is handled
in the code (by throwing an exception) but this is not shown in Figure
\ref{tsfcore:nonlin:fig:newton_solver_code} for sake of space.

After a step length \texttt{alpha} and trial point \texttt{y\_new}
have been accepted by the line search, the pointers to the current
estimate of the solution \texttt{y} and the trial point
\texttt{y\_new} are swapped on line 122
%
{\scriptsize\begin{verbatim}
    std::swap<Teuchos::RefCountPtr<Vector<Scalar> > >( y_new, y ); // Swap y_new and y
\end{verbatim}}
%
\noindent{}which removes the need to copy vectors (this is a small trick that you
can do with smart pointers).  The Jacobian object \texttt{DcDy} is
then computed at the accepted point on line 124.
%
{\scriptsize\begin{verbatim}
    np->calc_DcDy(*y,NULL,false);
\end{verbatim}}
%
Note that in the above call to \texttt{np->calc\_DcDy(...)} that we
pass in a value of \texttt{newPoint=false} which tells the object
\texttt{np} that this point was just used in the last call to
\texttt{np->calc\_c(...)}.  While we stated earlier that the
\texttt{np} object is not allowed to compute the Jacobian object
when only \texttt{np->calc\_c( ...)} is called, there are some
cases where it is still a good idea to let a
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} object know
that this is the same point.  For example, an implementation may need
to compute the residual as an intermediate step in computing a
specialized Jacobian and this intermediate residual evaluation can be
skipped if it is known to that the residual is already computed.  In
other circumstances, the software (or at least the interface) for the
underlying application may only be setup for computing the residual
and the Jacobian simultaneously in which case the Jacobian is already
available (but just not set to the object \texttt{DcDy} yet).  By
passing in the correct values for \texttt{newPoint} we give a
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} object every
opportunity to optimize calculations and minimize duplicate and
unnecessary computations.

After the Jacobian is evaluated at the new point, the loop executes
back to the convergence check that starts on line 63.  These outer
Newton loops continue until the convergence check is satisfied or the
maximum number of Newton iterations is exceeded.

There are only three ways to return from this \texttt{solve(...)}
method: (a) with a converged solution on line 75, (b) by exceeding the
number of outer Newton iterations and ending up on line 133, or (c)
by throwing an exception (e.g.~for a line search failure).  This
concludes the description of this simple ANA.

%
\section{Summary}
%

TSFCore::Nonlin addresses the need to develop an interface to
nonlinear problems that TSFCore (purposefully) lacks.  TSFCore::Nonlin
adds abstractions for invertible linear operators (i.e.~linear
operations equipped with a solve method) and a generic nonlinear
problem interface.  The interface
\texttt{\textit{Nonlinear\-Problem\-First\-Order}} contains
all of the functionality needed to suppport a varity of nonlinear
mathematical problems such as nonlinear equation solves (for both
single and coupled sets of equations), stability and bifurcation
methods, uncertainty quantification methods, and nonlinear programming
solvers for simultaneous analysis and design (SAND) optimization.
These interface provide a great potential to reduce duplication of
effort.




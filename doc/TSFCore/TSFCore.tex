\documentclass[10pt,fleqn]{article}
\setlength{\mathindent}{0.25in}
\usepackage{graphicx}
\usepackage{latexsym}

%*** Column setup
%\setlength{\columnsep}{0.5in}
%\setlength{\columnseprule}{0.01in}

\pagestyle{plain}

%*** Page numbering
%\pagenumbering{arabic}
%\setcounter{page}{1}

%*** Paragraph setup
\setlength{\parindent}{0ex}
\setlength{\parskip}{01.5ex plus 0.3ex minus 0.3ex}
\renewcommand{\baselinestretch}{1.2}

%*** Page formating

\setlength{\topmargin}{0.0in}
\setlength{\headheight}{0.3in}
\setlength{\headsep}{0.2in}
\setlength{\textheight}{8.0in}
\setlength{\footskip}{1in}

\setlength{\oddsidemargin}{-0.5in}
\setlength{\evensidemargin}{-0.5in}
\setlength{\textwidth}{7in}

\input{rab_commands}

\newtheorem{theorem}{Theorem}
\newtheorem{algorithm}{Algorithm}

%
% Body of document
%
\begin{document}

%
% Title page
%
\title{
{\Huge\bf TSFCore}\\[1.5ex]
A Package of Light-Weight Object-Oriented Abstractions for the
Development of Abstract Numerical Algorithms and Interfacing
to Linear Algebra Libraries and Applications.
}
\author{Roscoe A. Bartlett, Michael A. Heroux and Kevin R. Long \\
Sandia National Laboratories\footnote{
Sandia is a multiprogram laboratory operated by Sandia Corporation, a
Lockheed-Martin Company, for the United States Department of Energy
under Contract DE-AC04-94AL85000.}, Albuquerque NM 87185 USA, \\
}
\date{\today}

\maketitle

%
\begin{abstract}
%
Engineering and scientific applications are becoming increasingly
modular, utilizing publicly defined interfaces to integrate
third party tools and libraries for services such as
mesh generation, data partitioning, equation solvers and optimization.
As a result, it is important to understand and model the interplay between 
these various modules, and to develop good abstract interfaces between
the primary modules.  One class of modules that is becoming
increasingly important is abstract numerical algorithms (ANAs).  ANAs
such as linear and nonlinear equation solvers,
stability and bifurcation methods,
uncertainty quantification methods and nonlinear programming solvers
for optimization are typically mathematically 
sophisticated but have surprisingly little essential dependence on the
details of what computer system is being used or how matrices and
vectors are stored and computed.  As a result, using abstract
interface capabilities in languages such as C++, we can implement ANA
software such that it will work, unchanged, with a variety of
applications and linear algebra libraries.

In this paper we present a package of minimal but complete
object-oriented interfaces (implemented in C++) called TSFCore,
which allows the development many of these ANAs and
simplifies the development of interfaces to applications and linear
algebra libraries.
%
\end{abstract}
%

\tableofcontents

\listoffigures

\section{Introduction}

One area of the continual improvement in large-scale engineering 
and scientific applications is the increased modularity of application
design and development,  utilization of publicly defined interfaces,
and the related integration of third-party software
to satisfy critical technology needs such as mesh generation, data 
partitioning and solution optimizatio methods.  Although, 
from the application 
developer's perspective, use of third party software introduces 
dependencies, it also gives the application access to the latest
technology in these areas, amortizes library and tool development
across multiple applications and, if properly designed, gives the
application easy access to more than one option for each critical
technology area, e.g., access to multiple linear solver packages.
One class of modules that is becoming
increasingly important is abstract numerical algorithms (ANAs).  ANAs
such as linear and nonlinear equation solvers,
stability and bifurcation methods,
uncertainty quantification methods and nonlinear programming solvers
for optimization are typically mathematically 
sophisticated but have surprisingly little essential dependence on the
details of what computer system is being used or how matrices and
vectors are stored and computed.  As a result, using abstract
interface capabilities in languages such as C++, we can implement ANA
software such that it will work, unchanged, with a variety of
applications and linear algebra libraries.  Such an approach is often
referred to as {\it generic programming}~\cite{ref:boost_generic_programming}.

In this paper we propose the use of a new stripped
down version of the Trilinos Solver Framework (TSF) called TSFCore as
the common interface for the development of ANAs and their 
integration into applications (APPs) and
linear algebra libraries (LALs).  By agreeing on a simple minimal
common interface layer such as TSFCore, we get around the many-to-many
problem of ANA/APP interfaces.  The goal of TSFCore is not to
replace the use of other types of more established linear algebra
interfaces such as TSF \cite{ref:TSF}, \textit{AbstractLinAlgPack}
(the linear algebra interfaces in MOOCHO \cite{ref:moochouserguide}),
or HCL \cite{ref:hcl} as the interfaces that are directly used in the
development of ANAs.  Instead, TSFCore is designed to make it easier
for developers to provide the basic functionality from APPs and LALs
required by these existing ANA-specific interfaces.  

While TSFCore provides a mechanism to express 
all of the functionality required to be directly used in ANA
development it does not attempt to provide a full collection of 
methods that directly support the anticipated functionality
needs of ANAs.  Instead TSFCore relies on a simple but powerful 
reduction and transformation operator mechanism~\cite{ref:rtop_toms}
that can be used to express any vector reduction or transformation 
operation.  More direct and convenient access to functionality that might be 
required by a given ANA is 
provided by interfaces such as TSF and \textit{AbstractLinAlgPack}. 
This direct access can be very helpful in developing ANA code.
Section \ref{tsfcore:sec:convenience_functionality} discusses this topic
in detail.

It is difficult to describe a set of linear algebra interfaces outside
of the context of some class of numerical problems.  For this purpose,
we will consider numerical algorithms where it is possible to implement
all of the required operations exclusively through well defined interfaces
to vectors, vector spaces and linear operators.  Here we consider
only the type of functionality as required in the numerical solution
of optimal control problems~\cite{ref:opt_ctrl_itfc}.

To motivate TSFCore, we present a simple mathematical formulation in
Section \ref{tsfcore:sec:math} for a basic set of nonlinear equations
which is common to many different types of numerical problems.  This
is followed by an overview of the TSFCore linear algebra interfaces in
Section \ref{tsfcore:sec:TSFCore_base}.  A more detailed discussion of
the design of the TSFCore linear algebra interfaces is presented in
Section \ref{tsfcore:sec:TSFCore_Details} which includes numerious
examples.  A complete example ANA for the iterative solution of
simultaneous systems of linear equations (using a simple block BiCG
method) is described in Section
\ref{tsfcore:sec:ANA_iter_solver_example}.  A discussion of some of the
object-oriented and other general software design concepts and
principles that have goon into the development of TSFCore is deferred
to Section \ref{tsfcore:sec:general_software_concepts}.  Some of the
nonessential but convenient functionality that is useful to direct ANA
developers that is missing in TSFCore is described in Section
\ref{tsfcore:sec:convenience_functionality}. Finally, a few comments
about making the most of TSFCore by developing adapters is described in
Section \ref{tsfcore:sec:adapters}.

%
\section{Background}
\label{tsfcore:sec:background}
%

%
\subsection{APP, ANA and LAL Modules}
\label{tsfcore:sec:module_overview}
Although we will discuss APPs, ANAs and LALs in detail later in this
section, we want to briefly introduce these terms here to make them
clear.  Also, although there are certainly other types of modules 
in a large-scale application, we only focus on these three.
\begin{itemize}
\item Application (APP):  The code that is unique to the application 
itself.  This typically includes the code that formulates and
generates the discrete problem.  In principle it could include other
third-party software that is not an ANA or LAL module.
\item Abstract Numerical Algorithm (ANA):  Software that drives a 
solution process, e.g., a nonlinear solver.  This type of package 
provides solutions to and requires services from
the APP, and utilizes services from one or more LALs.  It can usually 
be written so that
it does not depend on the details of the computer platform, or the
details of how the APP and LALs are implemented, so that an ANA
can be used across many APPs and with many LALs.
\item Linear Algebra Library (LAL): Software that provides the 
ability to construct
concrete linear algebra objects such as matrices and vectors.  
A LAL can also be aspecific linear solver or preconditioner.
\end{itemize}

An important focus of this paper is to clearly identify the interfaces 
between APPs, ANAs 
and LALs for the purposes of defining a minimal interface that 
supports the APP/ANA connection, Namely, TSFCore.

%

%
\subsection{A mathematical formulation}
\label{tsfcore:sec:math}
%

In this section, we describe a mathematical formulation for nonlinear
equations that is related to by many different types of ANAs and APPs.
The set of nonlinear equations is shown below.

{\bsinglespace
\begin{equation}
c(y)  = 0
\label{tsfcore:eqn:c}
\end{equation}
\begin{tabbing}
\hspace{\mathindent}where:\hspace{5ex}\= \\
\>	$y \:\in\:\mathcal{Y}$ : Vector of state variables \\
\>	$c(y) : \:\mathcal{Y} \rightarrow \mathcal{C}$ : State constraint functions \\
\>	$\mathcal{Y} \:\subseteq\:\RE^{n_y}$: Space of the state variables $y$ \\
\>	$\mathcal{C} \:\subseteq\:\RE^{n_y}$: Space of the state constraints $c$
\end{tabbing}
\esinglespace}

Equation (\ref{tsfcore:eqn:c}) represents a set of large-scale
nonlinear simulation equations which we refer to as the \textit{state
constraints}.  In this notation, $c(y)$ is a vector function where
each component $c_j(y)$, for $j = 1 \ldots n_y$, represents a
nonlinear scalar function of the variables $y$.  The variables $y$ are
often called the {\em state variables}.  These variables are also
known as the simulation, or solution variables.  The basic assumption
is that the size of the state space $n_y = |\mathcal{Y}| =
|\mathcal{C}|$ may be very large (e.g.~$n_y \ge 10^{6}$).

Here we identify the mathematical entities that are required by an ANA
which are revealed through a first-order Taylor expansion of $c(y)$
about $c(y_k)$

\begin{equation}
c(y) \approx c(y_k) + \frac{\partial c}{\partial y} \delta y
\label{tsfcore:eqn:c_taylor}
\end{equation}
\begin{tabbing}
\hspace{\mathindent}where:\hspace{5ex}\= \\
\>	$\frac{\partial c}{\partial y}$ is a square, nonsingular $\RE^{n_y}$-by-$\RE^{n_y}$
    Jacobian matrix evaluated at $(y_k)$
\end{tabbing}

We use the notation where the Jacobian sub-matrix $\frac{\partial
c}{\partial y}$ is defined element-wise as $\left( \frac{\partial
c}{\partial y} \right)_{(j,l)} = \frac{\partial c_j}{\partial y_l}$ ,
for $j = 1 \ldots n_y$, $l = 1 \ldots n_y$.

Note that (\ref{tsfcore:eqn:c_taylor}) is used to define the 
Newton system

\begin{equation}
\frac{\partial c}{\partial y} \delta y = - c(y_k)
\label{tsfcore:eqn:c_newton}
\end{equation}

that is the cornerstone for many different types of ANAs.

%
\subsection{Classification of abstract numerical algorithms}
\label{tsfcore:sec:classification_of_ANAs}
%

There are many different types of ANAs that are related to the type of
formulation in (\ref{tsfcore:eqn:c}) and (\ref{tsfcore:eqn:c_newton})
and the requirements assoicated for each will be a little different.
There are also different levels of ANAs where ANAs build on one
another.  For example, ANAs for iterative linear solvers [???] solve
equations of the form (\ref{tsfcore:eqn:c_newton}) that generate
Newton steps for ANAs for solving the nonlinear equations [???] in
(\ref{tsfcore:eqn:c}).  The solution of linear systems is also
required in stability anaysis and bifurcation methods [???], implicit
DAE and ODE solvers [???], uncertainty quantification methods
(i.e.~SFE [???]) and many different types of optimization methods
[???].  In addition, various types numerical analysis methods such as
for eigenvalue problems [???] that can be applied, for example, to
matrices such as the Jacobian $\frac{\partial c}{\partial y}$, can be
consisered to be ANAs.  TSFCore is designed to provide functionality
for the intersection of all of the requriements for all of these
different types of ANAs.  Requirements for more specific types is
deferred [???].

%
\subsection{Classification of linear algebra interfaces}
\label{tsfcore:sec:classification_of_lin_alg_itfc}
%

The requirements for the linear algebra objects in
(\ref{tsfcore:eqn:c_taylor}) as imposed by
an ANA are very different from the requirements imposed by an
application code.  In order to differentiate the various types of
interfaces and the requirements associated with each, consider Figure
\ref{tsfcore:fig:ANA_LAL_APP}.  This figure shows the three major
categories of software that make up a complete numerical application.
The first category is application (APP) software in which the
underlying data is defined for the problem.  This could be something
as simple as the right-hand-side and matrix coefficients of a single
linear system or as complex as a finite-element method for a 3-D
nonlinear PDE-constrained optimization problem.  The second category
is linear algebra library (LAL) software that implements basic linear
algebra operations \cite{ref:demmel_1997,ref:anderson_1995,
ref:blackford_et_al_1997, ref:aztec, ref:petsc, ref:trilinos}. These
types of software include primarily matrix-vector multiplication, the
creation of a preconditioner (e.g.~ILU), and may even include several
different types of linear solvers.  The third category is ANA software
that drives the main solution process and includes such algorithms as
iterative methods for linear and nonlinear systems; explicit and
implicit methods for ODEs and DAEs; and nonlinear programming (NLP)
solvers \cite{ref:nocedal_wright_1999}.  There are many example
software packages
\cite{ref:petsc,ref:aztec,ref:trilinos,ref:pvode,ref:tao} that contain
ANA software.

{\bsinglespace
\begin{figure}[t]
\begin{center}
%\fbox{
\includegraphics*[bb= 0.245in 2.95in 10.85in 8.60in,angle=0,scale=0.50
]{analal}
%}
\end{center}
\caption{
\label{tsfcore:fig:ANA_LAL_APP}
UML \cite{ref:booch_et_al_1999} class diagram : Interfaces between abstract numerical algorithm
(ANA), linear algebra library (LAL), and application (APP) software.
}
\end{figure}
\esinglespace}

The types of ANAs described here only require operations like
matrix-vector multiplication, linear solves and certain types of
vector reduction and transformation operations.  All of these
operations can be performed with only a very abstract view of vectors,
vector spaces and linear operators.

An application code, however, has the responsibility of populating
vector and matrix objects and requires the passing of explicit
function and gradient value entries, sometimes in a distributed
parallel environment.  This is the purpose of a APP/LAL interface.
This involves a very different set of requirements than those
described above for the ANA/APP and ANA/LAL interfaces.  Examples of
these types of APP/LAL interfaces include the FEI \cite{ref:fei} and
much of the current TSF.

Figure \ref{tsfcore:fig:ANA_LAL_APP} also shows a set of LAL/LAL
interfaces that allows linear algebra objects from one LAL to
collaborate with the objects from another LAL.  Theses interfaces are
very similar to the APP/LAL interfaces and the requirements of this
type of interface is also not addressed by TSFCore.  The ESI
\cite{ref:esi_2001} and much of the current TSF are examples of
LAL/LAL interfaces.

The parts of TSFCore described in this paper specify only part of the
ANA/LAL interfaces.  The TSFCore-based ANA/APP interfaces are 
described elsewhere [???].

%
\section{TSFCore: Requirements}
\label{tsfcore:sec:TSFCore_requirements}
%

Before describing the C++ interfaces for TSFCore, some basic
requirements are stated.

\begin{enumerate}

\item
TSFCore interfaces should be portable to all the ASCI [???]
platforms where SIERRA
\cite{ref:SIERRA} and other ASCI applications might run.  However, a
platform where C++ templates are fundamentally broken will not be a
supported platform for TSFCore.

\item
TSFCore interfaces should provide for stable and accurate numerical
computations.

\item
TSFCore should provide a minimal, but complete interface that
addresses all for the basic efficiency needs (in both speed and
storage) and will result in near optimal implementations of all of the
linear algebra objects and all of the above mentioned ANA algorithms
that use these objects.  All other types of nonessential but
convenient functionality (e.g.~Matlab-like syntax using operator
overloading, see Section \ref{tsfcore:sec:operator_overloading}) will
not be addressed by TSFCore.  This extra functionality can be built on
top the basic TSFCore abstractions (e.g.~using TSF).

\item
ANAs developed with TSFCore should be able to transparently utilize
different types of computing enviornments such as SPMD\footnote{Single
Program Multiple Data (SPMD): A single program running in a
distributed-memory environment on multiple parallel processors},
client/server\footnote{Client/Server: The ANA runs in a process on a
client computer and the APP and LAL run in processors on a server} and
out-of-core\footnote{Out-of-core: The data for the problem is stored
on disk and is read from and written to back disk as needed}
implementations.  A hand coded program (e.g.~using Fortran and MPI)
should not provide any significant gains in performance in any of the
above categories in any off the computing environments.  This is
critical for the use of TSFCore in scientific computing.

\item
The work required to implement adapter subclasses (see the ``Adapter''
pattern in \cite{ref:gama_et_al_1995}) for and with TSFCore should be
minimal and straightforward for all of the existing related linear
algebra and ANA interfaces (e.g.~the linear algebra interfaces in
MOOCHO \cite{ref:moochouserguide}, NOX [???] and TSF interfaces, see
Section \ref{tsfcore:sec:adapters}).  This requirement is facilitated
by the fact that the TSFCore interfaces are minimal.

\end{enumerate}

%
\section{TSFCore: Overview}
\label{tsfcore:sec:TSFCore_core_overview}
%

The basic linear algebra abstractions that make up TSFCore are shown in
Figure \ref{tsfcore:fig:tsfl_basic}.  The key abstractions include
vectors, vector spaces and linear operators.  Note that all of the interfaces
are templated on the \texttt{Scalar} type (the UML notation for
templated classes is not used in the figure for the sake of improving
readability).  A vector space is the foundation for all other linear
algebra abstractions.  Vector spaces are abstracted through the
\texttt{\textit{VectorSpace}} interface.  A
\texttt{\textit{VectorSpace}} object acts primarily as an ``abstract
factory'' \cite{ref:gama_et_al_1995} that creates vector objects
(which are the products in the ``abstract factory'' design pattern).
Vectors are abstracted through the \texttt{\textit{Vector}} interface.
The \texttt{\textit{Vector}} interface is very minimal and really only
defines one nontrivial method -- \texttt{\textit{applyOp(...)}}.  The
\texttt{\textit{applyOp(...)}} method accepts user-defined
(ANA-defined) reduction/transformation operator (RTOp) objects through
the templated RTOp C++ interface \texttt{\textit{RTOpPack::RTOpT<Scalar>}}.
A set of standard vector operations is provided as nonmember functions
using standard RTOp subclasses and the method
\texttt{\textit{applyOp(...)}}  (see Section ???).  The set of operations
is also easily extensible.  Every \texttt{\textit{Vector}} object also
provides access to its \texttt{\textit{VectorSpace}} (that was used to
create the \texttt{\textit{Vector}} object) through the method
\texttt{space()} (shown in Figure \ref{tsfcore:fig:tsfl_basic} as the
role name \texttt{space} on the association connecting the
\texttt{\textit{Vector}} and \texttt{\textit{VectorSpace}} classes).
The \texttt{\textit{VectorSpace}} interface also provides the ability
to create \texttt{\textit{MultiVector}} objects through the
\texttt{createMembers(numMembers)} method.  A \texttt{\textit{MultiVector}}
is a tall thin dense matrix where each column in the matrix is a
\texttt{\textit{Vector}} object which is accessible through the
\texttt{\textit{col(...)}} method.  \texttt{\textit{MultiVector}}s
are needed in order to allow for both near optimal processor cache
performance (in serial and parallel programs) and to minimize the
number of global comunications in a distributed parallel.  The
\texttt{\textit{MultiVector}} interface is used to represent the
Jacobian matrix objects $\frac{\partial g}{\partial y}$ and
$\frac{\partial g}{\partial u_l}$ and is useful in many different
types ANAs as described later.
\texttt{\textit{VectorSpace}} also declares a virtual method called
\texttt{\textit{scalarProd(x,y)}} which computes the scalar product
$<x,y>$ for the vector space. This method has a default implementation
based on the dot product $x^T y$.  Subclasses can override the
\texttt{\textit{scalarProd(x,y)}} method for other, more specialized,
application-specific definitions of the scalar product. There is also
a \texttt{\textit{MultiVector}} version
\texttt{\textit{VectorSpace\-::scalarProds(...)}} (not shown in the
figure).  Finally, \texttt{\textit{VectorSpace}} also includes the
ability to determine the compatibility of vectors from different
vector spaces through the method
\texttt{\textit{isCompatible(vecSpc)}} (see Section
\ref{tsfcore:sec:vec_spc_compatibility}).  The concepts behind the design
of the \texttt{\textit{VectorSpace}},
\texttt{\textit{Vector}} and
\texttt{\textit{MultiVector}} interfaces are discussed later in Sections
\ref{tsfcore:sec:vec_space}, \ref{tsfcore:sec:vector} and \ref{tsfcore:sec:multi_vec}
respectively.

{\bsinglespace
\begin{figure}
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 3.3in 4.4in,scale=0.50
]{UML1}
%}%fbox
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 6.55in 4.4in,scale=0.70
]{TSFCore}
%}%fbox
\end{center}
\caption{
\label{tsfcore:fig:tsfl_basic}
UML class diagram : Major components of the TSF
interface to linear algebra
}
\end{figure}
\esinglespace}

Another important type of linear algebra abstraction is a linear
operator which is represented by the interface class
\texttt{\textit{LinearOp}}.  The \texttt{\textit{LinearOp}} interface
is used to represent quantities such as the Jacobian matrices
$\frac{\partial c}{\partial y}$. A \texttt{\textit{LinearOp}} object
defines a linear mapping from vectors in one vector space (called the
\texttt{domain}) to vectors in another vector space (called the
\texttt{range}).  Every \texttt{\textit{LinearOp}} object provides
access to these vector spaces through the methods \texttt{domain()} and
\texttt{range()} (shown as the role names \texttt{domain} and
\texttt{range} on the associations linking the
\texttt{\textit{OpBase}} and \texttt{\textit{VectorSpace}} classes).
The exact form of this mapping, as implemented by the
method \texttt{\textit{apply(...)}}, is

\begin{equation}
y = \alpha \, op(M) \, x + \beta y
\label{tsfcore:equ:apply_vec}
\end{equation}

where $M$ is a \texttt{\textit{LinearOp}} object; $x$ and $y$ are
\texttt{\textit{Vector}} objects; and $\alpha$ and $\beta$ are \texttt{Scalar}
objects.  Note that the linear operator in (\ref{tsfcore:equ:apply_vec})
is shown as $op(M)$ where $op(M) = M$ or $M^T$ (depending on the
argument \texttt{M\_trans}). This implies that both the non-transposed
and transposed (i.e.~adjoint) linear mappings can be performed.
However, support for transposed (adjoint) operations by a
\texttt{\textit{LinearOp}} object are only optional.  If adjoints are
not supported then the method \texttt{\textit{opSupported(TRANS)}}
will return \texttt{false} (see Section
\ref{tsfcore:sec:linear_op_adjoints}).  Note that when $op(M) = M^T$,
then $x$ and $y$ must lie in the \texttt{range} and
\texttt{domain} spaces respectively which is the opposite for the case
where $op(M) = M$.

In addition to implementing linear mappings for single
\texttt{\textit{Vector}} objects, the
\texttt{\textit{LinearOp}} interface also allows linear mappings of
\texttt{\textit{MultiVector}} objects through an overloaded method
also called \texttt{\textit{apply(...)}} which performs

\begin{equation}
Y = \alpha \, op(M) \, X + \beta Y
\label{tsfcore:equ:apply_multi_vec}
\end{equation}

where $X$ and $Y$ are \texttt{\textit{MultiVector}} objects.  The
\texttt{\textit{MultiVector}} version of the \texttt{\textit{apply(...)}} method has a
default implementation based on the \texttt{\textit{Vector}} version.
The \texttt{\textit{Vector}} version
\texttt{\textit{apply(...)}}  is a pure virtual method and therefore must be
overridden by subclasses.  The issues associated with supporting the
\texttt{\textit{MultiVector}} version verses the \texttt{\textit{Vector}}
version of this method are described in Section
\ref{tsfcore:sec:vector_vs_multivector}.

Section \ref{tsfcore:sec:TSFCore_Details} goes into much more detail behind
the design philosophy for the interfaces and the use of the core
interfaces by both clients and subclass developers.

%
\section{TSFCore: Details and Examples}
\label{tsfcore:sec:TSFCore_Details}
%

A basic overview of the interface classes shown in Figure
\ref{tsfcore:fig:tsfl_basic} was provided in Section
\ref{tsfcore:sec:TSFCore_core_overview}.  In the following sections, we go into
more detail as the the design decisions for these interfaces and give
examples of the use of these classes.  Note that in all the below code
examples it is assumed that the code is in source file(s) which
includes the appropriate header files.

%
\subsection{A motivating example sub-ANA : Compact limited-memory BFGS}
\label{tsfcore:sec:LBFGS}
%

To motivate the following discussion and to provide examples we
consider the issues involved in using TSFCore to implement an ANA for the
compact limited-memory BFGS (LBFGS) method described in
\cite{ref:byrd_et_all_lbfgs_1994}.  BFGS and other variable-metric
quasi-Newton methods are used to approximate a Hessian matrix $B$ of
second derivatives.  This approximation is then used to generate
search directions for various types of optimization algorithms.  The
Hessian matrix $B$ and/or its inverse $H = B^{-1}$ is approximated
using only changes in the gradient $y = \nabla f(\tilde{x}) - \nabla
f(\hat{x})$ of some scalar function $f(x)$ for changes in the
variables $s = \tilde{x} - \hat{x}$.  A set of matrix approximations
$B_k$ are formed using rank-2 updates where each update takes the
form

\begin{equation}
B_{k+1} = B_k - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \frac{y_k y_k^T}{y_k^T s_k}.
\end{equation}

In a limited-memory BFGS method, only a fixed maximum number
$m_{\tiny\mbox{max}}$ of updates are stored in the multi-vectors

\begin{eqnarray}
S & = & {\bmat{cccc} s_1 & s_2 & \ldots & s_{m} \emat} \\
Y & = & {\bmat{cccc} y_1 & y_2 & \ldots & y_{m} \emat}
\end{eqnarray}

where $m \le m_{\tiny\mbox{max}}$ is the current number of updates
stored.  When the optimization algorithm starts, $m=0$ and is
incremented each iteration until $m = m_{\tiny\mbox{max}}$ and then
the method starts dropping older update pairs $(s,y)$ to make room for
newer ones.  In a compact LBFGS method, the inverse $H$ (shown in
Figure \ref{tsfcore:fig:LBFGS}) of the quasi-Newton matrix $B$ is
approximated using the tall thin multi-vectors $S$ and $Y$ along with
a small (serial) coordinating matrix $Q$ (which is computed and
updated from $S$ and $Y$).  The scalar $g$ is chosen for scaling
reasons and $H_0 = B_0^{-1} = g I$ represetns the initial matrix
approximation from which the updates are performed.  A similar compact
formula also exists for $B$ directly which involves the same matrices
(and requires solves with $Q$).  In an SPMD configuration, the
multi-vectors $Y$ and $S$ may contain vector elements spread over many
processors.  However, the number of columns $m$ in $S$ and $Y$ is
usually less than $40$.  Because of the small number of columns in $S$
and $Y$, All of the linear algebra performed with the matrix $Q$ is
performed serially using dense methods (i.e.~BLAS and LAPACK).  A
parallel implementation of the compact LBFGS method, for example, is
implemented as an option MOOCHO.  TSFCore supports efficient versions all
of the operations needed for a near optimal parallel implementation.

{\bsinglespace
\begin{figure}[t]
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 7.0in 3.7in,angle=0,scale=0.60
]{LBFGS}
%}%fbox
\end{center}
\caption{
\label{tsfcore:fig:LBFGS}
A compact limited-memory representation of the inverse of a BFGS matrix.
}
\end{figure}
\esinglespace}

Note that the matrix $H$ in Figure \ref{tsfcore:fig:LBFGS} defines an
invertible linear operator for $B$ which will be described in more
detail later.  The requirements for this sub-ANA will be mentioned in
several of the below sections.

%
\subsection{\texttt{\textit{VectorSpace}}}
\label{tsfcore:sec:vec_space}
%

The basic design of the \texttt{\textit{VectorSpace}} interface was
taken directly out of HCL which is also used in TSF and
\textit{AbstractLinAlgPack}.

Now will show a few simple code examples as to the use of the
\texttt{\textit{VectorSpace}} and
\texttt{\textit{Vector}} interfaces.  The following code snippet shows
a function that performs several types of tasks:

{\scriptsize\begin{verbatim}
temaplate<class Scalar>
void TSFCore::foo0( const VectorSpace<Scalar>& vecSpc, const LinearOp<Scalar>& M )
{
    namespace mmp = MemMngPack;
    THROW_EXCEPTION(!vecSpc.isCompatible(*M.domain()),std::logic_error,"Error!"); // Check space compatibility
    mmp::ref_count_ptr<Vector<Scalar> > x = vecSpc.createMember();                // Create a new vector x
    mmp::ref_count_ptr<Vector<Scalar>> y = M.range()->createMember();             // Create a new vector x
    assign(x.get(),1.0);                                                          // x = 1.0
    M.apply(NOTRANS,*x,y.get());                                                  // y = M*x
    M.apply(TRANS,*y,x.get(),0.5,0.1);                                            // x = 0.5*M*y + 0.1*x
}
\end{verbatim}}

The above code snippet shows how memory management in TSFCore is handled
-- through the smart reference-counted pointer class
\texttt{MemMngPack\-::ref\_count\_ptr<>} (see Section
\ref{tsfcore:sec:general_software_concepts}).  The vector objects pointed
to by the objects \texttt{x} and \texttt{y} are accessed in various
ways in the last three lines.  For instance, in the statement

{\scriptsize\begin{verbatim}
    assign(x.get(),1.0);
\end{verbatim}}

the raw C++ pointer (of type \texttt{Vector*}) to the underlying
vector object is returned using the method
\texttt{ref\_count\_ptr<>\-::get()}.  The function
\texttt{assign(...)} is implemented through an RTOp object and its
implementation is shown in the next section.  The next statement

{\scriptsize\begin{verbatim}
    M.apply(NOTRANS,*x,y.get());
\end{verbatim}}

shows the created vectors being passed into the \texttt{apply(...)}
method of a \texttt{\textit{LinearOp}} object.  The expression
\texttt{*x} invokes the method
\texttt{ref\_count\_ptr<>\-::operator*()} which returns a reference
(of type \texttt{Vector\&}) to the underlying vector object.

%
\subsubsection{General compatibility of \texttt{\textit{VectorSpace}} objects}
\label{tsfcore:sec:vec_spc_compatibility}
%

There is one important aspect that distinguishes
\texttt{TSFCore\-::\textit{VectorSpace}} from vector space interfaces in
HCL and TSF for instance.  In HCL 1.0, the compatibility of vector
spaces is tested with a virtual \texttt{operator==(...)}  method.
This implies that vector spaces will be compatible only if they are of
the same concrete type and have the same setup.  Ideally, however, we
don't want to require that only vectors and vector spaces with the
same {\em concrete} type to be compatible but instead we would like to
allow vectors and vector spaces of the same {\em general} type be
compatible.  To see the difference, consider parallel programs running
in an SPMD configuration where vector elements are partitioned across
processors and communication is handled using MPI
\cite{ref:mpi}.  There are several different linear algebra libraries
that are designed to work in such an environment such as Aztec
\cite{sd:aztec}, Epetra \cite{ref:Epetra} and PETSc \cite{ref:petsc}.  TSFCore
adapter subclasses would be created for vectors and vector spaces for
each of these packages.  In principle, all implementations of SPMD MPI
vectors that have the same partitioning of elements to processors
should be compatible, regardless of which underlying libraries
involved.  The RTOp design, given the appropriate
\texttt{\textit{VectorSpace}} and \texttt{\textit{Vector}} interfaces,
allows the seamless integration of vectors of different {\em concrete}
types given the same {\em general} type.  If all of these adapter
subclasses inherited from node interface classes like
\texttt{\textit{VectorSpaceMPIBase}} and
\texttt{\textit{VectorMPIBase}} with the appropriate set of abstract
methods (like determining compatibility of maps and access to local
vector data) then Epetra vectors should be transparently compatible
with PETSc and Aztec vectors and so on.  This type of interoperability
is demonstrated for serial vectors and vector spaces in Section
\ref{tsfcore:sec:serial_vecs}

%
\subsection{\texttt{\textit{Vector}}}
\label{tsfcore:sec:vector}
%

The core design principles behind the \texttt{\textit{Vector}}
interface and the \texttt{\textit{applyOp(...)}} method (which accepts
RTOp objects) are described in \cite{ref:rtop_toms}.  The benefits of
the RTOp approach can be summarized as follows.

\begin{enumerate}
\item LAL developers need only implement one operation ---
\textit{\texttt{applyOp(...)}} --- and not a large collection of
primitive vector operations.  
\item ANA developers can implement
\textit{specialized} vector operations without needing any support
from LAL maintainers.  
\item ANA developers can optimize time
consuming vector operations on their own for the platforms they work
with.  
\item Reduction/transformation operators are more efficient
than using primitive operations and temporary vectors.  
\item ANA-appropriate vector
interfaces that desire built-in standard vector operations (i.e.~axpy
and norms) can use RTOp operators for the default implementations of
these operations (see \textit{AbstractLinAlgPack\-::\texttt{Vector}}).
\end{enumerate}

{\bsinglespace
\begin{figure}
\begin{minipage}{\textwidth}
{\scriptsize\begin{verbatim}
------------------------------------------------------------------------------------------------------------------------------------------
// TSFCoreVectorStdOpsDecl.hpp
...
namespace TSFCore {
template<class Scalar> Scalar sum( const Vector<Scalar>& v );                            // result = sum(v(i),i=1...v.space()->dim())
template<class Scalar> Scalar norm_1( const Vector<Scalar>& v );                         // result = ||v||1
template<class Scalar> Scalar norm_2( const Vector<Scalar>& v );                         // result = ||v||2
template<class Scalar> Scalar norm_inf( const Vector<Scalar>& v_rhs );                   // result = ||v||inf
template<class Scalar> Scalar dot( const Vector<Scalar>& x, const Vector<Scalar>& y );   // result = x'*y
template<class Scalar> Scalar get_ele( const Vector<Scalar>& v, Index i );               // result = v(i)
template<class Scalar> void set_ele( Index i, Scalar alpha, Vector<Scalar>* v );         // v(i) = alpha
template<class Scalar> void assign( Vector<Scalar>* y, const Scalar& alpha );            // y = alpha
template<class Scalar> void assign( Vector<Scalar>* y, const Vector<Scalar>& x );        // y = x
template<class Scalar> void Vp_S( Vector<Scalar>* y, const Scalar& alpha );              // y += alpha
template<class Scalar> void Vt_S( Vector<Scalar>* y, const Scalar& alpha );              // y *= alpha
template<class Scalar> void Vp_StV( Vector<Scalar>* y, const Scalar& alpha
                                    ,const Vector<Scalar>& x );                          // y = alpha*x + y
template<class Scalar> void ele_wise_prod( const Scalar& alpha, const Vector<Scalar>& x
                                         ,const Vector<Scalar>& v, Vector<Scalar>* y );  // y(i)+=alpha*x(i)*v(i), i=1...y->space()->dim()
template<class Scalar> void ele_wise_divide( const Scalar& alpha, const Vector<Scalar>& x
                                           ,const Vector<Scalar>& v, Vector<Scalar>* y );// y(i)=alpha*x(i)/v(i), i=1...y->space()->dim()
template<class Scalar> void seed_random_vector_generator( unsigned int );                // Seed for random_vector(l,u,v)
template<class Scalar> void random_vector( Scalar l, Scalar u, Vector<Scalar>* v );      // v(i) = random(l,u)
} // end namespace TSFCore
------------------------------------------------------------------------------------------------------------------------------------------
\end{verbatim}}
\end{minipage}
\caption{
\label{tsfcore:fig:std_vec_ops}
Some standard vector operations declared in the header file \texttt{TSFCoreVectorStdOpsDecl.hpp}
and defined in the header file \texttt{TSFCoreVectorStdOps.hpp}.
}
\end{figure}
\esinglespace}

The \texttt{\textit{applyOp(...)}}  method is described in more detail
in Section \ref{tsfcore:sec:vec_apply_op}.  Note that this approach does
not hinder the development of convenience functions in any way.  In
fact, a set of basic operations is already available in the header
file \texttt{TSFCoreVectorStdOpsDecl.hpp}.  The declarations for the
functions in this file are shown in Figure \ref{tsfcore:fig:std_vec_ops}.
Note, to use these template functions you should include the
definitions also from \texttt{TSFCoreVectorStdOps.hpp}.
Using one of these non-member vector functions is transparently
obvious and there is not even one hint that the method
\texttt{\textit{Vector::applyOp(...)}} is involved.

%
\subsubsection{\texttt{\textit{Vector::applyOp(...)}}}
\label{tsfcore:sec:vec_apply_op}
%

Several important issues regarding the specification of the
\texttt{\textit{Vector::applyOp(...)}} method where not discussed in
\cite{ref:rtop_toms}.  Before describing these issues, note that the
\texttt{\textit{Vector\-::applyOp(...)}} method is not directly called
by a client (it is protected) but is instead is called through a
non-member (friend) function of the same name.  This is done to
provide a uniform way to deal with all of the allowed permutations of
the number and types of vector arguments to this function when the
function is called by the client.  Therefore, we will only consider
the prototype for the non-member function \texttt{TSFCore::appyOp(...)}
which is

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::applyOp(
    const RTOpPack::RTOpT<Scalar> &op
    ,const size_t num_vecs, const Vector<Scalar>* vecs[]
    ,const size_t num_targ_vecs , Vector<Scalar>* targ_vecs[]
    ,RTOp_ReductTarget reduct_obj
    ,const Index first_ele = 1, const Index sub_dim = 0, const Index global_offset = 0
    );
\end{verbatim}}

and has nine arguments: the RTOp object that defines the
reduction/transformation operation to be performed \texttt{op}; the
non-mutable input vectors specified by \texttt{num\_vecs} and
\texttt{vecs[]} (\texttt{num\_vecs==0} and\texttt{vecs==NULL}
allowed); the mutable input/output vectors specified by
\texttt{num\_targ\_vecs} and \texttt{targ\_vecs[]}
(\texttt{num\_targ\_vecs==0} and\texttt{targ\_vecs==NULL} allowed);
the input/output reduction target object \texttt{reduct\_obj} (must be
set to the value \texttt{RTOp\_REDUCT\_OBJ\_NULL} if no reduction is
defined); the range of elements defining the sub-vector to apply the
operator to specified by \texttt{first\_ele} and \texttt{sub\_dim};
and the global offset \texttt{global\_offset} to use when applying
coordinate variant operators.

The role of the first five arguments in \texttt{TSFCore::applyOp(...)}
should be clear from the discussion in \cite{ref:rtop_toms}.  However,
the special handling of the object \texttt{reduct\_obj} and the use
cases where the last three arguments are important needs to be
carefully explained since they are critical to the success of this
design.  In short, what this specification allows is the ability to
take \texttt{\textit{Vector}} objects and then be able to create
abstract compositions of them into new vector \texttt{\textit{Vector}}
objects.  There are primarily four use cases
\cite{ref:booch_et_al_1999} that this specification is designed to
support: (a) treating all of the elements in a
\texttt{\textit{Vector}} object a a single logical vector, (b)
targeting an RTOp operator to a specific element or range of elements,
(c) creating a sub-view of an existing vector and treating it as a
vector in its own right, and (d) creating a new, larger composite
(i.e.~block) abstract vector out of a collection of other abstract
vectors.

The first use case (a), where all of the elements in a
\texttt{\textit{Vector}} object are treated as a single logical
vector, is the most common one.  Here, the default argument values of
\texttt{first\_ele=1}, \texttt{sub\_dim=0} (the value \texttt{0} is a
flag to include all of the remaining elements) and
\texttt{global\_offset=0} are used and \texttt{TSFCore::applyOp(...)} is
called with the vector arguments.  For example, consider the
application of an assignment-to-scalar transformation operator
in the following function.

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::assign( Vector<Scalar>* y, const Scalar& alpha )
{
    THROW_EXCEPTION(y==NULL,std::logic_error,"assign(...), Error!");           // Validate input
    RTOpPack::TOpAssignScalar<Scalar> assign_scalar_op;                        // Create the operator object
    Vector<Scalar>* targ_vecs[] = { y };                                       // Set up the array of mutable vector arguments
    applyOp<Scalar>(assign_scalar_op,0,NULL,1,targ_vecs,RTOp_REDUCT_OBJ_NULL); // Invoke the transformation operator on the vector arguments
}
\end{verbatim}}

In the above function, the operator \texttt{assign\_scalar\_op} of type
\texttt{RTOpPack::RTOpAssignScalar} and only
performs a transformation which does not require a reduction object.
In these cases the special value of \texttt{RTOp\_REDUCT\_OBJ\_NULL}
must be passed in for the reduction object \texttt{reduct\_obj}.

If a reduction is being performed, the reduction object is initialized
prior to a single call to \texttt{TSF::applyOp(...)} and then the
reduction value is extracted.  The following function shows an example
where the norm $||.||_2$ is computed

{\scriptsize\begin{verbatim}
template<class Scalar>
Scalar TSFCore::norm_2( const Vector<Scalar>& v )
{
    RTOpPack::ROpNorm2<Scalar>        norm_2_op;                     // Create the RTOp operator object
    RTOpPack::ReductTargetT<Scalar>   norm_2_targ(norm_2_op);        // Create and initialize the reduction object
    const Vector<Scalar>* vecs[] = { &v };                           // Set up the array of non-mutable vector arguments
    applyOp<Scalar>(norm_2_op,1,vecs,0,NULL,norm_2_targ.obj();       // Invoke the reduction operator on the vector arguments
    return norm_2_op(norm_2_targ);                                   // Extract the value from the reduction target object
}
\end{verbatim}}

A great many implementations of \texttt{RTOp} operator subclasses are
already available and wrapper functions to several of these more
standard operations, including the above functions
\texttt{assign(y,alpha)} and \texttt{norm\_2(v)},
are included in the header file \texttt{TSFCoreVectorStdOps.hpp} shown in
Figure \ref{tsfcore:fig:std_vec_ops}.

The second use case (b) is where the client targets an RTOp operator
for a specific element or set of elements in a \texttt{Vector} object.
Two important examples are getting and setting individual vector
elements.  This can be accomplished without having to write specialized
RTOp subclasses for these cases.  For example, getting an element
can be performed using the standard RTOp subclass as is done
in the following function.

{\scriptsize\begin{verbatim}
template<class Scalar>
Scalar TSFCore::get_ele( const Vector<Scalar>& v, Index i )
{
    RTOpPack::ROpSum<Scalar>          sum_op;                 // Create the RTOp operator object
    RTOpPack::ReductTargetT<Scalar>   sum_targ(sum_op);       // Create and initialize the reduction object
    const Vector<Scalar>* vecs[1] = { &v };                   // Set up the array of non-mutable vector arguments
    applyOp<Scalar>(sum_op,1,vecs,0,NULL,sum_targ.obj(),i,1); // Invoke the reduction operator on the vector arguments
    return sum_opt(sum_targ);                                 // Extract the value from the reduction target object
}
\end{verbatim}}

In the above call to \texttt{TSFCore::applyOp(...)}, the argument
\texttt{global\_offset} is left at its default value of \texttt{0},
since this argument is ignored by the RTOp object \texttt{sum\_op}
anyway (the sum operator is coordinate invariant).

Setting a vector element is performed in a similar manner using the
same transformation RTOp operator subclass for assigning the elements
of a vector that was used in the \texttt{apply(...)} function shown
above.  The following function shows how setting a vector element this
is performed using this transformation operator.

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::set_ele( Index i, Scalar alpha, Vector<Scalar>* v )
{
    THROW_EXCEPTION(v==NULL,std::logic_error,"set_ele(...), Error!");               // Validate input
    RTOpPack::TOpAssignScalar<Scalar> assign_scalar_op;                             // Create the operator object
    Vector<Scalar>* targ_vecs[1] = { v };                                           // Set up the array of mutable vector arguments
    applyOp<Scalar>(assign_scalar_op,0,NULL,1,targ_vecs,RTOp_REDUCT_OBJ_NULL,i,1);  // Invoke the transformation operator on the vector arguments
}
\end{verbatim}}

Again, since the assignment operator is also coordinate invariant, the
\texttt{assign\_scalar\_op} object ignores the \texttt{global\_offset}
argument so it is left at its default value in the call to
\texttt{TSFCore::applyOp(...)}.

This approach of applying RTOp operators to targeted elements negates
the need for specialized ``get'' or ``set'' methods in the
\texttt{\textit{Vector}} interface.  By letting the \texttt{\textit{Vector}}
object know what elements are involved, the communication overhead can
be reduced just as it would be for specialized methods.  For example,
in an master-slave configuration where the ANA runs on a master
processor and the vector data is distributed across $N_p$ slave
processors, the targeted operations can be implemented with a single
set of calls to \texttt{MPI\_Send(...)} and
\texttt{MPI\_Receive(...)} at a cost which is independent of number of
processors involved.  If the client used special RTOp operators and
then asked the \texttt{\texttt{Vector}} implementation to apply to
operators to the entire vector, then there would be a $O(log(N_p))$
communication cost involved.

For an example of the third use case (c), where a sub-view of an
existing vector is treating as a vector in its own right, consider an
optimization algorithm where the state $y$ and design $u$ variables
are physically concatenated into a single serial vector $x^T =
{\bmat{cc} y^T & u^T \emat}$.  For example, if $n_y = 10$ and $n_u =
5$, then the dimension of the vector $x$ would be $n_x = 15$.  There
are parts of the algorithm where it is most convenient to treat all of
the variables $x$ the same and there are others where access to the
individual state $y$ and design $u$ sub-vectors of $x$ is required.
Now suppose that a \texttt{\textit{Vector}} object \texttt{x} is
directly used by an optimization algorithm.  When the optimization
algorithm needs to apply an RTOp operator to the state variables $y$, it
sets \texttt{first\_ele=1} and \texttt{sub\_dim=10} and then calls
\texttt{TSF::applyOp(...)} (leaving the default value of
\texttt{global\_offset=0}).  When the algorithm needs to apply an
RTOp operator to the design variables $u$, it sets it sets
\texttt{first\_ele=11} and \texttt{sub\_dim=5} and then calls
\texttt{TSF::applyOp(...)} (also leaving the default value of
\texttt{global\_offset=0}).  In each case, if a reduction is being performed,
the reduction object is initialized prior to a single call to
\texttt{TSF::applyOp(...)} and then the reduction value is extracted
just as in the first use case (a).  For example, the following
function computes the $||.||_2$ norms for the state and design
sub-vectors given the vector object \texttt{x}.

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::compute_norm_2( const Vector<Scalar>& x, Index n_y, Scalar* nrm_2_y, Saclar* nrm_2_u )
{
    const Index  n_x = x.dim(), n_u = n_x - n_y;                              // Get the dimension of the state and design spaces
    RTOpPack::ROpNorm2<Scalar>        norm_2_op;                              // Create the RTOp operator object
    RTOpPack::ReductTargetT<Scalar>   norm_2_targ(norm_2_op);                 // Create and initialize the reduction object
    const Vector<Scalar>* vecs[1] = { &x };                                   // Set up the array of non-mutable vector arguments for x = [y,u]
    applyOp<Scalar>(norm_2_op,1,vecs,0,NULL,norm_2_targ.obj(),1,n_y);         // Invoke the reduction operator for subvector y only
    *nrm_2_y = norm_2_op(norm_2_targ);                                        // Extract the value of ||y||2
    norm_2_targ.reinit()                                                      // Reinitialize the reduction target object
    applyOp<Scalar>(norm_2_op,1,vecs,0,NULL,norm_2_targ.obj(),n_y+1,n_y+n_u); // Invoke the reduction operator for subvector u only
    *nrm_2_u = norm_2_op(norm_2_targ);                                        // Extract the value of ||u||2
}
\end{verbatim}}

Finally, as an example of the fourth use case (d), where a new larger
composite (i.e.~block) abstract vector is created out of a collection
of other abstract vectors, we use the same optimization example as
above, except this time the vector $x$ is actually represented as two
separate \texttt{\textit{Vector}} objects \texttt{y} and \texttt{u}.
With that said, consider how the element with the maximum absolute
value and its index can be determined for the full vector $x$ given
separate \texttt{Vector} objects for the state $y$ and design $u$
variables.  This can be done with the predefined RTOp subclass
\texttt{RTOp\_ROp\_max\_abs\_ele} which is applied in the following
function.

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::compute_max_abs_ele( const Vector<Scalar>& y, const Vector<Scalar>& u, Scalar* x_max, Index* x_i );
{
    const Index n_y = y.dim(), n_u = u.dim();                             // Get the dimension of the state and design spaces
    RTOpPack::ROpMaxAbsEle<Scalar>    max_abs_ele_op;                     // Create the RTOp operator object
    RTOpPack::ReductTargetT<Scalar>   max_abs_ele_targ(max_abe_ele_op);   // Create and initialize the reduction object
    const Vector<Scalar>* vecs[1];                                        // Declare array for non-mutable vector arguments
    vecs[0] = &y;                                                         // Set the pointer to the y vector object
    applyOp<Scalar>(max_abs_ele_op,1,vecs,0,NULL,max_abs_ele_targ.obj(),1,0,0);// Accumulate the reduction over y
    vecs[0] = &u;                                                         // Set the pointer to the u vector object
    applyOp(max_abs_ele_op,1,vecs,0,NULL,max_abs_ele_targ.obj(),1,0,n_y); // Combine the reduction over u with the reduction of y
    *x_max = max_abs_ele_op(max_abse_ele_targ).x_max();                   // Extract the values of the reduction object
    *x_i   = max_abs_ele_op(max_abse_ele_targ).x_i();                     // ...
}
\end{verbatim}}

The above reduction operation is not coordinate invariant and
therefore the value of \texttt{global\_offset} is critical in the
calls to \texttt{TSFCore\-::applyOp(...)} in the above function.

Note that optimization algorithms are not the only ANAs that require
the (logical) composition of individual \texttt{\textit{Vector}}
objects into a single vector.  For example, SFE methods form a large
blocked SFE system out of several smaller deterministic systems [???].
There can also be multiple sets of blocking such as embedding a
blocked SFE set of state vectors $y^T = \bmat{cccc} \tilde{y}_1^T &
\tilde{y}_2^T & \ldots & \tilde{y}_N^T \emat$ into the blocked set of
optimization variables $x^T = \bmat{cc} y^T & u^T \emat$.  The basic
functionality in \texttt{\textit{Vector\-::applyOp(...)}} supports all
of these examples through the above use cases.

%
\subsubsection{Explicit access to \texttt{\textit{Vector}} elements}
%

Another important feature of the \texttt{\textit{Vector}} interface
regards the methods that can be used to gain explicit access to the
vector elements (which are not shown in the UML diagram in Figure
\ref{tsfcore:fig:tsfl_basic}) .  First, it should be noted that requesting
explicit access to vector elements is ill advised in general
(especially in an SPMD or client-server environment).  However, there
are instances where this is perfectly appropriate.  One example is
when one needs to access elements for vectors in the domain space of a
\texttt{\textit{MultiVector}} object.  This, for example, is needed in the
implementation of the compact LBFGS method described in Section
\ref{tsfcore:sec:LBFGS} above.  For the implementation of this compact
LBFGS matrix, it is critical to be able to explicitly access elements
in the domain space of $Y$ and $S$ in order to compute and update the
coordinating matrix $Q$.  Another situation when explicit access to
vector elements is appropriate and needed is when the vector is in a
small dimensional design space in an optimization problem and where
the ANA uses dense quasi-Newton methods to approximate the reduced
Hessian of the Lagrangian (this is one option in MOOCHO).

The methods in \texttt{\textit{Vector}} support three different types
of use cases with respect to explicit element access: (a) extracting a
non-mutable view of the vector elements; (b) extracting a mutable view
of the vector elements and then committing the changes back to the
vector object; and finally, (c) explicitly setting the elements in the
vector.  The prototypes for these methods are shown below.

{\scriptsize\begin{verbatim}
namespace TSFCore {
teamplate<class Scalar>
class Vector {
public:
    ...
    void getSubVector( const Range1D& rng, RTOpPack::SubVectorT<Scalar>* sub_vec ) const;   // Extract a non-mutable view
    void freeSubVector( RTOpPack::SubVectorT<Scalar>* sub_vec ) const;                      // Free a non-mutable view
    void getSubVector( const Range1D& rng, RTOpPack::MutableSubVectorT<Scalar>* sub_vec );  // Extract a mutable view
    void commitSubVector( RTOpPack::MutableSubVectorT<Scalar>* sub_vec );                   // Commit and free a mutable view
    void setSubVector( const RTOpPack::SparseSubVectorT<Scalar>& sub_vec );                 // Set elements in a (possibly) sparse format
    ...
};
} // namespace TSFCore
\end{verbatim}}

All of these methods have reasonably efficient default implementations
based on fairly sophisticated RTOp subclasses and
\texttt{\textit{Vector::applyOp(...)}}.  These default implementations of the
\texttt{\textit{getSubVector(...)}} methods require dynamic memory allocation.
With respect to use cases in typical ANAs, \texttt{\textit{Vector}}
subclasses usually do not need to override these methods for the sake
of efficiency but may need to override them for other reasons (see the
subclass \texttt{VectorSerial} in Section \ref{tsfcore:sec:serial_vecs}).

In the first use case (a), extracting and releasing a non-mutable view of the
vector elements involves calling the \texttt{const} methods
\texttt{getSubVector(...)} and
\texttt{freeSubVector(...)} respectively.  These methods use the C struct
\texttt{RTOp\_SubVector} that is build into the C and C++ interfaces for RTOp
and was therefore a natural choice for this purpose.  To demonstrate
the use of these methods the following example function will will copy
the elements from a \texttt{\textit{Vector}} object into a raw C++
array.

{\scriptsize\begin{verbatim}
teamplate<class Scalar>
void foo1( const Vector<Scalar>& x, Scalar v[] )
{
    RTOpPack::SubVectorT<Scalar> sub_vec;                 // Create and initialize the object that will provide the subvector view
    x.getSubVector(Range1D(),&sub_vec);                   // Have the vector object initialize the view and store in sub_vec
    for( Index i = 0; i < sub_vec.subDim(); ++i )         // Loop through the explict elements and extract the values into v[]
        v[i] = sub_vec(i+1)                               // ...
    x.freeSubVector(&sub_vec);                            // Free the view of the vector x
}
\end{verbatim}}

In the statement

{\scriptsize\begin{verbatim}
    x.getSubVector(Range1D(),&sub_vec);
\end{verbatim}}

the constructed \texttt{Range1D()} object represents the full range of
vector elements (this is similar to the colon '\texttt{:}' syntax
in Matlab).  Note that this method call may require dynamic memory
allocation in order to create a strided view of the vector elements
that is represented in the output argument \texttt{sub\_vec}.  The
data pointed to by \texttt{sub\_vec.values} may be dynamically
allocated which is why it is always necessary to call

{\scriptsize\begin{verbatim}
    x.freeSubVector(&sub_vec);
\end{verbatim}}

after the view in \texttt{sub\_vec} is no longer needed.

The process of extracting and committing a mutable view of
vector elements, in the second use case (b), involves the non-\texttt{const}
methods \texttt{getSubVector(...)} and
\texttt{commitSubVector(...)} respectively.  These methods use the
RTOp C struct \texttt{RTOp\_MutableSubVector}.  As an example, consider the
following function that accepts a raw C++ array of values and then adds them
to a \texttt{\textit{Vector}} object's elements.

{\scriptsize\begin{verbatim}
template<class Scalar>
void foo2( const Scalar v[], Vector<Scalar>* x )
{
    RTOpPack::MutableSubVectorT<Scalar> sub_vec;          // Create and initialize the object that will provide the subvector view
    x->getSubVector(Range1D(),&sub_vec);                  // Have the vector object initialize the view and store in sub_vec
    for( Index i = 0; i < sub_vec.subDim(); ++i           // Loop through the explict elements and add v[] to them
        sub_vec(i+1) += v[i];  // ..
    x->commitSubVector(&sub_vec);                         // Commit and free the view of x
}
\end{verbatim}}

The last use case (c) is where a client simply wants to set elements
without creating a view.  This is accomplished through the
non-\texttt{const} method \texttt{setSubVector(...)}.  This method
uses yet another built-in RTOp C struct called
\texttt{RTOp\_SparseSubVector}.  This struct is different from the
\texttt{RTOp\_SubVector} and
\texttt{RTOp\_MutableSubVector} structs in that
\texttt{RTOp\_SparseSubVector} also allows the representation of 
sparse vectors.  This is very useful for quickly and efficiently
setting up sparse \texttt{\textit{Vector}} objects.  For example, one
way to initialize a \texttt{\textit{Vector}} object to represent a
column of identity is to use a function like the following.

{\scriptsize\begin{verbatim}
template<class Scalar>
void set_eta_vec( Index i, Vector<Scalar>* e_i )
{
    const Scalar av[] = { 1.0 };                                        // Create array for the values
    const Index  ai[] = { i   };                                        // Create array for the indexes
    RTOpPack::SparseSubVectorT<Scalar> sub_vec(                         // Initialize sub_vec with the sparse nonzero element arrays
        0,e_i->dim(),1,av,1,ai,1,0,1,&sub_vec);                         // ...
    x->setSubVector(sub_vec);                                           // Set all of the elements in sub_vec to 0 except x(i) = 1.0
}
\end{verbatim}}

%
\subsubsection{Serial vectors and vector spaces}
\label{tsfcore:sec:serial_vecs}
%

One of the remarkable features of the design of the
\texttt{\textit{VectorSpace}} and \texttt{\textit{Vector}} interfaces
is that they allow, in principle, for all serial vectors and vector
spaces of the same dimension to be automatically compatible with
little work.  Here we use the term serial to mean that all of the
vector elements are stored in core in the same process where the ANA
is running.  While this may not sound remarkable at first thought
consider the fact that there exist numerous C++ classes libraries
that contain some concept of a serial vector
\cite{ref:lumsdaine_and_siek_1998, ref:tnt, ref:roberts_et_al_1996,
ref:math++_1996} which are all largely incompatible (except perhaps
through explicit element access using \texttt{operator[]} or
\texttt{operator()}).  With TSFCore, these incompatibilities are not an
issue.  The way that this works is exemplified by the subclasses
\texttt{VectorSpaceSerial} and
\texttt{VectorSerial} which are derived from the node subclasses
\texttt{VectorSpace\-SerialBase} and
\texttt{VectorSerialBase} respectively.

The first step is for every serial \texttt{\textit{VectorSpace}}
subclass to implement the \texttt{isCompatible(...)}  method in the
same way as shown below (using \texttt{VectorSpaceSerialBase} as the
example).

{\scriptsize\begin{verbatim}
template<class Scalar>
bool VectorSpaceSerialBase<Scalar>::isCompatible( const VectorSpace<Scalar>& aVecSpc ) const
{
    return this->dim() == aVecSpc.dim();
}
\end{verbatim}}

The above implementation makes the assumption that if the dimensions
of the vector spaces are the same, then the vectors themselves should
also be compatible (through the explicit sub-vector element access
methods as described below).  This also technically assumes consistent
definitions of the scalar product.

The second critical step is to have every serial
\texttt{\textit{Vector}} subclass override of the explicit sub-vector
access methods \texttt{getSubVector(...)} (both the \texttt{const} and
non-\texttt{const} versions), \texttt{freeSubVector(...)} and
\texttt{commitSubVector(...)} to perform these operations without
calling the \texttt{applyOp(...)} method (see the subclass
\texttt{VectorSerial}).

The third step is to have every serial \texttt{Vector} subclass
override and implement the method \texttt{applyOp(...)} in exactly the
same way as shown below (using the \texttt{VectorSerialBase} node
subclass as the example).

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::VectorSerialBase::applyOp(
    const RTOpPack::RTOpT<Scalar> &op, const size_t num_vecs, const Vector<Scalar>* vecs[]
    ,const size_t num_targ_vecs, Vector<Scalar>* targ_vecs[]
    ,RTOp_ReductTarget reduct_obj
    ,const Index first_ele, const Index sub_dim, const Index global_offset
    ) const
{
    ...
    in_applyOp_ = true;
    TSFCore::apply_op_serial(
        op,num_vecs,vecs,num_targ_vecs,targ_vecs,reduct_obj
        ,first_ele,sub_dim,global_offset
        );
    in_applyOp_ = false;
}
\end{verbatim}}

The implementation of the above \texttt{applyOp(...)} method is really
quite simple and it uses a helper function
\texttt{apply\_op\_serial(...)}  that takes care of all of the details
of calling the sub-vector extraction methods on the \texttt{Vector}
objects.  No dynamic casting is performed during this process and in
the case of \texttt{VectorSerial}, no dynamic memory allocation is
performed either.  Therefore, for sufficiently large serial vectors,
the overhead of these function calls will be swamped by computation in
the RTOp operators, yielding near optimal implementations.

There are cases where it can not be determined until runtime whether a
vector is serial or not.  In these cases the concrete subclasses can
not simply derive from the \texttt{VectorSpace\-SerialBase} and
\texttt{VectorSerialBase} node subclasses but must instead implement
this this functionality themselves to be used when it is determined
that the vectors are indeed serial (see the TSFCore adapter subclasses
\texttt{TSFCore::VectorSpaceEpetra} and \texttt{TSFCore::VectorSerialEpetra} for
Epetra for instance).

By using this simple approach to developing serial
\texttt{\texttt{VectorSpace}} and \texttt{\texttt{Vector}} subclass,
the details of putting together many different types of numerical
algorithms becomes much easier.

%
\subsection{\texttt{\textit{LinearOp}}}
\label{tsfcore:sec:linear_op}
%

This section continues the discussion started in Section
\ref{tsfcore:sec:TSFCore_core_overview} for the
\texttt{\textit{LinearOp}} interface and includes some examples.

%
\subsubsection{\texttt{\textit{LinearOp::apply(...)}}}
\label{tsfcore:sec:linear_op_apply}
%

The C++ prototype for the \texttt{\textit{Vector}} version of
\texttt{\textit{LinearOp\-::apply(...)}} is

{\scriptsize\begin{verbatim}
namespace TSFCore{
template<class Scalar>
class LinearOp {
public:
    ...
    virtual void apply(
        ETransp M_trans, const Vector<Scalar> &x, Vector<Scalar> *y
        ,Scalar alpha = 1.0, Scalar beta = 0.0
        ) const = 0;
    ...
};
} // namespace TSFCore
\end{verbatim}}

where the type \texttt{ETransp} is the C++ \texttt{enum}

{\scriptsize\begin{verbatim}
enum ETransp { NOTRANS, TRANS, CONJTRANS };
\end{verbatim}}

The use of an \texttt{enum} instead of a simple \texttt{bool} for the
\texttt{M\_trans} argument is very important.  The use of an \texttt{enum}
disallows the implicit conversion from other types like \texttt{char},
\texttt{int}, \texttt{double} and any type of pointer.  Using \texttt{enum}s
instead of \texttt{bool}s requires more typing but greatly helps to
avoid introducing bugs into the program that are extremely difficult
to track down.

The \texttt{\textit{MultiVector}} version of
\texttt{\textit{LinearOp\-::apply(...)}} has an identical prototype
except the \texttt{\textit{Vector}} arguments are replaced with
\texttt{\textit{MultiVector}} arguments.  The \texttt{\textit{MultiVector}}
version has a default implementation based on the
\texttt{\textit{Vector}} version as described in Section
\ref{tsfcore:sec:vector_vs_multivector}.

In the above prototype, the scalars $\alpha$ and $\beta$ default to
$1.0$ and $0.0$ respectively.  Therefore, by leaving the default values,
the default operation becomes

\[
y = op(M) x
\]

which is the same form that is declared in
\texttt{\textit{TSF::LinearOperator::apply(...)}}  and
\texttt{\textit{HCL\_LinearOperator::apply(...)}}.  However, the scalars
$\alpha$ and $\beta$ provide direct calls to BLAS functions and remove
the need to create temporaries when performing long operations (see
Section \ref{tsfcore:sec:multi_vec_linear_op}).  For example, consider
the following long expression

\[
y = A u + \gamma B^T v + \eta C w
\]

where $A$, $B$ and $C$ are \texttt{\textit{LinearOp}} objects; and $y$,
$u$, $v$ and $w$ are \texttt{\textit{Vector}} objects.  Using TSFCore, this
long operation can be performed as follows

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::long_expression(
    const LinearOp<Scalar>& A, const Vector<Scalar>& u
    ,Scalar gamma, const LinearOp<Scalar>& B, const Vector<Scalar>& u
    ,Scalar eta, const LinearOp<Scalar>& C, const Vector<Scalar>& w
    ,Vector<Scalar>* y
    )
{
    A.apply(NOTRANS,u,y);         // y  =  A*u
    B.aply(TRANS,v,y,gamma,1.0);  // y +=  gamma*B'*v
    C.aply(NOTRANS,w,y,eta,1.0);  // y +=  eta*C*w
}
\end{verbatim}}

where no temporary vectors are required.  Note that if the arguments
\texttt{alpha=1.0} and \texttt{beta=0.0} where fixed (as the are
in HCL for instance), the above operation would have to be implemented
as:

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::bad_long_expression(
    const LinearOp<Scalar>& A, const Vector<Scalar>& u
    ,Scalar gamma, const LinearOp<Scalar>& B, const Vector<Scalar>& u
    ,Scalar eta, const LinearOp<Scalar>& C, const Vector<Scalar>& w
    ,Vector<Scalar>* y
    )
{
    MemMngPack::ref_count_ptr<Vector<Scalar> >
        t = A.range()->createMember(); // Create a temporary to store the intermidate matrix-vector products
    A.apply(NOTRANS,u,y);              // y  =  A*u
    B.aply(TRANS,v,t.get());           // t  =  B'*v
    axpy(gamma,*t,y);                  // y +=  gamma*t
    C.aply(NOTRANS,w,t.get());         // t  =  C*w
    axpy(eta,*t,y);                    // y +=  eta*t
}
\end{verbatim}}

Not only is the function \texttt{bad\_long\_expression(...)} less
efficient than \texttt{long\_expression(...)} but it is also longer
and more difficult to write.  Clearly the arguments \texttt{alpha} and
\texttt{beta} are important to achieve a near optimal implementation
and ease of use.

%
\subsubsection{Optional support for adjoints}
\label{tsfcore:sec:linear_op_adjoints}
%

The \texttt{\textit{LinearOp}} interface only optionally supports
transposed (adjoint) matrix-vector multiplications and linear solves.
If the method \texttt{\textit{opSupported(M\_trans)}} returns
\texttt{false}, then the argument \texttt{M\_trans}, when
passed to \texttt{\textit{apply(...)}} and
\texttt{\textit{solve(...)}}, will result in an
\texttt{OpNotSupported} exception being thrown.
This specfication, while not ideal form an object-orientation purest
point of view, does satsisfy the basic principles outlined in Section
\ref{tsfcore:sec:general_software_concepts}.

%
\subsection{\texttt{\textit{MultiVector}}}
\label{tsfcore:sec:multi_vec}
%

While the concepts of a \texttt{\textit{VectorSpace}} and
\texttt{\textit{Vector}} are well established, the
concept of a multi-vector is something that is fairly new.  The
concept of a multi-vector was motivated the library Epetra
\cite{ref:Epetra} which contains mostly concrete implementations of
distributed-memory linear algebra classes using MPI \cite{ref:mpi}.  A
key issue is how multi-vectors and vectors relate to each other.  In
Epetra, the vector class is a specialization of the multi-vector
class.  This make since from an implementation point of view.  The
Epetra approach takes the view that a vector {\em is a} type of
multi-vector.  An arguably more natural view from an abstract
mathematical perspective is that multi-vectors are composed out of a
set of vectors where each vector represents a column of the
multi-vector.  This is the view that multi-vectors {\em have} or {\em
contain} vectors and this is the approach that has been adopted for
TSFCore as shown in Figure \ref{tsfcore:fig:tsfl_basic}.

All of the below examples will involve the compact LBFGS
implementation described above in Section \ref{tsfcore:sec:LBFGS}.  For
these examples we will consider the interaction with the two principle
\texttt{\textit{MultiVector}} objects \texttt{Y\_store} and
\texttt{S\_store} which each have $m_{\tiny\mbox{max}}$ columns.

%
\subsubsection{Accessing columns of \texttt{\textit{MultiVector}} as \texttt{\textit{Vector}} objects}
%

The columns of a \texttt{\textit{MultiVector}} object can be accessed
using the \texttt{\textit{col(j)}} method which returns a
\texttt{ref\_count\_ptr<Vector>} object which points to an abstract
\texttt{\textit{Vector}} view of a column $j$.

The following example function copies the most recent update vectors
\texttt{s} and \texttt{y} into the multi-vectors \texttt{S\_store}
and \texttt{Y\_store} and increments the counter \texttt{m} for an
LBFGS implementation.

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::update_S_Y( const Vector<Scalar>& s, const Vector<Scalar>& y, MultiVector<Scalar>* S_store, MultiVector<Scalar>* Y_store, int* m )
{
    const int m_max = S_store->domain()->dim(); // Get the maximum number of updates allowed
    if(*m < m_max) {
        ++(*m);                                 // Increment the number of updates
        assign(S_store->col(*m).get(),s);       // Copy in s into S(:,m)         
        assign(Y_store->col(*m).get(),y);       // Copy in y into Y(:,m)
    }
    else {
        // We must drop the oldest pair (s,y) copy in the newest pair
        ...
    }
}
\end{verbatim}}

Note that the \texttt{\textit{MultiVector}} object that
\texttt{\textit{col(...)}} is called on is not guaranteed to be
updated until the returned \texttt{\textit{Vector}} object is
destroyed when the \texttt{ref\_count\_ptr<>} object returned from
\texttt{\textit{col(...)}} goes out of scope.  The use in the above function
guarantees that this happens after each call to the
\texttt{assign(...)}.

%
\subsubsection{\texttt{\textit{MultiVector}} sub-views}
%

In addition to being able to access the columns of a
\texttt{\textit{MultiVector}} object one column at a time, a client
can also create \texttt{const} and non-\texttt{const}
\texttt{\textit{MultiVector}} views of a contiguous set of columns
using the \texttt{\textit{subView(...)}} methods shown below.

{\scriptsize\begin{verbatim}
namespace TSFCore {
template<class Scalar>
class MultiVector : virtual public LinearOp<Scalar> {
public:
    ...
    MemMngPack::ref_count_ptr<const MultiVector<Scalar> > subView(const Range1D& col_rng) const;  // Get a non-const view
    MemMngPack::ref_count_ptr<MultiVector<Scalar> >       subView(const Range1D& col_rng) = 0;    // Get a const view
    ...
};
} // namespace TSFCore
\end{verbatim}}

The ability to extract a \texttt{\textit{MultiVector}} sub-view of a
contiguous set of columns of a \texttt{\textit{MultiVector}} object is
required in order to implement certain types of numerical methods.
For example, the implementation of the compact LBFGS method described
above in Section \ref{tsfcore:sec:LBFGS} requires this functionality.

The following example function shows how the
\texttt{\textit{subView(...)}} method is used in an LBFGS
implementation where \texttt{\textit{MultiVector}} storage objects
\texttt{S\_store} and \texttt{Y\_store} are used to create \texttt{\textit{MultiVector}}
view objects \texttt{S} and \texttt{Y} for only the number of updates
currently stored.  These sub-view objects are used in later example
code.

{\scriptsize\begin{verbatim}
template<class Scalar>
MemMngPack::ref_count_ptr<const TSFCore::MultiVector<Scalar> > TSFCore::get_updated( const MultiVector<Scalar>& Store, int m )
{
    return Store.subView(Range1D(1,m));
}
\end{verbatim}}

%
\subsubsection{\texttt{\textit{MultiVector}} support for \texttt{\textit{applyOp(...)}}}
\label{tsfcore:sec:multi_vec_apply_op}
%

RTOp operators can be applied to the columns of a 
\texttt{\textit{MultiVector}} object one column at a time
using the \texttt{\textit{col(...)}} method.  However, A potentially
more efficient approach is to allow the
\texttt{\textit{MultiVector}} object to apply the \texttt{RTOp} operator itself.
This is supported by the \texttt{\textit{applyOp(...)}} methods on
\texttt{\textit{MultiVector}}.  The \texttt{\textit{applyOp(...)}} methods are not
called directly (they are protected) but instead are called by
non-member (friend) methods \texttt{\textit{applyOp(...)}} which then
invoke the member functions \texttt{\textit{applyOp(...)}}.  This
approach allows a more natural way to invoke a
reduction/transformation operation in line with the mathematical
description in \cite{ref:rtop_toms}.

There are two versions of
\texttt{\textit{MultiVector\-::applyOp(...)}}: one that returns a list
of reduction objects (one for each column of the multi-vector) and
another that uses two \texttt{RTOp} operators to reduce all of the
reduction objects over each column into single reduction object which
is returned.  Both versions of the
\texttt{\textit{MultiVector\-::applyOp(...)}} have default implementations
that are based on \texttt{\textit{MultiVector\-::col(...)}} and
\texttt{\textit{Vector\-::applyOp(...)}}.

Below, two example operations, which are declared in the header
\texttt{TSFCoreMultiVectorStdOps.hpp}, are shown that are needed
by various ANAs.

The first example is the update operator $\alpha U + V \rightarrow V$
and is is implemented in the following function.

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::update( Scalar alpha, const MultiVector<Scalar>& Y, MultiVector<Scalar>* V )
{
    THROW_EXCEPTION(U==NULL,std::logic_error,"axpy(...), Error!");    // Validate input
    RTOpPack::TOpAxpy<Scalar> axpy_op(alpha);                         // Create and initialize the RTOp operator object
    const MultiVector<Scalar>* multi_vecs[]       = { &U };           // Set up the array of non-mutable mulit-vector arguments
    MultiVector<Scalar>*       targ_multi_vecs[]  = { V  };           // Set up the array of mutable mulit-vector arguments
    applyOp<Scalar>(axpy_op,1,multi_vecs,1,targ_multi_vecs,NULL);     // Invoke the transformation operator on the vector arguments
}
\end{verbatim}}

In the above call to \texttt{applyOp(...)}, a \texttt{NULL} pointer is
passed in for the array of reduction objects which is allowed since
this RTOp operator does not perform a reduction.

The second example is a block dot product operation and is implemented
in the below function.

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::dot( const MultiVector<Scalar>& V1, const MultiVector<Scalar>& V2, Scalar dot[] )
{
    const int m = V1.domain()->dim();                                        // Get the number of columns in V1 and V2
    RTOpPack::ROpDot<Scalar> dot_op;                                         // Allocate the RTOp operator object
    std::vector<RTOp_ReductTarget>  dot_targs(m);                            // Allocate the array of reduction target objects
    for( int kc = 0; kc < m; ++kc )                                          // For each column:
        dot_op.reduct_obj_create_raw(&(dot_targs[kc]=RTOp_REDUCT_OBJ_NULL)); //     Create each reduction target object
    const MultiVector<Scalar>* multi_vecs[] = { &V1, &V2 };                  // Set up the array of non-mutable mulit-vector arguments
    applyOp(dot_op,2,multi_vecs,0,NULL,&dot_targs[0]);                       // Invoke the reduction operator on the multi-vector arguments
    for( int kc = 0; kc < m; ++kc ) {                                        // For each column:
        dot[kc] = dot_op(dot_targs[kc]);                                     //     Extract the value for each dot product
        dot_op.reduct_obj_free_raw(&(dot_targs[kc]));                        //     Free each reduction target object
    }
}
\end{verbatim}}

Note that the above reduction operation will be performed with a
single global reduction when performed on a distributed-memory
parallel computer (using MPI).  Without the concept of a
\texttt{\textit{MultiVector}} or support for the
\texttt{\textit{applyOp(...)}} method, this type of mulit-vector
reduction operation would require $m$ separate global reductions,
where $m$ is the number of columns in the multi-vector.  This is
critical for a near optimial implementation with respect the
minimizing communication.

%
\subsubsection{\texttt{\textit{Vector}} and \texttt{\textit{MultiVector}} correspondence}
\label{tsfcore:sec:vector_vs_multivector}
%

The interface class \texttt{\textit{LinearOp}} takes the perspective
that most subclasses will naturally prefer to implement the
\texttt{\textit{Vector}} versions of the methods
\texttt{\textit{apply(...)}} and
\texttt{\textit{solve(...)}} and let the default implementations of the
\texttt{\textit{MultiVector}} versions of these methods deal with
\texttt{\textit{MultiVector}} objects.  There are many cases where there is no
way to provide more specialized implementations of these operations
for multi-vectors.  For example, while the BLAS and LAPACK are
designed from the ground up to be more efficient with multiple
right-hand-side vectors, most current implementations of sparse direct
linear solvers unfortunately only support the solution of single
linear systems (e.g.~the Harwell solvers such as MA47 and MA48
\cite{ref:hsl_1995}).  This realization provides the motivation for
choosing the \texttt{\textit{Vector}} versions of these methods as the
default methods for subclasses to override.  With that said, if a
\texttt{\textit{LinearOp}} subclass can
provide optimized implementations of the \texttt{\textit{MultiVector}}
versions of the \texttt{\textit{apply(...)}} and/or
\texttt{\textit{solve(...)}} methods, does such a subclass
also have to provide completely independent implementations of the
\texttt{\textit{Vector}} versions of these methods?  The answer is a
qualified no.  By using the provided utility subclass
\texttt{MultiVectorCols}, a \texttt{\textit{MultiVector}} wrapper can
easily be created for any \texttt{\textit{Vector}} object.  This
\texttt{MultiVectorCols} class, coincidentally, is also used
to provide the default implementation of the
\texttt{\textit{VectorSpace\-::createMembers(numMembers)}} method.
The following example shows how a
\texttt{\textit{LinearOp}} subclass, for instance, can easily provide
support for the \texttt{\textit{Vector}} version of
\texttt{\textit{apply(...)}} when providing an implementation of the
\texttt{\textit{MultiVector}} version.

{\scriptsize\begin{verbatim}
namespace TSFCore {
template<class Scalar>
class MyLinearOp : public LinearOp<Scalar> {
public:
    ...
    void apply( ETransp M_trans, const Vector<Scalar> &x, Vector<Scalar> *y, Scalar alpha, Scalar beta ) const
    {
        namespace mmp = MemMngPack;
        const MultiVectorCols<Scalar>  X(mmp::rcp(const_cast<Vector*>(&x),false));  // Create single-column MultiVector views of each vector
        MultiVectorCols<Scalar>        Y(mmp::rcp(y,false));                        // ...
        apply(alpha,M_trans,X,&Y,beta);                                             // Call the MultiVector version defined below
    }
    void apply( ETransp M_trans, const MultiVector<Scalar> &X, MultiVector<Scalar> *Y, Scalar alpha, Scalar beta ) const
    {
        // Optimized implemetation for multi-vectors
        ...
    }
    ...
};
} // namespace TSFCore
\end{verbatim}}

Note that the constructor for the the class \texttt{MultiVectorCols},
for instance called in the line

{\scriptsize\begin{verbatim}
    MultiVectorCols<Scalar>        Y(mmp::rcp(y,false));
\end{verbatim}}

takes a \texttt{ref\_count\_ptr<const Vector>} object.  In order to
call this constructor with memory not owned by the client (which is
the case here), the \texttt{rcp(...)} function must be called with the
argument \texttt{owns\_mem = false} so that the last
\texttt{ref\_count\_ptr<const Vector>} object to be destroyed will not try
to free the vector argument.

%
\subsubsection{\texttt{\textit{MultiVector}} acting as a \texttt{\textit{LinearOp}}}
\label{tsfcore:sec:multi_vec_linear_op}
%

The last issues to discuss with regard to
\texttt{\textit{MultiVector}} relate to where it fits in the class
hierarchy.  The decision adopted for TSFCore was to make
\texttt{\textit{MultiVector}} specialize \texttt{\textit{LinearOp}}.
In other words, a \texttt{\textit{MultiVector}} object can also act as
a \texttt{\textit{LinearOp}} object.

As an example where this is needed, consider using the LBFGS inverse
matrix $H$ shown in Figure \ref{tsfcore:fig:LBFGS} as an inverse linear
operator which acts on multi-vector arguments $U$ and $V$ in an
operation of the form

\begin{eqnarray*}
U & = & \alpha B^{-1} V \\
  & = & \alpha H V \\
  & = & \alpha g V + \alpha
                            {\bmat{cc} S & g Y \emat}
                            {\bmat{cc} Q_{ss} & Q_{sy} \\ Q_{sy}^T & Q_{yy} \emat}
                            {\bmat{c} S^T \\ g Y^T \emat} V
\end{eqnarray*}

where the matrices $Q_{ss}$, $Q_{ys}$ and $Q_{yy}$ are stored as small
\texttt{\textit{MultiVector}} objects.  A multi-vector solve using
the inverse $H = B^{-1}$ might be used, for instance, in an active-set
optimization algorithm where $V$ represents the $p$ gradient vectors
of the active constraints.  This is an important operation in the
formation of a Schur complement of the KKT system in the QP subproblem
of an reduced-space SQP method \cite{RABartlett_2001}.  This
multi-vector operation using $H$ can be performed with the following
operations

\begin{eqnarray*}
T_1 & = & S^T V \\
T_2 & = & Y^T V \\
T_3 & = & Q_{ss} T_1 + g Q_{sy} T_2 \\
T_4 & = & Q_{sy}^T T_1 + g Q_{yy} T_2 \\
U   & = & \alpha g V + \alpha S T_3 + \alpha g Y T_4
\end{eqnarray*}

where $T_1$, $T_2$, $T_3$ and $T_4$ are all temporary
\texttt{\textit{MultiVector}} objects of dimension $m \times p$.  The
following function shows how the above operations are performed in
order to implement the overall multi-vector solve.

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::LBFGS_solve(
    int m, Scalar g, const MultiVector<Scalar>& S_store, const MultiVector<Scalar>& Y_store
    ,const MultiVector<Scalar>& Q_ss, const MultiVector<Scalar>& Q_sy, const MultiVector<Scalar>& Q_yy
    ,const MultiVector<Scalar>& V, MultiVector<Scalar>* U, Scalar alpha = 1.0, Scalar = beta = 0.0
    )
{
    // validate input
    ...
    const int p = V.domain()->dim();                // Get the number of columns in V and U
    MemMngPack::ref_count_ptr<const MultiVector<Scalar> >
        S = get_updated(S_store,m),                 // Get the view of only the columns in S_store for stored updates
        Y = get_updated(S_store,m);                 // Get the view of only the columns in Y_store for stored updatse
    MemMngPack::ref_count_ptr<MultiVector<Scalar> >
        T_1 = S->domain()->createMembers(p),        // Create the tempoarary multi-vectors
        T_2 = Y->domain()->createMembers(p),        // ...
        T_3 = Q_ss->range()->createMembers(p),      // ...
        T_4 = Q_yy->range()->createMembers(p);      // ...
    S->apply(TRANS,V,T_1->get());                   // T_1  =  S'*V
    Y->apply(TRANS,V,T_2->get());                   // T_2  =  Y'*V
    Q_ss.apply(NOTRANS,*T_1,T_3->get());            // T_3  =  Q_ss*T_1
    Q_sy.apply(NOTRANS,*T_2,T_3->get(),g,1.0);      // T_3 +=  g*Q_sy*T_2
    Q_sy.apply(TRANS,  *T_1,T_4->get());            // T_4  =  Q_sy'*T_1
    Q_yy.apply(NOTRANS,*T_2,T_4->get(),g,1.0);      // T_4 +=  g*Q_yy*T_2
    S->apply(NOTRANS,*T_3,U,alpha);                 // U    =  alpha*S*T_3
    Y->apply(NOTRANS,*T_4,U,alpha*g,1.0);           // U   +=  alpha*g*Y*T_4
    axpy(alpha*g,V,U);                              // U   +=  alpha*g*V
}
\end{verbatim}}

Consider the use of the above function in an SPMD environment where
the ANA runs in duplicate and in parallel on each processor.  Here,
the elements for the multi-vector objects \texttt{S\_store},
\texttt{Y\_store}, \texttt{V} and \texttt{U} are distributed across
many different processors.  Note that in this case all of the elements
in the multi-vector objects \texttt{Q\_ss}, \texttt{Q\_sy}, \texttt{Q\_yy},
\texttt{T\_1}, \texttt{T\_2}, \texttt{T\_3} and \texttt{T\_4} are stored
locally and in duplicate on each processor.  Now let's consider the
performance of this set of operations in this context.  Note that
there are principally three different types of operations with
multi-vectors that are performed through the
\texttt{\textit{MultiVector\-::apply(...)}} method.

The first type of operation performed by
\texttt{\textit{MultiVector\-::apply(...)}} is the parallel/parallel
matrix-matrix products performed in the lines

{\scriptsize\begin{verbatim}
    S->apply(TRANS,V,T_1->get());
    Y->apply(TRANS,V,T_2->get());
\end{verbatim}}

where the results are stored in the local multi-vectors 
\texttt{T\_1} and \texttt{T\_2}.  These two operations only
require a single global reduction each, independent of the number of
updates $m$ represented in $S$ and $Y$ or columns $p$ in $V$.  Note
that if there was no concept of a multi-vector and these matrix-matrix
products had to be performed one set of vectors at a time, then these
two parallel matrix-matrix products would require a whooping $2 m p$
global reductions.  For $m = 40$ and $p = 20$ this would result in $2
m p = 2(40)(20) = 1600$ global reductions.  Clearly this many global
reductions would destroy the parallel scalability of the overall ANA.
It is in this type of operation that the concept of a
\texttt{\textit{MultiVector}} is most critical for near optimal
performance in parallel programs.

The second type of operation performed by
\texttt{\textit{MultiVector\-::apply(...)}} is the local/local matrix-matrix
products of small local \texttt{\textit{MultiVector}} objects in the
lines

{\scriptsize\begin{verbatim}
    Q_ss.apply(NOTRANS,*T_1,T_3->get());
    Q_sy.apply(NOTRANS,*T_2,T_3->get(),g,1.0);
    Q_sy.apply(TRANS,  *T_1,T_4->get());
    Q_yy.apply(NOTRANS,*T_2,T_4->get(),g,1.0);
\end{verbatim}}

Note that these types of local computations classify as serial
overhead and therefore it is critical that the cost of these
operations be kept to a minimum or they could also cripple the
parallel scalability of the overall ANA.  Each of these four
matrix-matrix multiplications involve only one virtual function call
and the matrix-matrix multiplication itself can be performed with
level-3 BLAS, achieving the fastest possible flop rate attainable on
most processors \cite{ref:demmel_1997}.

The third type of operation performed by
\texttt{\textit{MultiVector\-::apply(...)}} is local/parallel matrix-matrix
multiplications performed in the lines

{\scriptsize\begin{verbatim}
    S->apply(NOTRANS,*T_3,U,alpha);
    Y->apply(NOTRANS,*T_4,U,alpha*g,1.0);
\end{verbatim}}

This type of operation involves fully scalable work with no
communication or synchronization required.  For this type of operation
a vector-by-vector implementation would not be a bottleneck form a
standpoint of global comunication.  However, this operation will
utilize level-3 BLAS and yield near optimal local cache performance
where a vector-by-vector implementation would not.

The last type of operation performed in the above
\texttt{LBFGS\_solve(...)}  function does not involve
\texttt{\textit{MultiVector\-::apply(...)}} and is shown in the line

{\scriptsize\begin{verbatim}
    axpy(alpha*g,V,U);
\end{verbatim}}

The implementation of this function was shown above in Section
\ref{tsfcore:sec:multi_vec_apply_op} and uses an RTOp transformation
operator with the \texttt{\textit{MultiVector\-::applyOp(...)}}
method.  Note that this function only involves transformation
operations (i.e.~no communication) which are fully scalable.

%
\subsubsection{Aliasing of \texttt{\textit{Vector}} and \texttt{\textit{MultiVector}} arguments}
\label{tsfcore:sec:aliasing}
%

It has not been stated specifically yet but in all
\texttt{\textit{Vector}}, \texttt{\textit{MultiVector}} and
\texttt{\textit{LinearOp}} methods where a \texttt{\textit{Vector}} or
\texttt{\textit{MultiVector}} object may be modified, it is strictly
forbidden for any of the mutable objects to alias any of the other
objects of the same type in the same method.  For example, code like the
following is strictly forbidden.

{\scriptsize\begin{verbatim}
template<class Scalar>
void foo3( const LinearOp& M, ETransp M_trans, Vector<Scalar>* x )
{
    M.apply(M_trans,*x,x);  // Error!!!!!!!!!!
}
\end{verbatim}}

Note that typically the above function would not even get to the numerics
(where it would most likely compute the wrong results) because
\texttt{M.range()->isCompatible(*M.domain())==false} in general.
Instead, this operation must be implemented as follows.

{\scriptsize\begin{verbatim}
template<class Scalar>
void foo4( const LinearOp& M, ETransp M_trans, Vector<Scalar>* x )
{
    MemMngPack::ref_count_ptr<Vector<Scalar> > x_tmp = x->clone();   // Create a copy
    M.apply(M_trans,*x_tmp,x);                                       // Okay!
}
\end{verbatim}}

Allowing client code to pass in aliased arguments would greatly
complicate the implementation of most RTOp,
\texttt{\textit{MultiVector}} and \texttt{\textit{LinearOp}}
subclasses and would introduce the possibility of many different types
of bugs that would be extremely difficult to track down.  This is an
issue that is usually not well defined in most linear algebra
interfaces but it is a very important issue.  Allowing ANA developers
to alias objects in these methods does not provide any new
functionality and is considered to be only nonessential but convenient
functionality and is therefore not included in TSFCore.  In general,
it is not possible to determine , from the abstract interfaces for the
objects themselves, if objects alias each other.  To perform this type
of test would require special methods be added to the
\texttt{\textit{Vector}} and
\texttt{\textit{MultiVector}} interfaces and implementing these test
methods would complicate the development of these types of subclasses
greatly.

In summary, dont't alias output arguments with each other or with
other input arguments in any of the TSFCore interface methods.

%
\section{An Example Abstract Numerical Algorithm : An Iterative Linear Solver}
\label{tsfcore:sec:ANA_iter_solver_example}
%

In this section we describe how TSFCore can be directly used to build
ANAs and while this is not the primary role TSFCore is designed for, this
example shows that TSFCore provides all of the needed functionality for a
near optimial performing implementation.  Code for a partial ANA in
the form of a compact LBFGS method was described in Section
\ref{tsfcore:sec:LBFGS}.  In this section, we will describe the
implementation of a simple block BiCG
\cite{ref:tmpls_for_iter_systems} method.  The reason that BiCG was
chosen for this example was because it requires adjoints and is fairly
simple.  Other types of block iterative linear solvers such as methods
as CG, BiCGStab, GMRES and QMR \cite{ref:tmpls_for_iter_systems} can
all be implemented in a similar manner.

\texttt{\textit{Iterative\-Linear\-Solver}} is a strategy interface
(see the ``Strategy'' pattern in \cite{ref:gama_et_al_1995}) for
iterative linear solvers.  This interface declares the method
\texttt{\textit{solve(...)}}  which accepts a
\texttt{\textit{LinearOp}} object \texttt{M} (which may be transposed
based on the argument \texttt{M\_trans}); right-hand-side and guess
\texttt{\textit{MultiVector}} objects \texttt{Y} and \texttt{X}
respectively; and optional left and/or right preconditioner objects
\texttt{M\_tilde\_inv\_left} and \texttt{M\_tilde\_inv\_right} (which
may also be transposed based on the arguments
\texttt{M\_tilde\_inv\_left\_trans} and
\texttt{M\_tilde\_inv\_right\_trans}) and then returns the solution
(or final estimate of the solution) in
\texttt{X}.

The subclass \texttt{Iterative\-Linear\-Solver\-BiCG} implements a
simple block BiCG method.  A listing for a single-vector version of
the BiCG method is shown in Figure \ref{tsfcore:fig:BiCG}.  This listing
is identical to the listing in \cite{ref:tmpls_for_iter_systems}
except for the substitutions $A = op(M)$, $M = op(\tilde{M})$ and $b
=a y$ (where $a$ is the scalar multiplier $\alpha$ in
(\ref{tsfcore:equ:apply_inverse_multi_vec})).  The multi-vector version,
as implemented using TSFCore in code, follows in a straightforward
manner.  This implementation does not take advantage of any potential
linear dependence in the right-hand-side vectors in an attempt to
accelerate the method such as is described in [???].  Such an enhanced
multi-vector version could be implemented in a similar manner.

\begin{figure}[t]
\begin{center}
\fbox{
\begin{minipage}{\textwidth}
{\bsinglespace
\begin{tabbing}
\hspace{4ex}\=\hspace{4ex}\=\hspace{4ex}\=\hspace{4ex} \\
\>	Compute $r^{(0)} = a y - op(M) x^{(0)}$ for the initial guess $x^{(0)}$. \\
\>	Choose $\tilde{r}^{(0)}$ (for example, $\tilde{r}^{(0)} = r^{(0)}$
	if $domain(M)$ is compatible with $range(M)$).\hspace{4ex} \\
\>	\textbf{for} $i = 1, 2, \ldots$ \\
\>	\>	solve $op(\tilde{M}) z^{(i-1)} = r^{(i-1)}$ \\
\>	\>	solve $op(\tilde{M})^T \tilde{z}^{(i-1)} = \tilde{r}^{(i-1)}$ \\
\>	\>	$\rho_{i-1} = z^{{(i-1)}^T} \tilde{r}^{(i-1)}$ \\
\>	\>	\textbf{if} $\rho_{i-1} = 0$, \textbf{method fails} \\
\>	\>	\textbf{if} $i = 1$ \\
\>	\>	\>	$p^{(i)} = z^{(i-1)}$ \\
\>	\>	\>	$\tilde{p}^{(i)} = \tilde{z}^{(i-1)}$ \\
\>	\>	\textbf{else} \\
\>	\>	\>	$\beta_{i-1} = \rho_{i-1}/\rho_{i-2}$ \\
\>	\>	\>	$p^{(i)} = z^{(i-1)} + \beta_{i-1} p^{(i-1)}$ \\
\>	\>	\>	$\tilde{p}^{(i)} = \tilde{z}^{(i-1)} + \beta_{i-1} \tilde{p}^{(i-1)}$ \\
\>	\>	\textbf{endif} \\
\>	\>	$q^{(i)} = op(M) p^{(i)}$ \\
\>	\>	$\tilde{q}^{(i)} = op(M)^T \tilde{p}^{(i)}$ \\
\>	\>	$\gamma_{i} = \tilde{p}^{{(i)}^T} q^{(i)}$ \\
\>	\>	$\alpha_{i} = \rho_{i-1}/\gamma_{i}$ \\
\>	\>	$x^{(i)} = x^{(i-1)} + \alpha_{i-1} p^{(i)}$ \\
\>	\>	$r^{(i)} = r^{(i-1)} - \alpha_{i-1} q^{(i)}$ \\
\>	\>	$\tilde{r}^{(i)} = \tilde{r}^{(i-1)} - \alpha_{i-1} \tilde{q}^{(i)}$ \\
\>	\>	check convergence; continue if necessary \\
\>	\textbf{end}
\end{tabbing}
\esinglespace}
\end{minipage}
}%fbox
\end{center}
\caption{
\label{tsfcore:fig:BiCG}
A single-vector version of the preconditioned bi-conjugate gradient method (BiCG).
}
\end{figure}

Figure \ref{tsfcore:fig:BiCG_code} shows a partial listing for the
\texttt{Iterative\-Linear\-Solver\-BiCG\-::solve(...)} method as
implemented in the file \texttt{IterativeLinearSolverBiCG.cpp}.
%
{\bsinglespace
\begin{figure}
\begin{minipage}{\textwidth}
ToDo: Update this!
{\tiny\begin{verbatim}
\end{verbatim}}
\end{minipage}
\caption{
\label{tsfcore:fig:BiCG_code}
Implementation of the multi-vector version of BiCG.
}
\end{figure}
\esinglespace}
%
All of the functions and methods called in the C++ code shown in
Figure \ref{tsfcore:fig:BiCG_code} have already been described except for
the non-member functions \texttt{assign(...)} (lines 216, 217, 138,
139, 150 and 151) and \texttt{update(...)} (lines 155, 156 and
162--164) which are declared in the header
\texttt{TSFCoreMultiVectorStdOps.hpp}.  The other version of the \texttt{update(...)}
function that is used in this algorithm (in line 124) was already
described in Section \ref{tsfcore:sec:multi_vec_apply_op}.  There are two
assignment function \texttt{assign(...)}: one that assigns a
\texttt{\textit{MultiVector}} object to a \texttt{Scalar}, and another
that assigns one \texttt{\textit{MultiVector}} object to another.
Both of these methods are implemented through
\texttt{\textit{MultiVector\-::applyOp(...)}} and use already-defined
RTOp operators.  The second version of the \texttt{update(...)}, however,
can not use \texttt{\textit{MultiVector\-::applyOp(...)}} and instead is
implemented column-by-column as

\[
(\alpha_{(j)} \beta) U_{(:,j)} + V_{(:,j)} \rightarrow V_{(:,j)}, \; \mbox{for} \; j = 1 \ldots m
\]

in the function

{\scriptsize\begin{verbatim}
template<class Scalar>
void TSFCore::update( Scalar alpha[], Scalar beta, const MultiVector<Scalar>& U, MultiVector<Scalar>* V )
{
    ...
    const int m = U.domain()->dim();
    for( int j = 1; j <= m; ++j )
        Vp_StV( V->col(j).get(), alpha[j-1]*beta, *U.col(j) );
}
\end{verbatim}}

where the \texttt{Vp\_StV(...)} function is the axpy operation for
vectors and is declared in the header \texttt{TSFCoreVectorStdOps.hpp}.  Note
that when running the above BiCG method in an SPMD configuration
(where the ANA runs in parallel and in duplicate in each process) this
implementation does not involve any communication or require any
synchronization and therefore will not affect the performance of the
algorithm for a communication point of view.  However, when running in
a master-slave configuration (where the ANA runs on the master and the
linear algebra runs in the $N_p$ slave process) every method
invocation of a method on a nonlocal TSFCore object involves
communication, including each call to
\texttt{\textit{MultiVector\-::col(j)}}.  While the number of method
invocations on TSFCore objects for all of the other operations shown in
Figure \ref{tsfcore:fig:BiCG_code} are independent of the number of
right-hand-sides $m$, this is not true for the above implementation of
the \texttt{update(...)} function.  However, form a local cache
performance point of view, note that this is a level-1 BLAS operation
so there is no real performance motivation for providing a
multi-vector version.

The reason that this operation is performed column-by-column is that
is that it is not well supported by the methods
\texttt{\textit{MultiVector\-::applyOp(...)}} or
\texttt{\textit{MultiVector\-::apply(...)}}.  The problem is that in
the current design of RTOp and
\texttt{\textit{MultiVector\-::applyOp(...)}}, an RTOp operator object
does not have any way to distinguish between different columns of a
multi-vector in order to apply different values of $\alpha_{(j)}$ for
each column $j$.  To allow this would require changing the design of
RTOp to deal with multi-vectors directly instead of just individual
vectors.

This operation could be implemented with the
\texttt{\textit{MultiVector\-::apply(...)}} method using a
\texttt{\textit{MultiVector}} object

\[
A = {\bmat{cccc} \alpha_{(1)} \beta \\ & \alpha_{(2)} \beta \\ & & \ddots \\ & & & \alpha_{(m)} \beta \emat}
\]

and then performing

\[
U A + V \rightarrow V
\]

but, since it would generally be assumed that the local multi-vector
$A$ is dense, this would likely cost $O(n m^2)$ flops instead of the
$O(n m)$ flops of the actual update operation (where $n$ is the global
number of unknowns in each linear system).

To yield a near optimal implementation in all computing environments,
this type of update operation would have to be added directly to the
\texttt{\textit{MultiVector}} interface.  However, it is not clear
that this is justified since iterative linear solvers such as this
BiCG method are likely to only run in SPMD mode.

With that said, assuming that the BiCG method shown if Figure
\ref{tsfcore:fig:BiCG_code} is run in SPMD mode, the entire
algorithm only involves three global reductions per BiCG iteration --
independent of the number of linear systems $m$ that are being solved.
These global reductions include the two multi-vector dot products on
lines 141 and 150, along with the multi-vector norm calculation (for
the convergence check) on line 166.  The two preconditioner solves on
lines 135--135 and the two multi-vector operator applications in lines
158--159 likely involve global communication also, so in general there
will be a total of seven parallel synchronizations per BiCG iteration
(or only five is no preconditioner is used).  Therefore, this
implementation allows for near optimal performance both in terms of
minimizing the number of global synchronizations and in local cache
performance (because of the use of block operations with
multi-vectors).

%
\section{General Object-Oriented Software Design Concepts and Principles}
\label{tsfcore:sec:general_software_concepts}
%
 
In this section we discuss some of the basic C++ idioms and design
patterns that have been used to construct the TSFCore C++ classes.  The
primary issues relate to modern approaches to general memory
management for object-oriented programming in C++ and to object
allocation verses initialization.  There is also a short discussion of
proper object-oriented design principles.

The basic design patterns used for memory management in TSFCore are the
``abstract factory'' and the ``prototype'' patterns as described in
the well known ``gang-of-four'' book \cite{ref:gama_et_al_1995}.  When
combined with the C++ idiom of smart reference-counted pointers for
automatic garbage collection (see \cite[Items 28-29]{ref:meyers_1996})
these design patterns become very powerful and greatly help C++
developers to dodge many of the pitfalls of dynamic memory allocation
in C++.  The basic memory management infrastructure is defined in a
namespace called \textit{MemMngPack} which is external to TSFCore.  By
far the most important class in \textit{MemMngPack} (see
\cite{ref:moochodevguide}) is the templated smart reference-counted pointer class
\texttt{ref\_count\_ptr<>}.  This templated class is very close to the
templated class \texttt{shared\_ptr<>} that is provided in the
\texttt{boost} library \cite{ref:boost}.  The use of the class
\texttt{ref\_count\_ptr<>} is described very well in the Doxygen
documentation so it will not be described here.  However, example C++
code that uses this class was shown in the above sections.

All memory management issues associated with abstract objects, which
include instantiations of all of the classes shown in Figure
\ref{tsfcore:fig:tsfl_basic}, are handled using
\texttt{ref\_count\_ptr<>}.  In this way, a client never needs to
explicitly delete one of these objects.  The object will be
automatically deleted once all of the
\texttt{ref\_count\_ptr<>} objects that point to the object go out of
scope.  The methods
\texttt{\textit{VectorSpace\-::createMember()}} and
\texttt{\textit{VectorSpace\-::createMembers(...)}}, as well as may others that
(may) have to allocate new objects all return pointers to these
objects embedded in \texttt{ref\_count\_ptr<>} objects.  Note that
there are many types of C++ client code, such as functions and
methods, that simply collaborate with preallocated objects for a short
period of time and do not need to assume any responsibilities for
memory management.  In these cases, the reference or raw pointer to the
underlying object can be extracted from the
\texttt{ref\_count\_ptr<>} object which is then passed on to C++ code
that accepts only references or raw pointers.  There are several
examples of this type of usage in the code examples in the previous
sections.

The ``abstract factory'' design pattern (as implemented by
\texttt{\textit{VectorSpace}} for instance) enabled with \texttt{ref\_count\_ptr<>}
effectively relieves clients from having to deal with how objects are
created and destroyed but there is another type of memory management
task that is also required in some use cases.  To describe the
problem, suppose that a C++ client has a handle to a
\texttt{\textit{LinearOp}} object (either through a smart or raw
pointer) and that client wants to copy the object so that some other
client will not modify the object before said client is finished with
the current \texttt{\textit{LinearOp}} object.  This is a classical
problem with the use of objects with {\em reference} (or {\em
pointer}) semantics which does not occur with objects that use {\em
value} semantics \cite{ref:stroustrup_1997}.  This use case requires
the ability to ``clone'' an object which is the basis of the
``prototype'' design pattern.  Every abstract interface shown in
Figure \ref{tsfcore:fig:tsfl_basic} defines some type of
\texttt{\textit{clone()}} method which return
\texttt{ref\_count\_ptr<>} objects pointing to the cloned (or copied)
object.  In some cases the concrete subclass does not have to
override the \texttt{\textit{clone()}} method in order achieve this
functionality (i.e.~\texttt{\textit{Vector}} and
\texttt{\textit{MultiVector}}) while in other cases it does
(i.e.~\texttt{\textit{LinearOp}}).  In cases
where a meaningful default implementation for the
\texttt{\textit{clone()}} method can not be provided, a default implementation
returning a null \texttt{ref\_count\_ptr<>} object is provided.
The implication of this approach is that while the \texttt{\textit{clone()}}
method is a useful feature, it is considered an optional feature where
subclasses are not required to provide an implementation.  However,
every good subclass implementation should provide an implementation of
the \texttt{\textit{clone()}} method since it makes the work of the client much
easier in some use cases.
	
Another set of issues that are related to the memory management issues
described above are issues concerning object allocation verses object
initialization.  Scott Myers \cite{ref:meyers_1996} and others
advocate the ``object initialization on construction'' style of
developing subclasses on the basis that is makes the subclasses easier
to write.  However, this approach is not optimal for the reusablity of
a subclass in different use cases from the ones for which the subclass
was originally designed.  To maximize ease of use by clients and
maximize reusability, another style of developing subclasses
``independent object allocation and initialization'' is to be
preferred.  This latter style of developing subclasses is the approach
that is adopted by all of the TSFCore concrete subclasses.  To support
this, every concrete subclass has a default constructor (which
constructs to an uninitialized state) and a set of
\texttt{initialize(...)}  functions that are used to actually
initialize the object.  In order to also support the ``object
initialization on construction'' style (which is useful in many
different cases) there are also a corresponding set of constructors
that call these \texttt{initialize(...)} methods using the same
arguments.  For an example of this style, see the concrete subclass
\texttt{MultiVectorCols} in the Doxygen documentation.

Error handling in TSFCore uses the built-in exception handling in C++.
All exceptions thrown by TSFCore code are derived from
\texttt{std::exception}.  Exceptions are thrown using the macro
\texttt{THROW\_EXCEPTION(...)} which results in the \texttt{std::exception::what()}
method containing an error message with the file name and line number
from where the exception was thrown.  This type of information is very
helpful in debugging.  In many cases, armed with just this information
and a good programmer developed error message, a bug can be found,
diagnosed and fixed without even needing to open a debugger.  The use
of the macro \texttt{THROW\_EXCEPTION(...)} was shown in several of
the above example code snippets.

Finally, a few comments on proper object-oriented design are in order.
It is generally accepted that object-oriented interfaces should be
minimal and every method in an interface should be implementable by
every concrete implementation \cite[Section ???]{ref:stroustrup_1997}.
However, there are some cases where the goals of simplicity and strict
conformance to this principle of ideal object-oriented design are at
odds.  Finding the proper balance of simplicity and strict
object-oriented correctness requires knowledge, experience and taste.
In all but one case, the TSFCore interfaces strictly conform to this
ideal principle of object-oriented design.  The one exception is the
support of transposed (adjoint) operations.  If an operation may not
be supportable by an implementation then the interface should provide
a way for the client to discern this without having to actually invoke
the operation.  This is related to another principle of proper
object-oriented design that absolutely every interface and method in
TSFCore adheres to and this is the principle that every method should
have its preconditions (see \cite{ref:uml_distilled_2nd_ed} for a
decision of pre- and postconditions) clearly stated and the client
should be able to check the preconditions before the method is called.
Failure to use this principle makes the use of such software very
difficult and results in a lot of unexpected runtime errors.  If an
operation can not be performed by an object because of the violation
of a precondition, then a good way to handle this is for the method to
throw an exception.  However, proper object-oriented design does not
require this since it is the responsibility of the client to ensure
that preconditions are satisfied (see
\cite{ref:uml_distilled_2nd_ed}).  In practice, however, defensive
programming practices (see \cite{ref:stroustrup_1997}) dictate that clients
should be considered to be unreliable and therefore all preconditions
should be checked by every major method implementation and if a
precondition is found to be violated then an exception should be
thrown which contain a detailed error message that describes the
problem (i.e.~as returned from \texttt{std::exception::what()}).  If
the preconditions are met before the method is called and the method
can not satisfy the postconditions for some reason then the method
should throw an exception in general.  This latter type of exception
is the primary reason that exception handling was added to the C++
standard in the first place \cite{ref:design_evol_cpp}.

Another desirable principle of object-oriented design is that an
interface should provide declarations for all important methods for
which if specialized implementations for all of these methods were
provided, then the resulting overall software implementation would be
near optimal with respect to storage and runtime efficiency.  Again,
knowledge, experience and taste are required in the selection of the
appropriate set of required methods.  However, there is conflict
between the goals of declaring many methods for the sake of near
optimal performance and the desire to keep the number of methods to a
minimum to ease subclass development.  The approach that each TSFCore
interface takes to this issue is that the (nearly) full set of methods
needed for a near optimal implementation are declared in the interface
but reasonable (suboptimal) default implementations are provided for
as many of the methods as possible.  Examples of the application of
this principle are mentioned for every major TSFCore interface (for
example, the default implementation of the
\texttt{\textit{MultiVector}} version of the method
\texttt{\textit{LinearOp\-::apply(...)}} which is based on
the \texttt{\textit{Vector}} version).

%
\section{Nonessential but Convenient Functionality Missing in TSFCore}
\label{tsfcore:sec:convenience_functionality}
%

While TSFCore provides all of the functionality required to be
directly used in ANA development it lacks much nonessential but convenient
functionality that is very helpful in developing ANA code.  This
nonessential but convenient functionality can be built on top of the core
functionality which is precisely the type of extra functionality that
TSF and \textit{AbstractLinAlgPack} provide.  In this section, several
different examples of nonessential but convenient functionality are given along
with references to where this functionality exists in TSF and
\textit{AbstractLinAlgPack}.

%
\subsection{Sub-vector views as \texttt{\textit{Vector}} objects}
%

In Section \ref{tsfcore:sec:vec_apply_op}, the use case where the
sub-vectors of a \texttt{\textit{Vector}} object are treated as logical
vector was discussed.  The example in that section got the job done
but a better approach to providing access to sub-vectors is to create a
sub-view decorator subclass (see the ``decorator'' pattern in
\cite{ref:gama_et_al_1995}) that allows the creation of a
\texttt{Vector} view object of a contiguous range of elements in
another \texttt{Vector} object.  Such a subclass is included in
\textit{AbstractLinAlgPack} (see \texttt{VectorSubView} and
\texttt{VectorMutableSubView}) and is very useful for high-level ANA
code.  These ``sub-view'' subclasses can be easily developed just
using \texttt{\textit{Vector\-::applyOp(...)}}.

%
\subsection{Composition of \texttt{\textit{Vector}} and \texttt{\textit{LinearOp}} objects}
%

The ideal way to represent composite vector object, such as described
in Section \ref{tsfcore:sec:vec_apply_op}, is to create a composite block
vector subclass such as \texttt{TSF::TSFProductVector} in TSF or
\texttt{AbstractLinAlgPack\-::VectorBlock} in
\textit{AbstractLinAlgPack}.  Associated with these composite vector
subclasses are composite vector spaces subclsses.  These subclasses
are called \texttt{TSF\-::TSFProductSpace} in TSF and
\texttt{AbstractLinAlgPack\-::VectorSpaceBlock} in
\textit{AbstractLinAlgPack}.  These types of composite
\texttt{\textit{Vector}} and \texttt{\textit{VectorSpace}} subclasses
are easy to develop because of the specification of
\texttt{\textit{Vector\-::applyOp(...)}}.

Similar generic composition subclasses also exist for linear operators
in TSF (see \texttt{TSF::TSFBlockLinearOperator} and
\texttt{TSF::TSFSumOperator})) and \texttt{AbstractLinAlgPack} (see
\texttt{AbstractLinAlgPack\-::MatrixOpBlock} and
\texttt{AbstractLinAlgPack\-::MatrixOpComposite}).  In addition, more
application-specific composite \texttt{\textit{LinearOp}} subclasses
can also be developed (for example for the SFE system in [???]).

%
\subsection{Matlab-like notation and handle classes for linear algebra using operator overloading}
\label{tsfcore:sec:operator_overloading}
%

TSFCore contains abstractions for linear algebra objects.  Mathematicians
use a precise syntax to describe linear algebra operations.  Matlab
\cite{ref:matlab} has established a useful convention for mathematical
linear algebra syntax using only ASCII characters.  C++ has operator
overloading.  When you put all of this together it seems obvious, at
first glance, that operator overloading in C++ should be used to
specify linear algebra operations like

\[
y = A u + \gamma B^T v + \eta C w
\]

in C++ as

\begin{verbatim}
    y = A*u + gamma*trans(B)*v + eta*w;
\end{verbatim}

However, providing a near optimal implementation (i.e.~no unnecessary
temporaries or multiple memory accesses) of operator overloading for
linear algebra in C++ is nontrivial.  While this type of syntax is
desirable, it does not provide any new functionality and is only
nonessential but convenient functionality and is therefore not included in TSFCore.
An efficient operator overloading mechanism in C++ is hard to
implement and is difficult for C++ novices to debug through.  If
operator overloading is to be built on top of TSFCore (e.g.~using TSF for
instance) then this implementation must be bullet proof and provide
unmatched exception handling so that users must never need to debug
through this code.  TSF has started to implement linear algebra
operations using operator overloading but at the present time only
vector-vector operations are supported.

Closely associated with operator overloading is the concept of handle
classes [???].  Handles assume the same type of role as a smart
pointers except all of the method forwarding (which is performed
automatically with the operator function
\texttt{ref\_count\_ptr<>\-::operator->()}) must be performed manually
in handle class (which must be programmed an maintained for every
method on every class by some developer).  Handles make the
implementation of linear algebra operations with operator overloading
much easier.  Handles are used extensively in TSF.  Since TSFCore does
not implement operator overloading, handles classify as unnecessary
nonessential but convenient functionality and are therefore not included in TSFCore.

%
\section{Making the most of TSFCore : Adapters}
\label{tsfcore:sec:adapters}
%

To leverage TSFCore to its fullest benefit, TSFCore should be used as the
standard basic set of linear algebra abstracts that forms the basis of
every ANA/LAL and ANA/APP interface.  In addition, every set of
compatible linear algebra interfaces like TSF, HCL and
\textit{AbstractLinAlgPack} should provide adapters to and from TSFCore.
For example, there already exist adapter subclasses that implement the
\textit{AbstractLinAlgPack} interfaces using TSFCore objects
(i.e.~TSFCore-to-\textit{AbstractLinAlgPack}).  There are also adapter
subclasses that implement the TSFCore interfaces using
\textit{AbstractLinAlgPack} objects
(i.e.~\textit{AbstractLinAlgPack}-to-TSFCore).  If these same set of
adapters are also developed for TSF and HCL then scenarios such as the
following are possible.

Consider an advanced transient PDE-constrained optimization problem
where the basic PDE constraints are modeled and discretized (in
space) using Sundance [???].  Sundance uses TSF for all of its linear
algebra needs.  If the adapters from TSF-to-TSFCore and TSFCore-to-HCL are
available, then the adjoint-sensitivity time integrator described in
[???] could be used to compute adjoint-sensitivities for objective and
auxiliary constraint functions.  Then, with HCL-to-TSFCore and
TSFCore-to-\textit{AbstractLinAlgPack} adapters available, these adjoint
sensitivities to could be used in an optimization algorithm in MOOCHO.
In turn, MOOCHO may solve for Newton steps with the Hessian (using a
LBFGS matrix as described in Section \ref{tsfcore:sec:LBFGS}, implemented
using \textit{AbstractLinAlgPack}, as a preconditioner) using an
iterative conjugate gradient method as implemented using TSF as
provided in Trilinos.  This would be easy if adapters for
\textit{AbstractLinAlgPack}-to-TSFCore and TSFCore-to-TSF were implemented.
Without going into any more detail about the above optimization
scenario, it should be clear how the adoption of TSFCore as a standard
basic minimal set of linear algebra interfaces would make such
advanced examples of reuse possible.

%
\section{Summary}
%

Here the basic requirements for linear and nonlinear iterative
abstract numerical algorithms (ANAs) have been defined and a set of
minimal but efficient object-oriented interfaces for linear algebra
called TSFCore has been described.  By adapting TSFCore as a standard
interface layer, interoperability between applications,
linear algebra libraries and ANAs can become a reality.


{\bsinglespace \bibliographystyle{plain} \section{References}
\nopagebreak \bibliography{references} \esinglespace}

\end{document}

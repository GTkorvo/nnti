\documentclass[pdf,ps2pdf,11pt]{SANDreport}
\usepackage{pslatex}

%Local stuff
\usepackage{graphicx}
\usepackage{latexsym}
\input{rab_commands}

% If you want to relax some of the SAND98-0730 requirements, use the "relax"
% option. It adds spaces and boldface in the table of contents, and does not
% force the page layout sizes.
% e.g. \documentclass[relax,12pt]{SANDreport}
%
% You can also use the "strict" option, which applies even more of the
% SAND98-0730 guidelines. It gets rid of section numbers which are often
% useful; e.g. \documentclass[strict]{SANDreport}

% ---------------------------------------------------------------------------- %
%
% Set the title, author, and date
%

\title{
Rythmos: A set of abstract and concrete C++ classes for the solution and
analysis of DAEs and ODEs
}
\author{
Roscoe A. Bartlett \\ Optimization/Uncertainty Estim \\ \\
Todd Coffey \\ Computational Math/Algorithms \\ \\
... \\ \\
Sandia National Laboratories\footnote{
Sandia is a multiprogram laboratory operated by Sandia Corporation, a
Lockheed-Martin Company, for the United States Department of Energy
under Contract DE-AC04-94AL85000.}, Albuquerque NM 87185 USA, \\
}
\date{}

% ---------------------------------------------------------------------------- %
% Set some things we need for SAND reports. These are mandatory
%
\SANDnum{SAND2005-xxxx}
\SANDprintDate{January 12, 2005}
\SANDauthor{
Roscoe A. Bartlett \\ Optimization/Uncertainty Estim \\ \\
Todd Coffey \\ Computational Math/Algorithms \\ \\
... \\ \\
}

% ---------------------------------------------------------------------------- %
% The following definitions are optional. The values shown are the default
% ones provided by SANDreport.cls
%
%\SANDreleaseType{Unlimited Release}
\SANDreleaseType{Not approved for release outside Sandia}

% ---------------------------------------------------------------------------- %
% The following definition does not have a default value and will not
% print anything, if not defined
%
%\SANDsupersed{SAND1901-0001}{January 1901}

% ---------------------------------------------------------------------------- %
%
% Start the document
%
\begin{document}
\raggedright

\maketitle

% ------------------------------------------------------------------------ %
% An Abstract is required for SAND reports
%

%
\begin{abstract}
%
Blah blah blah ...
%
\end{abstract}
%

% ------------------------------------------------------------------------ %
% An Acknowledgement section is optional but important, if someone made
% contributions or helped beyond the normal part of a work assignment.
% Use \section* since we don't want it in the table of context
%
\clearpage
\section*{Acknowledgement}
The authors would like to thank ...

The format of this report is based on information found
in~\cite{Sand98-0730}.

% ------------------------------------------------------------------------ %
% The table of contents and list of figures and tables
% Comment out \listoffigures and \listoftables if there are no
% figures or tables. Make sure this starts on an odd numbered page
%
\clearpage
\tableofcontents
\listoffigures
%\listoftables

% ---------------------------------------------------------------------- %
% An optional preface or Foreword
%\clearpage
%\section{Preface}
%Although muggles usually have only limited experience with
%magic, and many even dispute its existence, it is worthwhile
%to be open minded and explore the possibilities.

% ---------------------------------------------------------------------- %
% An optional executive summary
%\clearpage
%\section{Summary}
%Once a certain level of mistrust and scepticism has
%been overcome, magic finds many uses in todays science
%and engineering. In this report we explain some of the
%fundamental spells and instruments of magic and wizardry. We
%then conclude with a few examples on how they can be used
%in daily activities at national Laboratories.

% ---------------------------------------------------------------------- %
% An optional glossary. We don't want it to be numbered
%\clearpage
%\section*{Nomenclature}
%\addcontentsline{toc}{section}{Nomenclature}
%\begin{itemize}
%\item[alohomora]
%spell to open locked doors and containers
%\end{itemize}

% ---------------------------------------------------------------------- %
% This is where the body of the report begins; usually with an Introduction
%
\setcounter{secnumdepth}{3}
\SANDmain % Start the main part of the report

\section{Introduction}

Here we describe to design of a set of C++ interfaces and concrete
implementations for the solution of a broad class of transient ordinary
differential equations (ODEs) and differential algebraic equations (DAEs) in a
consistent manner.

\section{Requirements}

\begin{enumerate}

{}\item Provide a single interface that applications can implement that allows
for the development of many different types of popular explicit and implicit
initial-value ODE and DAE solvers.

{}\item Exploit as much a possible a common set of software interfaces and
utilities to combine support for ODEs and DAEs together.  Minimize specialized
code and algorithms that apply only to special classes of ODEs and DAEs.

{}\item Provide a single interface where an application can seamlessly select
from one of a number of available time integration methods.

{}\item Provide an automated support framework for direct and adjoint
sensitivity methods for ODEs and DAEs.

{}\item Provide support for general checkpointing of a time integration method
and a uniform way to deal with different time integration methods.

{}\item Require checkpointing of data to result in being able to go back and
continue a computation exactly as if the method would have continued on as
usual.

{}\item As much as possible, build step-size adaptivity strategies independent
from specific time integration methods.

{}\item Present an DAE/ODE interface for applications to implement that
upwardly supports the computation of direct and adjoint sensitivities and
other types of information.

{}\item Use the basic checkpointing capability to implement checkpointing
methods for nonlinear adjoint sensitivity computations in a way that is
independent of specific applications.

{}\item Allow applications detailed control over the time stepping algorithms
to support breakpoints (i.e.\ discontinuities) and other such capabilities.

{}\item Allow basic time stepping algorithm building blocks to be composed
together to build various types of operator splitting methods for complex
physics and multi-physics coupling.

\end{enumerate}

ToDo: Spell out more requirements!

\section{Mathematical formulation for DAE/ODE solvers}
\label{rythmos:scn:mathformulation}

Here we describe the basic mathematical form of a general nonlinear DAE (or
ODE) for the purpose of presenting it to a forward time integrator.  At the
most general level of abstraction, we will consider the solution of fully
implicit DAEs of the form
%
\begin{eqnarray}
f(\dot{x},x,t) & = & 0, \; \mbox{for} \; t \in [t_i, t_f], \label{rythmos:eqn:dae} \\
x(t_i) & = & x_0 \label{rythmos:eqn:dae:ic}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where\hspace{5ex}\= \\
\>	$x\:\in\:\mathcal{X}$ is the vector of differential state variables, \\
\>	$\dot{x} = d(x)/d(t)\:\in\:\mathcal{X}$ is the vector of temporal derivatives of $x$, \\
\>	$t, t_i, t_f\:\in\:\RE$ are the current, initial, and the final times respectively, \\
\>	$f(\dot{x},x,t)\:\in\:\mathcal{X}^2\times\RE\rightarrow\mathcal{F}$ defines the DAE vector function, \\
\>	$\mathcal{X} \:\subseteq\:\RE^{n_x}$ is the vector space of the state variables $x$, and \\
\>	$\mathcal{F} \:\subseteq\:\RE^{n_x}$ is the vector space of the output of the DAE function $f(\ldots)$.
\end{tabbing}

Here we have been careful to define the Hilbert vector spaces $\mathcal{X}$
and $\mathcal{F}$ for the involved vectors and vector functions.  The
relevance of defining these vector spaces is that they come equipped with a
definition of the space's scalar product (i.e.\ $<u,v>_{\mathcal{X}}$ for
$u,v\in\mathcal{X}$) which should be considered and used when designing the
numerical algorithms.

The above general DAE can be specialized to more specific types of problems
based on the nature of Jacobian matrices ${}\partial f / {}\partial
{}\dot{x}\in\mathcal{F}|\mathcal{X}$ and ${}\partial f / {}\partial
{}\dot{x}\in\mathcal{F}|\mathcal{X}$.  Here we use the notation
$\mathcal{F}|\mathcal{X}$ to define a linear operator space that maps vectors
from the space $\mathcal{X}$ to the space $\mathcal{F}$.  Note that the
adjoint of such linear operators are defined in terms of these vector spaces
(i.e.\ $<A u,v>_{\mathcal{F}} = <A^* v,u>_{\mathcal{X}}$ where
$A\in\mathcal{F}|\mathcal{X}$ and $A^*$ denotes the adjoint operator for $A$).
Here we assume that these first derivatives exist for the specific intervals
of time $t\in[t_i,t_f]$ of which such an time integrator algorithm will be
directly applied the the problem.

The precise class of the problem is primarily determined by the nature of the
Jacobian ${}\partial f / {}\partial {}\dot{x}$:
%
\begin{itemize}
%
{}\item ${}\partial f / {}\partial {}\dot{x} {}\in {}\mathcal{F}|\mathcal{X}$ yields an explicit ODE
%
{}\item ${}\partial f / {}\partial {}\dot{x}$ full rank yields an implicit ODE
%
{}\item ${}\partial f / {}\partial {}\dot{x}$ rank deficient yields a general
DAE.
\end{itemize}
%
In addition, the ODE/DAE may linear or nonlinear and DAEs are classified by
their index \cite{BCP}.  It is expected that a DAE will be able to tell a time
integrator what type of problem it is (i.e.\ explicit ODE, implicit ODE,
general DAE) and which, if any, of the variables are linear in the problem.
This type of information can be exploited in a time integration algorithm.

For each transient computation, the formulation will be cast into the general
form in (\ref{rythmos:eqn:dae})--(\ref{rythmos:eqn:dae:ic}) to demonstrate how
this general formulation can be exploited in many different contexts.

\section{Mathematics of Basic Time Integration Strategies}

In this section we show how time steps are formulated and computed for a
variety popular explicit and implicit time integration methods.  In general,
all of these time integration methods are based on some basic smoothness
assumptions of the underlying DAE/ODE but the details of the theoretical
background for this methods will not be covered here.

We first consider explicit methods followed by implicit methods.

\subsection{Formulation for Explicit Time Steppers for ODEs}

Explicit integration methods are primarily only attractive for explicit and
implicit ODEs but some classes of DAEs can be considered as well (i.e.\ by
nonlinearly eliminating the algebraic variables using the algebraic equations
in a semi-explicit DAE \cite{BCP}).  For this discussion, we will also assume
that the DAE has been written in the explicit ODE form (i.e.\ ${}\partial f /
{}\partial {}\dot{x} = I$).  Note that implicit ODEs can always be written as
explicit ODEs by multiplying the implicit ODE from the right with $({}\partial
f / {}\partial {}\dot{x})^{-1}$ as:
%
\begin{eqnarray*}
f(\dot{x},x,t) & = & 0 \\
& \Rightarrow \\
\frac{\partial f}{\partial \dot{x}} \dot{x} + \hat{f}(x,t) & = & 0 \\
& \Rightarrow \\
\left( \frac{\partial f}{\partial \dot{x}} \right)^{-1}
\left( \frac{\partial f}{\partial \dot{x}} \dot{x} + \hat{f}(x,t) \right) & = & 0 \\
& \Rightarrow \\
\dot{x} & = & -\left( \frac{\partial f}{\partial \dot{x}} \right)^{-1} \hat{f}(x,t) \\
& = & \bar{f}(x,t) \\
\end{eqnarray*}
%
where ${}\dot{x} = \bar{f}(x,t)$ is the new explicit form of the ODE that is
considered by the explicit time integration strategies below.

The above transformation of course requires that the matrix $({}\partial f /
{}\partial {}\dot{x})^{-1}$ be fairly easily invertible.

Below, we describe some popular explicit integration methods for ODEs.

\subsubsection{Explicit Runge-Kutta methods}

\subsubsection{Explicit multi-step methods}

\subsubsection{Explicit ??? methods}

\subsection{Formulation for Implicit Time Steppers for ODEs and DAEs}

Here we consider several different classes of implicit time stepping methods.
For each class of method we show the set of general nonlinear equations that
defines a single time step and then show how a linearized form of the
equations may be formed to be solved by a Newton-type nonlinear equation
solver.

In particular, for each method, we will show how to define a set of nonlinear
equations of the form
%
\begin{equation}
r(z) = 0
\label{rythmos:eqn:r}
\end{equation}
%
such that when solved will define an implicit time step from $t_k$ to
$t_{k+1}$, where $\Delta t = t_{k+1} - t_k$ denotes the time-step.  In
addition, for each method, we will show how to use the DAE residual evaluation
$(\dot{x},x,t) {}\rightarrow f$ to define the nonlinear time step equation
(\ref{rythmos:eqn:r}).  At the highest level, the time step method only
requires very general convergence criteria for the time step equation
(\ref{rythmos:eqn:r}) and therefore great flexibility is allowed in how the
time step equation is solved.

Even through the time step equation can be solved by a variety of means, a
large class of DAEs/ODEs can also potentially provide support for a general
Newton-type method for solving these equations and can therefore leverage
general software for solving such problems (e.g.\ NOX).  The foundation of
Newton-like methods is the ability to solve linear systems similar to the
Newton system
%
\begin{equation}
\frac{\partial r}{\partial z} \Delta z = - r(z_l)
\end{equation}
%
where $z_l$ is the current candidate solution of the nonlinear equations
(which also defines the point where $\partial r / {}\partial z$ is evaluated)
and $\Delta z = r_{l+1} - r_l$ is the update direction.  Line-search Newton
methods then define an update to the solution along the direction $\Delta z$.
The essential functionality needed to perform a Newton-like method are the the
abilities to evaluate the nonlinear residual $z {}\rightarrow r$ and to
(approximately) solve linear systems involving the Jacobian matrix ${}\partial
r / {}\partial z$.  For each type of implicit time integration method, we will
show, if possible, how to perform solves with ${}\partial r / {}\partial z$ by
performing solves with the matrix
%
\begin{equation}
M = \alpha \frac{\partial f}{\partial \dot{x}} + \frac{\partial f}{\partial x},
\label{rythmos:eqn:M}
\end{equation}
%
evaluated at points $(\dot{x},x,t)$ selected by the time integration method
and where $\alpha$ is some constant also defined by the time integration
method.  Note that the matrix $M$ above in (\ref{rythmos:eqn:M_bdf}) may not
necessarily exactly represent ${}\partial r / {}\partial z$ and $z$ and $r$
may not simply lie in the vector spaces $\mathcal{X}$ and $\mathcal{F}$
respectively; but in many cases, they will.

\subsubsection{Implicit multi-step BDF methods}

The first class of methods that we will consider are so called multi-step
backward difference formula (BDF) methods.  In these methods, an $p$-step
approximation for the time derivative $\dot{x}$ for the new time point
$t_{k+1}$ is formed based on a linear combination of state values $y_{k+1-j}$
for next, current and previous time values $t_{k+1-j}$, for $j = 0 {}\ldots p$
and the form of the approximation is
%
\begin{equation}
\dot{x}_{k+1} = \frac{1}{\Delta t} \sum_{j=0}^{p} \beta_j \: x_{k+1-j}
\label{rythmos:eqn:bdf_x_dot}
\end{equation}
%
where $\Delta t = t_{k+1} - t_k$ and $\beta_j$, for $j=0 {}\ldots p$, are the
BDF method coefficients \cite{AscherPetzold}.

The nonlinear time step equation to advance the solution from $t_k$ to
$t_{k+1}$ is then formed by substituting $\dot{x} = \dot{x}_{k+1}$ in
(\ref{rythmos:eqn:bdf_x_dot}), $x = x_{k+1}$ and $t = t_{k+1}$ into
(\ref{rythmos:eqn:dae}) to obtain
%
\begin{equation}
f\left( \frac{1}{\Delta t} \left[ \beta_0 x_{k+1} + \sum_{j=1}^{p} \beta_j \: x_{k+1-j} \right],x_{k+1},t_{k+1}\right) = 0.
\label{rythmos:eqn:bdf_dae_ne}
\end{equation}
%
One can immediately identify the BDF time step equations
(\ref{rythmos:eqn:bdf_dae_ne}) with the general form of the time step
equations (\ref{rythmos:eqn:r}) and with unknown solution variables $z =
x_{k+1}$.  All of the other state variables $x_{k+1-j}$, for $j = 1 {}\ldots
p$, are given.

Note that the first-order BDF method with $p=1$, $\beta_0 = 1$ and $\beta_1 =
-1$ is simply the standard backward Euler time integration method \cite{AscherPetzold}.

When considering a general Newton-like method for solving
(\ref{rythmos:eqn:bdf_dae_ne}), note that the Newton Jacobian of these
equations is
%
\begin{equation}
\frac{\partial r}{\partial z}
= \frac{\beta_0}{\Delta t} \frac{\partial f}{\partial \dot{x}} + \frac{\partial f}{\partial x},
\label{rythmos:eqn:M_bdf}
\end{equation}
%
which is evaluated at the point $\dot{x}$ in (\ref{rythmos:eqn:bdf_x_dot}), $x
= x_{k+1}$ and $t = t_{k+1}$.  One can immediately identify
(\ref{rythmos:eqn:M_bdf}) with the general form of the matrix $M$ in
(\ref{rythmos:eqn:M}) where $\alpha = \beta_0 / \Delta t$.  Note that the
Jacobian (\ref{rythmos:eqn:M_bdf}) is the exact Jacobian for the nonlinear
time step equations; this will not be true for some of the other methods.

\subsubsection{Generalized Theta methods}

The next fairly straightforward class of methods are the so called $\theta$
(Theta) methods \cite{HairerWanner}.  Theta methods define a time step equation by requiring
enforcement of the DAE equation at an intermediate time in the range
$t_{\theta} {}\in [t_k,t_{k+1}]$ where $t_{\theta} = t_k + \theta ( t_{k+1} -
t_k )$ and $\theta {}\in [0,1]$.  Then, Theta-averaged values for $x$ and $t$
are then used to form the implicit time step equation
%
\begin{equation}
f\left( \frac{x_{k+1} - x_{k}}{\Delta t},x_k + \theta ( x_{k+1} - x_{k} ), t_k + \theta \Delta t \right) = 0
\label{rythmos:eqn:theta_dae_ne}
\end{equation}
%
which is then solved for the unknown state $x_{k+1}$.  The values of $\theta =
0$, $\myonehalf$ and $1$ give the well known forward Euler, midpoint
rule, and backward Euler methods \cite{HairerWanner}.

{}\textbf{ToDo: Todd, is this correct?  I could not find a specific reference
for a general DAE but this seems not to be consistent with the midpoint rule
for ODEs and described in [Burden].}

\subsubsection{Trapezoidal Methods}

The next class of methods that we consider are Trapezoidal methods.
Trapezoidal methods are essentially equivalent to the trapezoidal rule for
numerical quadrature (i.e.\ integrating under a curve) and takes the form of
the averaging of the DAE at $t_k$ and $t_{k+1}$ as
%
\begin{equation}
\frac{1}{2} \left[
f\left( \frac{x_{k+1} - x_{k}}{\Delta t}, x_{k+1} , t_{k+1} \right)
+ f\left( \frac{x_{k+1} - x_{k}}{\Delta t}, x_k , t_k \right)
\right]
 = 0
\label{rythmos:eqn:trap_dae_ne}
\end{equation}
%
which defines a set of nonlinear equations that are solved for the unknown
state variables $x_{k+1}$.  The straightforward Newton Jacobian for the
equations in (\ref{rythmos:eqn:trap_dae_ne}) is
%
\begin{equation}
\frac{\partial r}{\partial z}
= \frac{1}{\Delta t} \left[
\left( \frac{\partial f}{\partial \dot{x}} \right)_{k+1}
+ \left( \frac{\partial f}{\partial \dot{x}} \right)_{k}
\right]
+ \left( \frac{\partial f}{\partial x} \right)_{k+1},
\label{rythmos:eqn:drdz_trap}
\end{equation}
%
where we use the shorthand notation of $(\ldots)_{k+1}$ and $(\ldots)_k$ to
mean that an object is evaluated at the points $(\dot{x},x,t) =
((x_{k+1}-x_k)/\Delta t, x_{k+1}, t_{k+1})$ and $(\dot{x},x,t) =
((x_{k+1}-x_k)/\Delta t, x_k, t_k)$ respectively.  Note that the form of the
Jacobian in (\ref{rythmos:eqn:drdz_trap}) does not match the form of
(\ref{rythmos:eqn:M}) that we are trying to utilize.  However, a common trick
in these types of methods to simply replace $x_{k+1}$ and $t_{k+1}$ in the
second and third arguments of $((x_{k+1}-x_k)/\Delta t, x_{k+1}, t_{k+1})$
with $x_k$ and $t_k$ in the Jacobian which gives the approximate Jacobian
%
\begin{equation}
M
= \frac{2}{\Delta t} \frac{\partial f}{\partial \dot{x}}
+ \frac{\partial f}{\partial x},
\label{rythmos:eqn:trap_M}
\end{equation}
%
where now all of these terms are evaluated at the point $(\dot{x},x,t) =
((x_{k+1}-x_k)/\Delta t, x_k, t_k)$.  It is easy to see that this is the same
form as the Jacobian in (\ref{rythmos:eqn:M}) where $\alpha = 2/\Delta t$.
The Jacobian in (\ref{rythmos:eqn:trap_M}) is not an exact Jacobian for the
equations in (\ref{rythmos:eqn:trap_dae_ne}) but can produce good Newton steps
in many cases.  However, an inexact Jacobian such as this can cause a problem
when sensitivity computations are being performed (see Section \ref{rythmos:scn:transientsensitivity}).

\subsubsection{Generalized Alpha methods}

\cite{GeneralizedAlpha}
{}\textbf{Todd: Can you fill this in?}

\subsubsection{Implicit Runge-Kutta methods}

We now consider a class of powerful and popular one-step methods for solving
implicit DAEs, implicit Runge-Kutta (RK) methods.  The most general form
of implicit RK methods requires the simultaneous solution of $p$ sets
of coupled nonlinear equations that take the form
%
\begin{equation}
r_i(z) = f\left( \dot{x}_i, x_k + \Delta t \sum_{j=1}^{p} a_{ij} \dot{x}_j,
t_k + c_i \Delta t \right) = 0,
\; \mbox{for} \; i = 1 \ldots p
\label{rythmos:eqn:irk_dae_ne}
\end{equation}
%
where $\dot{x}_i$ are essentially approximations to the derivatives
$\dot{x}(t_k + c_i \Delta t)$ called {}\textit{stage derivatives} and $z = [
{}\dot{x}_1, {}\dot{x}_1, {}\ldots, {}\dot{x}_p ]^T$ are the unknowns in this
set of equations.  After this set of coupled equations is solved, the state
solution $x_{k+1}$ is given as the linear combination
%
\begin{equation}
x_{k+1} = x_k + \Delta t \sum_{i=1}^{p} b_i \dot{x}_j.
\end{equation}

Implicit RK methods are classified by both the order $p$ and the selection of
the constants $a_{ij}$, $c_i$ and $b_i$.  It is customary to consider these
constants in the form of the {}\textit{Butcher diagram}
%
\[
\begin{array}{c|c}
c & A \\
\hline
  & b^T
\end{array}
\; \; = \; \;
\begin{array}{c|cccc}
c_1 & a_{11} & a_{12} & \cdots & a_{1p} \\
c_2 & a_{21} & a_{22} & \cdots & a_{2p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
c_p & a_{p1} & a_{p2} & \cdots & a_{pp} \\
\hline
 & b_1 & b_2 & \cdots & b_p
\end{array}
\]
%
where $A = [a_{ij}]$, $b = [b_1, b_2, {}\ldots, b_p]^T$, and $c =
[c_1, c_2, {}\ldots, c_p]^T$.

It is clear how to form the residual for the fully coupled system for $r(z)$ in
(\ref{rythmos:eqn:irk_dae_ne}) just from individual evaluations $(\dot{x},x,t)
{}\rightarrow f$ but how the Newton system for such a system is solved will
vary greatly based on the structure and properties of the Butcher matrix $A$.

\subsubsection*{Fully implicit IRK methods}

Fully implicit IRK methods present somewhat of a problem for developing
general software since they involve the need to solve a fully coupled system
of $p$ sets of equations of the form (\ref{rythmos:eqn:irk_dae_ne}).  Each
block ${}\partial r_i / {}\partial z_j = {}\partial r_i / {}\partial
{}\dot{x}_j$ of the full Jacobian ${}\partial r / {}\partial z$ is represented
as
%
\begin{equation}
\frac{\partial r_i}{\partial \dot{x}_j}
= \frac{\partial f}{\partial \dot{x}}
+ \Delta t a_{ij} \frac{\partial f}{\partial x},
\; \mbox{for} \; i = 1 \ldots p, \; j = 1 \ldots p
\label{rythmos:eqn:dridzi_trap}
\end{equation}
%
which is evaluated at the points $(\dot{x},x,t) = ( {}\dot{x}_i, x_k +
{}\Delta t {}\sum_{j=1}^{p} a_{ij} {}\dot{x}_j, t_k + c_i {}\Delta t )$.  Note
that ${}\partial r_i / {}\partial z_j = {}\partial r_i / {}\partial
{}\dot{x}_j$ in (\ref{rythmos:eqn:dridzi_trap}) is of the same form as
(\ref{rythmos:eqn:M}) where $\alpha = 1 / (\Delta t a_{ij})$ when scaling by
$1 / (\Delta t a_{ij})$.

When considering a iterative method for solving systems with the block
operator matrix ${}\partial r / {}\partial z$, it is easy to see how to use
the general matrix $M$ in (\ref{rythmos:eqn:M}) to implement a matrix-vector
product, but it is not obvious how to precondition such a system.  Clearly a
block diagonal preconditioner could be used but the effectiveness of such a
preconditioning strategy is open to question.  Other preconditioning strategies
are also possible just given the basic block operators and this is an open
area of research.

In some cases, however, it may be possible to form a full matrix object for
${}\partial r / {}\partial z$ but this is not something that can be expected
for most applications.

\subsubsection*{Semi-explicit IRK methods}

Semi-explicit IRK methods are those IRK methods where the Butcher matrix $A$
is lower diagonal and therefore gives rise to a block lower diagonal Jacobian
matrix ${}\partial r / {}\partial z$.  For these types of methods, the
nonlinear equations in (\ref{rythmos:eqn:dridzi_trap}) can be solved one at a
time for $i = 1 {}\ldots p$ which is easily accommodated using a Newton-type
method where the Newton Jacobian for each $i$ is given by
(\ref{rythmos:eqn:dridzi_trap}) which is of our basic general form
(\ref{rythmos:eqn:M}).

\subsubsection*{Diagonally-implicit IRK methods}

The next specialized class of IRK methods that we consider are
diagonally-implicit IRK methods where the Butcher coefficients in $A$ and $c$
give rise to a lower triangular Jacobian ${}\partial r / {}\partial z$ (and
hence are also semi-explicit IRK methods) that has the same nonsingular matrix
block of the form in (\ref{rythmos:eqn:dridzi_trap}) along the diagonal.
This, of course, requires that $a_{11} = a_{22} = {}\ldots = a_{pp}$ and
$c_{1} = c_{2} = {}\ldots = c_{p}$.  In this class of IRK methods, significant
savings may be achieved since a single set of linear solver objects (i.e.\
operators and preconditioners) can be utilized for the solution of the fully
coupled system.  In fact, it may even be possible to utilize multi-vector
applications of the operator and preconditioner for matrices of the form
(\ref{rythmos:eqn:M}) which can be supported by many applications.

{}\textbf{ToDo: I need to write up some more ideas here to see how to take
advantage of multi-vector operations when possible and if it makes since to do
so.}

\subsubsection{Discontinuous Galerkin in time}

\cite{DGTime}
{}\textbf{Todd: Scott Collis mentioned this a something that can be important
from a variety of standpoints but I don't have any references for this yet.}

\subsubsection{Finite-elements in time }

\cite{FETime}
{}\textbf{Todd: Scott Collis mentioned this a something that can be important
from a variety of standpoints but I don't have any references for this yet.}

\subsubsection{Implicit ??? methods}

{}\textbf{Todd: Are there any other implicit methods here?}

%\subsection{General Solution Stragegies for Implicit Nonlinear Time Step Equations}

\section{Mathematical Formation for ODEs/DAEs for Applications}

The form of the DAE described in Section \ref{rythmos:scn:mathformulation} relates directly to what is
needed for the implementation of explicit and implicit time integration
methods.  In this section, we will present a form of the the DAE that is most
directly related to what applications must provide, especially as it relates
to higher-level computations like transient direct and adjoint sensitivity
computations.  Here we will consider application DAE formulations that are
required for sensitivity computations but are also related to other types of
problems such as coupling multiple physics using operator split methods.

The basic application model that we consider here takes the form
%
\begin{eqnarray}
c\left( \dot{y}(t), y(t), p, v(t), t \right) & = & 0,
\; t \in \left[ t_i, t_f \right] \label{rythmos:eqn:app:c} \\
y(0) & = & y_0(p) \label{rythmos:eqn:app:c:ic}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$y(t) \:\in\:\mathcal{Y}$ are the transient state variables, \\
\>	$\dot{y}(t) = d(y)/d(t)\:\in\:\mathcal{Y}$ is the vector of temporal derivatives of differential state variables $y(t)$, \\
\>	$p \:\in\:\mathcal{P}$ are steady-state auxiliary variables, \\
\>	$v(t) \:\in\:\mathcal{V}$ are transient auxiliary variables, \\
\>	$c(\dot{y}(t), y(t), p, v(t), t) :
		\mathcal{Y}^2 \times \mathcal{P} \times \mathcal{V} \times \RE
		\rightarrow \mathcal{C}$ are the state DAEs, \\
\>	$y_0(p) \:\in\:\mathcal{P} \rightarrow \mathcal{Y}$ define the initial conditions for $y(0)$ as a function of $p$, \\
\>	$\mathcal{Y} \:\subseteq\:\RE^{n_y}$ is the vector space of the state variables $y(t)$, \\
\>	$\mathcal{P} \:\subseteq\:\RE^{n_p}$ is the vector space of the steady-state auxiliary variables $p$, \\
\>	$\mathcal{V} \:\subseteq\:\RE^{n_v}$ is the vector space of the transient auxiliary variables $v(t)$, and \\
\>	$\mathcal{C} \:\subseteq\:\RE^{n_y}$ is the vector space of the output of the DAE function $c(\ldots)$.
\end{tabbing}

The auxiliary variables $p$ and $v(t)$ in (\ref{rythmos:eqn:app:c}) represent
adjustable variables that can be manipulated for some purpose and, once
selected, determine a fully specified set of DAEs that can be solved forward
in time for the state variables $y(t)$.  The form of the DAE in
(\ref{rythmos:eqn:app:c}), which has auxiliary variables, arises in a number
of contexts such as direct sensitivity computations (see Section \ref{rythmos:scn:transientsensitivity:direct}) and in
multi-disciplinary (i.e.\ multi-physics) coupling.

In many situations, quantities of interest or response functions are important
in various types of analyzes such a sensitivity computations for uncertainty
quantification, optimization and error estimation.  For transient problems of
this type, auxiliary response functions can be formulated by adding a set of
auxiliary differential variables and DAEs
%
\begin{eqnarray}
h\left( \frac{\partial w(t)}{\partial t}, w(t), y(t), p, v(t), t \right) & = & 0,
\; t \in \left[ t_i, t_f \right] \label{rythmos:eqn:app:h} \\
w(0) & = & w_0(p) \label{rythmos:eqn:app:h:ic}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$w(t) \:\in\:\mathcal{W}$ are auxiliary differential differential variables, \\
\>	$\dot{w}(t) = d(w)/d(t)\:\in\:\mathcal{W}$ is the vector of temporal derivatives of $w(t)$, \\
\>	$h(\dot{w}(t), w(t), y(t), u, v(t), t) : \:
 		\mathcal{W} \times \mathcal{W} \times \mathcal{Y} \times \mathcal{P} \times \mathcal{V} \times \RE
		\rightarrow \mathcal{H}$ are the auxiliary DAEs, \\
\>	$w_0(p) \:\in\:\mathcal{P} \rightarrow \mathcal{W}$ define the initial conditions for $w(0)$ as a function of $p$, \\
\>	$\mathcal{W} \:\subseteq\:\RE^{n_w}$ is the vector space of the transient auxiliary differential state variables $w(t)$, and \\
\>	$\mathcal{H} \:\subseteq\:\RE^{n_w}$ is the vector space of the output of the DAE function $h(\ldots)$.
\end{tabbing}
%
and a set of auxiliary response, or observable, functions
%
\begin{equation}
g(y(t_f),w(t_f),p)
\label{rythmos:eqn:app:g}
\end{equation}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$g(y(t_f),w(t_f),p) : \:
		\mathcal{Y} \times \mathcal{W} \times \mathcal{P}
		\rightarrow \mathcal{G}$ are the auxiliary response functions, and \\ 
\>	$\mathcal{G} \:\subseteq\:\RE^{n_g}$ is the vector space of the auxiliary response functions $g(\ldots)$.
\end{tabbing}

The purpose of the auxiliary DAEs in
(\ref{rythmos:eqn:app:h})--(\ref{rythmos:eqn:app:h:ic}) are typical and the
allow the accumulation of some set of integrated quantities needed to evaluate
the auxiliary response functions in (\ref{rythmos:eqn:app:g}).  The
formulation of these equations is very general and covers many different types
of applications.

\subsection{Implicit state functions and reduced auxiliary response functions}

The state DAEs in (\ref{rythmos:eqn:app:c})--(\ref{rythmos:eqn:app:c:ic}) and
(\ref{rythmos:eqn:app:h})--(\ref{rythmos:eqn:app:h:ic}) may be used to define
the implicit state functions $y(p,v(t_i:t_f))$ and $w(p,v(t_i:t_f))$,
respectively, where $v(t_i:t_f)$ is short hand for the selection of $v(t)$ (or
a discrete approximation in a practical implementation) in the range
$t\in[t_i,t_f]$.  Given these implicit state functions, a reduced set of
auxiliary response functions is defined as
%
\begin{equation}
\hat{g}(p,v(t_i:t_f)) = g(y(p,v(t_i:t_f)),w(p,v(t_i:t_f)),p).
\label{rythmos:eqn:app:g_hat}
\end{equation}

Note that the input $v(t_i:t_f)$ is really an infinite dimensional
input in general but in practice will be a finite-dimensional discretization.

\subsection{Combined abstract state and auxiliary response functions}

Note that the state equations and differential variables in
(\ref{rythmos:eqn:app:c})--(\ref{rythmos:eqn:app:c:ic}) and
(\ref{rythmos:eqn:app:h})--(\ref{rythmos:eqn:app:h:ic}) and can be combined
into a single set of equations and differential variables and thereby give
the more abstract problem
%
\begin{eqnarray}
\tilde{c}\left( \dot{\tilde{y}}(t), \tilde{y}(t), p, v(t), t \right) & = & 0,
\; t \in \left[ t_t, t_f \right] \label{rythmos:eqn:app:c_tilde} \\
\tilde{y}(0) & = & \tilde{y}_0(u) \label{rythmos:eqn:app:c_tilde:ic}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$\tilde{y}(t) = {\bmat{c} y(t) \\ w(t) \emat} \:\in\:\tilde{\mathcal{Y}}$ are the combined (product) transient state variables, \\
\>	$\dot{\tilde{y}}(t) = d(\tilde{y})/d(t)\:\in\:\mathcal{X}$ is the vector of temporal derivatives of $\tilde{y}(t)$, \\
\>	$\tilde{c}\left( \dot{\tilde{y}}(t), \tilde{y}(t), p, v(t), t \right) =
		{\bmat{c} c(\dot{y}(t),y(t),u,v(t),t) \\ h(\dot{w}(t),w(t),y(t),u,v(t),t) \emat}
		: \tilde{\mathcal{Y}}^2 \times \mathcal{P} \times \mathcal{V} \times \RE \rightarrow \tilde{C}$ \\
\>	\hspace{4ex} are the combined (product) state DAE function, \\
\>	$\tilde{\mathcal{Y}} = \mathcal{Y} \times \mathcal{W}$ is the combined (product) vector space of the combined state variables $\tilde{y}(t)$, and \\
\>	$\tilde{\mathcal{C}} = \mathcal{C} \times \mathcal{H}$ is the combined (product) vector space of the combined DAE $\tilde{c}(\ldots)$.
\end{tabbing}
%
with the auxiliary response functions
%
%
\begin{equation}
\tilde{g}(\tilde{y}(t_f),p)
\label{rythmos:eqn:app:g_tilde}
\end{equation}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$\tilde{g}(\tilde{y}(t_f),p) = g(y(t_f),w(t_f),p) : \:
		\tilde{\mathcal{Y}} \times \mathcal{P}
		\rightarrow \mathcal{G}$.
\end{tabbing}

Note that (\ref{rythmos:eqn:app:c_tilde})--(\ref{rythmos:eqn:app:g_tilde})
defines an implicit state solution $\tilde{y}(p,v(t_i:t_f))$ and the reduced auxiliary
response function
%
\begin{equation}
\hat{g}(p,v(t_i:t_f)) = \tilde{g}(\tilde{y}(u,v(t_i:t_f)),p).
\label{rythmos:eqn:app:g_hat_combined}
\end{equation}
%
Note that when considering the reduced auxiliary response function that
$\hat{g}(u,v(t_i:t_f))$ in (\ref{rythmos:eqn:app:g_hat_combined}) is exactly
the same function as $\hat{g}(u,v(t_i:t_f))$ in (\ref{rythmos:eqn:app:g_hat}).

In some cases, it will be the most convenient to work with the combined DAE
and auxiliary response functions in
(\ref{rythmos:eqn:app:c_tilde})--(\ref{rythmos:eqn:app:g_tilde}) while in in
other situations it will be most clear to work with the original state and
auxiliary DAEs in (\ref{rythmos:eqn:app:c})--(\ref{rythmos:eqn:app:h:ic}) and
response functions (\ref{rythmos:eqn:app:g}).  For example, mathematical
derivations for various sensitivity computations are most readily followed
using the more abstract form in
(\ref{rythmos:eqn:app:c_tilde})--(\ref{rythmos:eqn:app:g_tilde}) while
describing requirements from applications is most straightforward using the
lower-level formulation in
(\ref{rythmos:eqn:app:c})--(\ref{rythmos:eqn:app:g}).

When the auxiliary variables $p$ and $v(t_i,t_f)$ are given, the combined DAE
in (\ref{rythmos:eqn:app:c_tilde})--(\ref{rythmos:eqn:app:c_tilde:ic}) takes
exactly the same form as (\ref{rythmos:eqn:dae}) with $x = \tilde{y}$ and
$f(\ldots) = \tilde{c}(\ldots)$ so that it can be presented to a general time
integration algorithm.  The form of the Jacobian $M$ in (\ref{rythmos:eqn:M})
for this problem is
%
\begin{eqnarray}
\alpha \frac{\partial f}{\partial \dot{x}} + \frac{\partial f}{\partial x}
& = & \alpha \frac{\partial \tilde{c}}{\partial \dot{\tilde{y}}} + \frac{\partial \tilde{c}}{\partial \tilde{y}} \nonumber \\
& = & \alpha {\bmat{cc}
          \frac{\partial c}{\partial \dot{y}} & 0 \\
          \frac{\partial h}{\partial \dot{y}} & \frac{\partial h}{\partial \dot{w}}
       \emat}
       + {\bmat{cc}
          \frac{\partial c}{\partial y} & 0 \\
          \frac{\partial h}{\partial y} & \frac{\partial h}{\partial  w}
       \emat} \nonumber \\
& = & {\bmat{cc}
      \alpha \frac{\partial c}{\partial \dot{y}} + \frac{\partial c}{\partial y}
      & 0 \\
      \alpha \frac{\partial h}{\partial \dot{y}} + \frac{\partial h}{\partial y}
      & \alpha \frac{\partial h}{\partial \dot{w}} + \frac{\partial h}{\partial w}
       \emat} \label{rythmos:eqn:c_tilde_jac}
\end{eqnarray}

The matrix (\ref{rythmos:eqn:c_tilde_jac}) is block lower triangular and is
easily used to solve linear systems by exploiting solves with the individual
blocks along the diagonal.  In other words, the functionality from each set of
DAEs can be used when solving the combined system.  This should not be
surprising given the nature of these DAEs (i.e.\ the original DAEs are
independent of the auxiliary DAEs).

\section{Transient Sensitivity Computations}
\label{rythmos:scn:transientsensitivity}

In this section we describe the basics of direct and adjoint sensitivity
computations.  The results from these computations are useful in a variety of
advanced analysis algorithms.  The goal of this section is to state the
relevant equations and expressions and to show what applications need to
provide in order to support such computations.

Below, we will consider direct and adjoint approaches to the computation of
the first derivative object
%
\begin{equation}
\frac{\partial \hat{g}}{\partial p} \in \mathcal{G} \times \mathcal{P}
\label{rythmos:eqn:d_g_hat_d_p}
\end{equation}
%
and adjoint approaches for the computation of
%
\begin{equation}
\frac{\partial \hat{g}}{\partial v(t)} \in \mathcal{G} \times \mathcal{V}, \; t \in [t_i,t_f].
\label{rythmos:eqn:d_g_hat_d_v}
\end{equation}
%
Note that it is generally not computationally tractable to compute even a
course discrete approximation to the first derivative object
(\ref{rythmos:eqn:d_g_hat_d_v}) involving the transient auxiliary variables
$v(t)$ because of the large discretized dimension of $v(t_i:t_f)$.

\subsection{Transient Direct Sensitivity Solvers}
\label{rythmos:scn:transientsensitivity:direct}

In this section we consider the solution of direct sensitivity equations to
compute the first derivative object ${}\partial {}\hat{g} / {}\partial p$ in
(\ref{rythmos:eqn:d_g_hat_d_p}).  In this section, we will ignore the presence
of transient auxiliary variables $v(t)$ and will work with the combined DAE
model in (\ref{rythmos:eqn:app:c_tilde})--(\ref{rythmos:eqn:app:g_tilde}).
From (\ref{rythmos:eqn:app:g_hat_combined}) it is easy to see that
%
\begin{equation}
\frac{\partial \hat{g}}{\partial p}
= \frac{\partial \tilde{g}}{\partial \tilde{y}(t_f)} \frac{\partial \tilde{y}(t_f)}{\partial p} 
+ \frac{\partial \tilde{g}}{\partial p}
\label{rythmos:eqn:app:g_hat_direct}
\end{equation}
%
where $\tilde{y}(t_f)$ is shorthand for the implicit state function for the
solution of
(\ref{rythmos:eqn:app:c_tilde})--(\ref{rythmos:eqn:app:c_tilde:ic}) at the
final time $t_f$, which is a function of $p$.

The final partial derivatives $\partial \tilde{y}(t_f) / {}\partial p$ are
known as the {}\textit{direct sensitivities} and are independent of any
particular choice of the auxiliary response functions $\tilde{g}(\ldots)$.
In fact, these direct sensitivities $\partial \tilde{y}(t_f) / {}\partial p$
can be useful in their own right in some types of analysis (see [???]).

Solving for $\partial \tilde{y}(t_f) / {}\partial p$ requires solving the
following forward direct sensitivity equations
%
\begin{eqnarray}
%
\frac{\partial \tilde{c}}{\partial \dot{\tilde{y}}} \left(\frac{\partial}{\partial t} \frac{\partial \tilde{y}(t)}{\partial p} \right)
+ \frac{\partial \tilde{c}}{\partial y} \left(\frac{\partial \tilde{y}(t)}{\partial p}\right)
+ \frac{\partial \tilde{c}}{\partial p} & = & 0, \; t \in \left[ t_i, t_f \right], \label{rythmos:eqn:app:direct-c_tilde} \\
\frac{\partial y(t_i)}{\partial p} & = & \frac{\partial y_0}{\partial p}. \label{rythmos:eqn:app:direct-c_tilde:ic}
\end{eqnarray}
%
These sensitivity equations are easily derived by simply differentiating
(\ref{rythmos:eqn:app:c_tilde})--(\ref{rythmos:eqn:app:c_tilde:ic}) with
respect to each parameter $p_i$, for $i = 1 {}\ldots n_p$, and then combining
each of the columns in the multi-vector form show above.

The state sensitivity DAEs
(\ref{rythmos:eqn:app:direct-c_tilde})--(\ref{rythmos:eqn:app:direct-c_tilde:ic})
are integrated together with the state DAEs
(\ref{rythmos:eqn:app:c_tilde})--(\ref{rythmos:eqn:app:c_tilde:ic}).  Because
all of these DAE equations are integrated together, only local time-step
storage is required.  Note that the differential variables ${}\partial
\tilde{y}(t) / {}\partial p$ appear linearly in the equations and the
direct sensitivity DAEs are linear DAEs.  The dominant cost of the sensitivity
computation is the $n_p$ independent DAEs represented by in
(\ref{rythmos:eqn:app:direct-c_tilde})--(\ref{rythmos:eqn:app:direct-c_tilde:ic})
that must be solved.  It is this set of equations that gives the direct
approach its $O(n_p)$ complexity.

Once the direct sensitivity equations are solved for ${}\partial \tilde{y}(t)
/ {}\partial p$, then $\partial {}\hat{g} / {}\partial p$ in
(\ref{rythmos:eqn:app:g_hat_direct}) is easily evaluated using the derivatives
$\partial \tilde{g} / {}\partial \tilde{y}(t_f)$ and $\partial \tilde{g} /
{}\partial p$, or approximated using directional finite differences of
$\tilde{g}(\ldots)$.

The direct sensitivity equations can be presented to an implicit time
integrator in the form
(\ref{rythmos:eqn:dae})--(\ref{rythmos:eqn:dae:ic}) by simply
defining the composite state vector
%
\begin{equation}
x =
{\bmat{c}
\tilde{y} \\
\frac{\partial \tilde{y}}{\partial p_1} \\ \vdots \\ \frac{\partial \tilde{y}}{\partial p_{n_u}}
\emat}
\in \mathcal{Y}^{n_p+1}
\end{equation}
%
and then defining a similar composite DAE function
%
\begin{equation}
f(\dot{x},x,t) =
{\bmat{c}
%
\tilde{c}\left( \dot{\tilde{y}}(t), \tilde{y}(t), p, v(t), t \right) \\
%
\frac{\partial \tilde{c}}{\partial \dot{\tilde{y}}} \left(\frac{\partial}{\partial t} \frac{\partial \tilde{y}(t)}{\partial p_1} \right)
+ \frac{\partial \tilde{c}}{\partial y} \left(\frac{\partial \tilde{y}(t)}{\partial p_1}\right)
+ \frac{\partial \tilde{c}}{\partial p_1} \\
%
\vdots \\
%
\frac{\partial \tilde{c}}{\partial \dot{\tilde{y}}} \left(\frac{\partial}{\partial t} \frac{\partial \tilde{y}(t)}{\partial p_{n_p}} \right)
+ \frac{\partial \tilde{c}}{\partial y} \left(\frac{\partial \tilde{y}(t)}{\partial p_{n_p}}\right)
+ \frac{\partial \tilde{c}}{\partial p_{n_p}}
%
\emat}
\in \tilde{\mathcal{Y}}^{n_p+1} \times \mathcal{P} \times \mathcal{V} \rightarrow \tilde{\mathcal{C}}^{n_p+1}.
\label{rythmos:eqn:direct-sens-daes}
\end{equation}
%
Therefore, the direct sensitivity equations can be cast as a single DAE from
the standpoint of a general time integration algorithm.

The matrix $M$ in (\ref{rythmos:eqn:M}) for
(\ref{rythmos:eqn:direct-sens-daes}) is given by
%
\begin{equation}
\alpha \frac{\partial f}{\partial \dot{x}} + \frac{\partial f}{\partial x} = 
{\bmat{cccc}
\alpha \frac{\partial \tilde{c}}{\partial \dot{\tilde{y}}} + \frac{\partial \tilde{c}}{\partial \tilde{y}} \\
& \alpha \frac{\partial \tilde{c}}{\partial \dot{\tilde{y}}} + \frac{\partial \tilde{c}}{\partial \tilde{y}} \\
& & \ddots \\
& & & \alpha \frac{\partial \tilde{c}}{\partial \dot{\tilde{y}}} + \frac{\partial \tilde{c}}{\partial \tilde{y}}
\emat}
\label{rythmos:eqn:direct-sens-daes-jac}
\end{equation}
%
which is a block diagonal matrix with the same block along the diagonal and
all quantities are evaluated at the same trial point
$(\dot{\tilde{y}},\tilde{y},u,t)$ as defined by the time integration
algorithm.

From looking at the structure of (\ref{rythmos:eqn:direct-sens-daes-jac}) it
is clear that specialized solution strategies are in order.  Significant
computational savings to be gained when solving the direct sensitivity
equations by taking advantage of block linear solvers and these equations also
offer great opportunities for course-grained parallel speedup (i.e.\ by
solving for blocks of different sensitivities on different clusters of
processors, see [???]).

Also note that the computation of forward direct sensitivities can take
advantage of cheap and easy to use forward AD tools or just directional finite
differences to evaluate sub-Jacobian vector products needed by the algorithms.

Issue: How does adaptive step-size control affect the direct sensitivity
computation?  This will effect how general software is used and what other
specializations are needed.

\subsection{Transient Adjoint Sensitivity Solvers}
\label{rythmos:scn:transientsensitivity:adjoint}

Adjoint sensitivity approaches are more computationally efficient when $n_u$
is large and/or when $n_v > 0$.  The adjoint sensitivity equations are derived in
Appendix~??? and for the model in (???)--(???) are given as
%
???
%

\section{Stability analysis and Eigen problems}

{}\textbf{ToDo: Andy, can you fill this section in or tell me what the
requirements are?}

\section{Application requirements}

In this section we state the requirements for applications to support various
time integration methods, direct and adjoint sensitivity computations, and
other types of numerical problems.  Our goal here is to model the requirements
at a level of abstraction most convenient for application developers.  Here we
define the concept of a parameterized atomic DAE (or ODE) that takes the form
%
\begin{eqnarray}
c\left( \dot{y}(t), y(t), \{u_l\}, t \right) & = & 0,
\; t \in \left[ t_i, t_f \right] \label{rythmos:eqn:atomic:c} \\
y(0) & = & y_0(\{u_l\}) \label{rythmos:eqn:atomic:c:ic}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$y(t) \:\in\:\mathcal{Y}$ are the transient state variables, \\
\>	$\dot{y}(t) = d(y)/d(t)\:\in\:\mathcal{Y}$ is the vector of temporal derivatives of $y(t)$, \\
\>	$\{u_l\}$ is a set of auxiliary parameter vectors with $|\{u_l\}| = N_u$ members, \\
\>	$u_l\in\mathcal{U}_l$, $l=1 {}\ldots N_u$, is the $l^{\mbox{th}}$ subset of auxiliary parameters, \\
\>	$c(\dot{y}(t), y(t), \{u_l\}, t) :
		\mathcal{Y}^2 \times \mathcal{U}_1 \times \ldots \times \mathcal{U}_{N_u} \times \RE
		\rightarrow \mathcal{C}$ are the state DAEs, \\
\>	$y_0(\{u_l\}) \:\in\:\mathcal{U}_1 \times \ldots \times \mathcal{U}_{N_u} \rightarrow \mathcal{Y}$
		define the initial conditions for $y(0)$ as a function of $\{u_l\}$, \\
\>	$\mathcal{Y} \:\subseteq\:\RE^{n_y}$ is the vector space of the state variables $y(t)$, \\
\>	$\mathcal{U}_l \:\subseteq\:\RE^{n_{u,l}}$ is the vector space of $u_l$ ($l = 1 {}\ldots N_u$), \\
\>	$\mathcal{C} \:\subseteq\:\RE^{n_y}$ is the vector space of the output of the DAE function $c(\ldots)$.
\end{tabbing}

The DAE formulation above in
(\ref{rythmos:eqn:atomic:c})--(\ref{rythmos:eqn:atomic:c:ic}) differs slightly
than the application DAE formulation in
(\ref{rythmos:eqn:app:c})--(\ref{rythmos:eqn:app:c:ic}) in that the latter is
more specific to sensitivity computations while the former is more general and
more amiable to a general software implementation.  The DAE in
(\ref{rythmos:eqn:atomic:c})--(\ref{rythmos:eqn:atomic:c:ic}) is meant to
represent any atomic set of DAEs that require a coupled solution and are
parameterized on one or more different subsets of auxiliary parameters
$u_l\in\mathcal{U}_l$.  Different subsets of auxiliary parameters
$u_l\in\mathcal{U}_l$ can be mapped into different types of quantities in
different types of numerical methods.  Some of the kinds of roles that an
individual subset vector $u_l\in\mathcal{U}_l$ of auxiliary parameters can
represent are:
%
\begin{itemize}
%
{}\item Steady-state parameters for sensitivity computations (i.e.\ $u_l =
p$ in (\ref{rythmos:eqn:app:c}))
%
{}\item Transient parameters for sensitivity computations (i.e.\ $u_l = v(t)$
in (\ref{rythmos:eqn:app:c}))
%
{}\item Coupling variables between different DAEs (e.g.\ $u_l = y$ in
(\ref{rythmos:eqn:app:h}))
%
{}\item Continuation parameters (i.e.\ to aid in the solution of the implicit
nonlinear time step equations $r(z)=0$ in (\ref{rythmos:eqn:r}))
%
{}\item Uncertain parameters (i.e.\ to be manipulated in an uncertainty
quantification study)
%
\end{itemize}

From the application developer's point of view, how a particular set of
parameters is used in some higher-level numerical algorithm does not greatly
influence what is required from a application to allow the manipulation of
these parameters or in how derivatives for those variables are computed by the
application.  The atomic DAE model in
(\ref{rythmos:eqn:atomic:c})--(\ref{rythmos:eqn:atomic:c:ic}) can be used to
represent (\ref{rythmos:eqn:app:c})--(\ref{rythmos:eqn:app:c:ic}) and
(\ref{rythmos:eqn:app:h})--(\ref{rythmos:eqn:app:h:ic}) in a sensitivity
computation and we only need to consider computational requirements with a
single set of DAE equations.

For the purpose of sensitivity computations we will also define a general
set of auxiliary response functions of the form
%
\begin{equation}
g(\{\hat{u}_l\}) \in \hat{\mathcal{U}}_1 \times \ldots \times \hat{\mathcal{U}}_{\hat{N}_u} \rightarrow \mathcal{G}.
\label{rythmos:eqn:atomic:g}
\end{equation}
%
where this set of auxiliary parameters $\{\hat{u}_l\}$ does not map one-to-one
with the set $\{u_l\}$ for any particular set of DAE.  For example, for the
sensitivity response function problem in (\ref{rythmos:eqn:app:g}) we have
$\hat{u}_1 = y(t)$, $\hat{u}_2 = p$, and $\hat{u}_3 = v(t)$.  The purpose of
defining a more general set of response functions in
(\ref{rythmos:eqn:atomic:g}) is to provide a concise description of the
requirements for various types of computations.

Here we divide up the requirements for an atomic DAE in
(\ref{rythmos:eqn:atomic:c})--(\ref{rythmos:eqn:atomic:c:ic}) and an atomic
set of response functions in (\ref{rythmos:eqn:atomic:g}) into three different
categories: forward solve, direct sensitivities and adjoint sensitivities.

\subsection{Requirements for forward solve}

Here we describe requirements for
(\ref{rythmos:eqn:atomic:c})--(\ref{rythmos:eqn:atomic:c:ic}) needed to
implement a basic class of implicit forward integration methods.

\begin{itemize}

\item\textbf{Zero-order requirements}

\begin{itemize}
%
{}\item Initial condition evaluation: Given $u_l\in\mathcal{U}_l$ (for
$l=1\ldots{}N_u$) evaluate the initial condition function
\[
\{u_l\} {}\rightarrow y_0(\{u_l\})\in\mathcal{Y}.
\]
%
{}\item DAE residual evaluation: Given selections for $\dot{y}\in\mathcal{Y}$,
$y\in\mathcal{Y}$, $u_l\in\mathcal{U}_l$ (for $l=1\ldots{}N_u$), and
$t\in\RE$, evaluate
\[
(\dot{y},y,\{u_l\},t) {}\rightarrow c(\dot{y},y,\{u_l\},t)\in\mathcal{C}.
\]
%
{}\item Auxiliary response evaluation: Given selections for
$\hat{u}_l\in\hat{\mathcal{U}}_l$ (for $l=1\ldots{}\hat{N}_u$), evaluate
\[
\{u_l\} {}\rightarrow g(\{\hat{u}_l\},t)\in\mathcal{G}.
\]
%
\end{itemize}

\item\textbf{First-order requirements}

\begin{itemize}
%
{}\item Formation of $M = {}\alpha ({}\partial c / {}\partial {}\dot{y}) +
({}\partial c / {}\partial y)$: Given selections for $\dot{y}\in\mathcal{Y}$,
$y\in\mathcal{Y}$, $u_l\in\mathcal{U}_l$ (for $l=1\ldots{}N_u$), $t\in\RE$,
and $\alpha\in\RE$, evaluate an abstract nonsingular object
\[
(\dot{y},y,\{u_l\},t) {}\rightarrow M
= {}\alpha \frac{\partial c}{\partial {}\dot{y}} + \frac{\partial c}{\partial y}
{}\in {}\mathcal{C}|\mathcal{Y}.
\]
%
{}\item Solve with $M = {}\alpha ({}\partial c / {}\partial {}\dot{y}) +
({}\partial c / {}\partial y)$: Given a preformed object for $M = {}\alpha
({}\partial c / {}\partial {}\dot{y}) + ({}\partial c / {}\partial y)$, then
provide the ability to (approximately) solve for systems of the form
\[
X = M^{-1} B
\]
where $B\in\mathcal{Y}|\RE^m$ is some arbitrary RHS $m$-column multi-vector
and $X\in\mathcal{C}|\RE^m$ is the LHS solution multi-vector.  The purpose of
stating the linear solve requirement in multi-vector form is to encourage the
development of block linear solver approaches that are useful for other types
of more advanced algorithms.  However, note that the ability to solve single
linear systems of the form
\[
x = M^{-1} b,
\]
where $b\in\mathcal{Y}$ and $x\in\mathcal{C}$, automatically satisfies this
requirement.  The exact specification of what is meant to (approximately) solve
systems of this nature must be accurately specified.
%
\end{itemize}

\end{itemize}

\subsection{Requirements for first-order direct sensitivities}

In addition for the requirements for forward solves described in the previous
section, here we describe an additional set of requirements for
(\ref{rythmos:eqn:atomic:c})--(\ref{rythmos:eqn:atomic:c:ic}) needed to
implement the implicit direct sensitivity methods as described in Section \ref{rythmos:scn:transientsensitivity:direct}.

\begin{itemize}

{}\item Initial condition direct sensitivity: Given $u_l\in\mathcal{U}_l$ (for
$l=1\ldots{}N_u$) and some $U_l\in\mathcal{U}_l|\RE^m$ compute the action of
\[
(\{u_l\},U_l) {}\rightarrow \frac{\partial y_0}{\partial u_l} U_l \in \mathcal{Y}|\RE^m
\; \mbox{for} \; l = 1 {}\ldots N_u,
\]
or equivalently evaluate the sensitivity themselves
\[
\{u_l\} {}\rightarrow \frac{\partial y_0}{\partial u_l} \in \mathcal{Y}|\mathcal{U}_l
\; \mbox{for} \; l = 1 {}\ldots N_u.
\]

{}\item DAE Auxiliary sensitivities: Given selections for
$\dot{y}\in\mathcal{Y}$, $y\in\mathcal{Y}$, $u_l\in\mathcal{U}_l$ (for
$l=1\ldots{}N_u$), $t\in\RE$, and some $U_l\in\mathcal{U}_l|\RE^m$ compute
the action of
\[
(\dot{y},y,\{u_l\},t,U_l) {}\rightarrow \frac{\partial c}{\partial u_l} U_l \in \mathcal{C}|\RE^m,
\; \mbox{for} \; l = 1 {}\ldots N_u,
\]
or alternatively compute the sensitivities themselves
\[
(\dot{y},y,\{u_l\},t) {}\rightarrow \frac{\partial c}{\partial u_l} \in \mathcal{C}|\mathcal{U}_l,
\; \mbox{for} \; l = 1 {}\ldots N_u.
\]

{}\item Auxiliary response function sensitivities: Given selections for
$\hat{u}_l\in\hat{\mathcal{U}}_l$ (for $l=1\ldots{}\hat{N}_u$), and some
$\hat{U}_l\in\hat{\mathcal{U}}_l|\RE^m$ compute the action of
\[
(\{\hat{u}_l\},\hat{U}_l) {}\rightarrow \frac{\partial g}{\partial \hat{u}_l} U_l \in \mathcal{G}|\RE^m,
\; \mbox{for} \; l = 1 {}\ldots N_u,
\]
or alternatively compute the sensitivities themselves
\[
(\{\hat{u}_l\}) {}\rightarrow \frac{\partial g}{\partial u_l} \in \mathcal{G}|\hat{\mathcal{U}}_l,
\; \mbox{for} \; l = 1 {}\ldots N_u.
\]

\end{itemize}

Note that these three different types of forward derivative computations can
always be approximated using directional finite differences of $\{u_l\}
{}\rightarrow y_0$, $(\dot{y},y,\{u_l\},t) {}\rightarrow c$ and
$(\{\hat{u}_l\},t) {}\rightarrow g$, respectively.  Therefore, numerical
methods that only require forward sensitivities as described above can always
use directional finite differences and therefore require really extra
functionality beyond what is needed for the forward solve.  Some numerical
methods, such as sensitivity computations, do require some degree of
exactness from derivative computations used for $M = {}\alpha ({}\partial c /
{}\partial {}\dot{y}) + ({}\partial c / {}\partial y)$ and the objects
described above.

\subsection{Requirements for first-order adjoint sensitivities}

In this section we describe an extra set of requirements for implementing
adjoint methods.

\section{General adaptive step-size control strategies}

Todd: Can you say something general here?  How can this be modeled in the most
general way to allow for reuse?

\section{Checkpointing Storage Management Strategies}

Notes: Here we want to outline the basic hierarchal checkpointing strategy of
Griewank and describe how it relates to the time integrators.  We also want to
discuss the basic idea of the ``Momento'' design pattern and how it can be
brought to bare here.

\section{Example applications}

Notes: The purpose of this section is to describe some important Sandia
applications and describe how the propose time integration tools may be used
with these applications.

\subsection{Xyce}

\subsection{Premo}

\subsection{Charon}

\subsection{Sundance}

\section{Conclusions}

% ---------------------------------------------------------------------- %
% References
%
\clearpage
\bibliographystyle{plain}
\bibliography{RythmosDesignSAND}
\addcontentsline{toc}{section}{References}

% ---------------------------------------------------------------------- %
% Appendices should be stand-alone for SAND reports. If there is only
% one appendix, put \setcounter{secnumdepth}{0} after \appendix
%
\appendix

\section{Derivation of general direct sensitivity equations}

\section{Derivation of general adjoint sensitivity equations}

\begin{SANDdistribution}
% External
% Housekeeping copies necessary for every unclassified report:
\SANDdistInternal{1}{9018}{Central Technical Files}{8945-1}
\SANDdistInternal{2}{0899}{Technical Library}{9610}
\SANDdistInternal{2}{0612}{Review \& Approval Desk}{4916}
% If report has a Patent Caution or Patent Interest, add this:
%\SANDdistInternal{3}{0161}{Patent and Licensing Office}{4916}
\end{SANDdistribution}

\end{document}

\documentclass[pdf,ps2pdf,11pt]{SANDreport}
\usepackage{pslatex}

%Local stuff
\usepackage{graphicx}
\usepackage{latexsym}
\input{rab_commands}
\newtheorem{theorem}{Theorem}

% If you want to relax some of the SAND98-0730 requirements, use the "relax"
% option. It adds spaces and boldface in the table of contents, and does not
% force the page layout sizes.
% e.g. \documentclass[relax,12pt]{SANDreport}
%
% You can also use the "strict" option, which applies even more of the
% SAND98-0730 guidelines. It gets rid of section numbers which are often
% useful; e.g. \documentclass[strict]{SANDreport}

% ---------------------------------------------------------------------------- %
%
% Set the title, author, and date
%

\title{
A derivation of forward and adjoint sensitivities for ODEs and DAEs
}
\author{Roscoe A. Bartlett}
\date{}

% ---------------------------------------------------------------------------- %
% Set some things we need for SAND reports. These are mandatory
%
\SANDnum{SAND2007-6699}
\SANDprintDate{May 2008}
\SANDauthor{Roscoe A. Bartlett}

% ---------------------------------------------------------------------------- %
% The following definitions are optional. The values shown are the default
% ones provided by SANDreport.cls
%
\SANDreleaseType{Unlimited Release}
%\SANDreleaseType{Not approved for release outside Sandia}

% ---------------------------------------------------------------------------- %
% The following definition does not have a default value and will not
% print anything, if not defined
%
%\SANDsupersed{SAND1901-0001}{January 1901}

% ---------------------------------------------------------------------------- %
%
% Start the document
%
\begin{document}
\raggedright

\maketitle

% ------------------------------------------------------------------------ %
% An Abstract is required for SAND reports
%

%
\begin{abstract}
%
Here we derive the basic forward and adjoint sensitivity approaches for fully
implicit ODEs and DAEs using just basic calculus and weak formulations.  The
purpose of this derivation is to allow a reader to understand transient
sensitivities at a first-principles level and to understand all of the
assumptions and steps that go into the derivations.
%
\end{abstract}
%

% ------------------------------------------------------------------------ %
% An Acknowledgment section is optional but important, if someone made
% contributions or helped beyond the normal part of a work assignment.
% Use \section* since we don't want it in the table of context
%
%\clearpage
%\section*{Acknowledgment}
%The authors would like to thank ...
%
%The format of this report is based on information found
%in~\cite{Sand98-0730}.
%

% ------------------------------------------------------------------------ %
% The table of contents and list of figures and tables
% Comment out \listoffigures and \listoftables if there are no
% figures or tables. Make sure this starts on an odd numbered page
%
%\clearpage
\tableofcontents
%\listoffigures
%\listoftables

% ---------------------------------------------------------------------- %
% An optional preface or Foreword
%\clearpage
%\section{Preface}
%Although muggles usually have only limited experience with
%magic, and many even dispute its existence, it is worthwhile
%to be open minded and explore the possibilities.

% ---------------------------------------------------------------------- %
% An optional executive summary
%\clearpage
%\section{Summary}
%Once a certain level of mistrust and scepticism has
%been overcome, magic finds many uses in todays science
%and engineering. In this report we explain some of the
%fundamental spells and instruments of magic and wizardry. We
%then conclude with a few examples on how they can be used
%in daily activities at national Laboratories.

% ---------------------------------------------------------------------- %
% An optional glossary. We don't want it to be numbered
%\clearpage
%\section*{Nomenclature}
%\addcontentsline{toc}{section}{Nomenclature}
%\begin{itemize}
%\item[alohomora]
%spell to open locked doors and containers
%\end{itemize}

% ---------------------------------------------------------------------- %
% This is where the body of the report begins; usually with an Introduction
%
\setcounter{secnumdepth}{3}
\SANDmain % Start the main part of the report

%
\section{Introduction}
%

Here we derive forward and adjoint sensitivity methods for general ODEs and
DAEs using just simple calculus and weak formulations.  The goal of this
derivation is to allow the reader to understand every part of the derivation
and to gain understanding at a basic level.

The general fully implicit parameterized DAE (or ODE) state model that we
consider is
%
\begin{eqnarray}
f\left( \dot{x}(t), x(t), p, v(t), t \right) & = & 0,
\; t \in \left[ t_0, t_f \right] \label{eqn:sens:f} \\
x(0) & = & x_0(p), \label{eqn:sens:f:ic} \\
\dot{x}(0) & = & \dot{x}_0(p), \label{eqn:sens:f:ic-dot}
\end{eqnarray}
%
where
\bifthen
%
{}\> $x: t {}\rightarrow x(t){}\:\in\:\mathcal{X}$ for $t\in[t_0,t_f]$ are the
state variables, \\
%
{}\> $\dot{x}: t {}\rightarrow \dot{x}(t) = d(x)/d(t)\:\in\:\mathcal{X}$ for
$t\in[t_0,t_f]$ are the differential state variables,
\\
%
{}\> $p{}\:\in\:\mathcal{P}$ are the steady-state parameters,\\
%
{}\> $v: t {}\rightarrow v(t){}\:\in\:\mathcal{V}$ for $t\in[t_0,t_f]$ are the
time-dependent parameters, \\
%
{}\> $f(\dot{x}, x, p, v, t)
:{}\mathcal{X}^2{}\times{}\mathcal{P}{}\times{}\mathcal{V}{}\times{}\RE
{}\rightarrow{}\mathcal{F}$ is the state DAE (ODE) residual function, \\
%
{}\> $x_0(p){}\:\in\:\mathcal{P}{}\rightarrow{}\mathcal{X}$ is the initial
condition function for $x(t_0)$, \\
%
{}\> $\dot{x}_0(p){}\:\in\:\mathcal{P}{}\rightarrow{}\mathcal{X}$ is the
initial condition function for $\dot{x}(t_0)$, \\
%
{}\> $\mathcal{X}{}\:\subseteq\:\RE^{n_x}$ is the vector space for the state
variables, \\
%
{}\> $\mathcal{P}{}\:\subseteq\:\RE^{n_p}$ is the vector space for the
steady-state parameters, \\
%
{}\> $\mathcal{V}{}\:\subseteq\:\RE^{n_v}$ is the vector space for the
transient parameters, and \\
%
{}\> $\mathcal{F}{}\:\subseteq\:\RE^{n_x}$ is the vector space for the state
DAE residual function.
\eifthen

Note that we assume that we have a consistent initial condition where
%
\begin{eqnarray}
f\left( \dot{x}_0(t_0), x_0(t_0), p, v(t_0), t_0 \right) = 0. \label{dae::consistent-ic}
\end{eqnarray}

{}\textit{Remarks on notation}: We use the commonly accepted convention where
the unadorned identifier $x$ for a function such as $t {}\rightarrow x(t)$,
$t\in[t_0,t_f]$, is used to represent the function over the entire domain
$t\in[t_0,t_f]$.  When appropriate, the notation $x(t)$ will be used to
represent a particular evaluation of the function at points $t$ in the domain
$[t_0,t_f]$.  However, in some situations, the argument $t$ of $x(t)$ will be
omitted and instead a bare identifier $x$ is used and it should be clear from
the context (i.e.\ inside of an integral) that really a single value of the
function is being represented.  Cases of possible ambiguity will be addressed
in the sequel.

Here we consider both distributed and terminal responses as shown by the
aggregate response function:
%
\begin{equation}
d(x,p,v)
= \int_{t_0}^{t_f} g(\dot{x}(t),x(t),p,v(t),t) dt + h(\dot{x}(t_f),x(t_f),p),
\label{eqn:sens:d}
\end{equation}
%

where:
\bifthen
%
%
{}\> $g(\dot{x},x,p,v,t) :{}\:
{}\mathcal{X}^2{}\times{}\mathcal{P}{}\times{}\mathcal{V}{}\times{}\RE
{}\rightarrow{}\mathcal{G}$ is the distributed response function, \\
%
{}\> $h(\dot{x},x,p) :{}\: {}\mathcal{X}^2{}\times{}\mathcal{P}
{}\rightarrow{}\mathcal{G}$ is the terminal response function, and \\
%
{}\> $\mathcal{G}{}\:\subseteq\:\RE^{n_g}$ is the vector space for the
response functions. \\
%
\eifthen

Above, note that there may be many response functions as $n_g=|\mathcal{G}|$
may be greater than 1.  The ratio $n_g/n_p=|\mathcal{G}|/|\mathcal{P}|$ has
great implications when considering forward and adjoint sensitivity methods.

The implicit state solution for the DAEs in
(\ref{eqn:sens:f})--(\ref{eqn:sens:f:ic-dot}) at
points $t$ is signified as $x(p,v,t)\in\mathcal{X}$ and
$\dot{x}(p,v,t)\in\mathcal{X}$, where $v$ in this case is the selection of
transient parameters in the range $t\in[t_0,t]$.  Note that selections for $v$
in the range $(t:t_f]$ have no influence on the implicit state solution $x$ up
to the time $t$.  The full infinite-dimensional implicit state solution for
$x$ in $t\in[t_0,t_f]$ is signified as $x(p,v)$ where in this case $v$
represents the entire selection for the transient parameters in $[t_0,t_f]$.

Given the implicit state function $x(p,v)$ and its time derivative
$\dot{x}(p,v)$, a reduced set of auxiliary response functions are defined as
%
\begin{equation}
\hat{d}(p,v)
= \int_{t_0}^{t_f} \hat{g}(p,v(t),t) dt + \hat{h}(p),
\label{eqn:sens:d_hat}
\end{equation}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>  $\hat{g}(p,v(t),t) = g(\dot{x}(p,v,t),x(p,v,t),p,v(t),t)) : \:
    \mathcal{P} \times \mathcal{V} \times \RE \rightarrow \mathcal{G}$ is defined on $t\in[t_0,t_f]$, and \\
\>  $\hat{h}(p) = h(\dot{x}(p,v,t_f),x(p,v,t_f),p)) : \:
    \mathcal{P} \rightarrow \mathcal{G}$ is defined only at $t=t_f$.
\end{tabbing}


%
\section{Derivation of forward sensitivities}
\label{rythmos:app:forward-sens-derivation}
%

For the derivation of forward sensitivities we will ignore the transient
parameters $v(t)$ for $t\in[t_0,t_f]$ since forward sensitivity methods for
such problems are typically impractical\footnote{Note: For some applications
where $n_v$ is not too large and where $v$ is given a very course
discretization in $t\in[t_0,t_f]$, computing forward sensitivities with respect
to the discretized $v$ can be practical.}.

The forward sensitivity problem is easily stated by differentiating
(\ref{eqn:sens:d_hat}) with respect to $p$ to obtain
%
\begin{equation}
\Jac{\hat{d}}{p}
= \int_{t_0}^{t_f} \left(
    \Jac{g}{\dot{x}} \Jac{\dot{x}}{p} + \Jac{g}{x} \Jac{x}{p} +  \Jac{g}{p}
    \right) dt
+ \left. \left(
    \Jac{h}{\dot{x}} \Jac{\dot{x}}{p} + \Jac{h}{x} \Jac{x}{p} + \Jac{h}{p}
  \right) \right|_{t=t_f},
\label{eqn:sens:d_d_hat_d_p}
\end{equation}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>  $\jac{\hat{g}}{p} \in \mathcal{G}|\mathcal{P}$ is defined on $t\in[t_0,t_f]$, \\
\>  $\jac{\hat{h}}{p} \in \mathcal{G}|\mathcal{P}$ is defined only at $t=t_f$, \\
\>  $\jac{x}{p} \in \mathcal{X}|\mathcal{P}$ is the forward sensitivity of the state defined on $t\in[t_0,t_f]$, \\
\>  $\jac{\dot{x}}{p} \in \mathcal{X}|\mathcal{P}$ is the forward sensitivity of the state time derivative defined on $t\in[t_0,t_f]$, and \\
\>  $\jac{\hat{d}}{p} \in \mathcal{G}|\mathcal{P}$ is defined independent of time.
\end{tabbing}

The forward sensitivity $\jac{x}{p}$ and $\jac{\dot{x}}{p}$ in
(\ref{eqn:sens:d_d_hat_d_p}) is computed by solving a set of
$n_p$ independent forward sensitivity equations which are obtained by
differentiating
(\ref{eqn:sens:f})--(\ref{eqn:sens:f:ic-dot}) with
respect to $p$ and changing the order of differentiation with respect to $t$
and $p$ to give
%
\begin{eqnarray}
\Jac{f}{\dot{x}} \left(\Jac{\dot{x}}{p} \right) + \Jac{f}{x} \left(\Jac{x}{p}\right)
+ \Jac{f}{p} & = & 0, \; t \in \left[ t_0, t_f \right], \label{eqn:sens:forward-f} \\
\Jac{x(t_0)}{p} & = & \Jac{x_0}{p}, \label{eqn:sens:forward-f:ic} \\
\Jac{\dot{x}(t_0)}{p} & = & \Jac{\dot{x}_0}{p}, \label{eqn:sens:forward-f:ic-dot}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$\jac{f}{\dot{x}} \in \mathcal{F}|\mathcal{X}$ is defined on $t\in[t_0,t_f]$, \\
\>	$\jac{f}{x} \in \mathcal{F}|\mathcal{X}$ is defined on $t\in[t_0,t_f]$, \\
\>	$\jac{f}{p} \in \mathcal{F}|\mathcal{P}$ is defined on $t\in[t_0,t_f]$, \\
\>	$\jac{x_0}{p} \in \mathcal{X}|\mathcal{P}$ is defined only at $t=t_0$, and \\
\>	$\jac{\dot{x}_0}{p} \in \mathcal{X}|\mathcal{P}$ is defined only at $t=t_0$.
\end{tabbing}

Note that above, $\jac{\dot{x}}{p} = \frac{d}{dt} {}\jac{x}{p} = \jac{}{p}
{}\frac{dx}{dt}$ because the order of differentiation does not matter with
smooth differentiable functions, which is the assumption here.

These $n_p$ independent sensitivity equations are solved for $\jac{x}{p}
{}\in\mathcal{X}|\mathcal{P}$ and $\jac{\dot{x}}{p}
{}\in\mathcal{X}|\mathcal{P}$ forward in time from $t_0$ to $t_f$.  The
integral in (\ref{eqn:sens:d_d_hat_d_p}) can be evaluated along
with the integration of
(\ref{eqn:sens:f})--(\ref{eqn:sens:f:ic-dot}) and
(\ref{eqn:sens:forward-f})--(\ref{eqn:sens:forward-f:ic-dot}).
Note that (\ref{eqn:sens:f})--(\ref{eqn:sens:f:ic-dot})
and
(\ref{eqn:sens:forward-f})--(\ref{eqn:sens:forward-f:ic-dot})
are a staggered set of DAE equations in that
(\ref{eqn:sens:f})--(\ref{eqn:sens:f:ic-dot}) are solved
first followed by
(\ref{eqn:sens:forward-f})--(\ref{eqn:sens:forward-f:ic-dot}).

\section{Derivation of adjoint sensitivities}

Here we describe a derivation for the adjoint equation and the gradient
expressions that are based on simple and straightforward principles.  In this
derivation, we assume that the response functions $g()$ and $h()$ do not
depend on $\dot{x}$.
% 20071005: rabartl: I can most likely find a way to revise the derivation to allow
% a dependence on \dot{x} but for now I just don't want to have to deal with this.

\subsection{Derivation of adjoint equation and sensitivities for steady-state parameters}
\label{rythmos:app:adj-equ-derivation}

In this section we begin with the basic expressions for the reduced derivative
$\jac{\hat{d}}{p}$ in (\ref{eqn:sens:d_d_hat_d_p}) and the forward sensitivity
equations for $\jac{x}{p}$ in
(\ref{eqn:sens:forward-f})--(\ref{eqn:sens:forward-f:ic-dot}) and then perform
basic manipulations in order to arrive at the adjoint equations and the
adjoint sensitivities for $\jac{\hat{d}}{p}$.

We begin our derivation of the adjoint equation and sensitivities for
steady-state parameters by writing the weak form of the forward sensitivity
equation (\ref{eqn:sens:forward-f}) which, in multi-vector form, is
%
\begin{equation}
\int_{t_0}^{t_f} \Lambda^T \left(
\Jac{f}{\dot{x}} \left(\frac{d}{dt} \Jac{x}{p} \right)
+ \Jac{f}{x} \left(\Jac{x}{p}\right)
+ \Jac{f}{p}
\right) dt = 0,
\label{eqn:sens:weak-forward-f}
\end{equation}
%
where at this point the multi-vector function $\Lambda$, for
$t\rightarrow\Lambda(t)\in\mathcal{F}|\mathcal{G}$ in $t\in[t_0,t_f]$, is any
appropriate weighting function\footnote{The admissible class of weighting
functions $\Lambda$ are those functions that are sufficiently smooth such that
the integrals of the weak form (and the modified weak form after integration
by parts) are finite and well defined.  Other requirements may also need to be
satisfied in some cases {}\cite{BeckerCareyOden-FE}.}.  Later, $\Lambda$ will
be chosen to be the adjoint variables but for now $\Lambda$ is nothing more
than an arbitrary weighting function for the purpose of stating the weak form.
The solution to the weak form of (\ref{eqn:sens:weak-forward-f}) is every bit
as valid and is indeed more general than the strong form in
(\ref{eqn:sens:forward-f}) and we lose nothing by considering the weak form
{}\cite{BeckerCareyOden-FE}.  Also note that the multi-vector form of
(\ref{eqn:sens:weak-forward-f}) really gives $n_g$ separate weak form
equations.

Next, we substitute the integration by parts
%
\begin{equation}
\int_{t_0}^{t_f} \left[ \left( \Lambda^T \Jac{f}{\dot{x}} \right) \frac{d}{dt}\left( \Jac{x}{p} \right) \right] dt
= \left. \left( \Lambda^T \Jac{f}{\dot{x}} \Jac{x}{p} \right) \right|_{t_0}^{t_f}
- \int_{t_0}^{t_f} \left[ \frac{d}{dt}\left( \Lambda^T \Jac{f}{\dot{x}} \right) \Jac{x}{p} \right] dt
\end{equation}
%
into (\ref{eqn:sens:weak-forward-f}) and rearrange which yields
%
\begin{equation}
\int_{t_0}^{t_f} \left( \Lambda^T \Jac{f}{p} \right) dt
+ \int_{t_0}^{t_f} \left[
    - \frac{d}{dt}\left( \Lambda^T \Jac{f}{\dot{x}} \right)
    + \Lambda^T \Jac{f}{x}
  \right] \Jac{x}{p} dt
+ \left. \left( \Lambda^T \Jac{f}{\dot{x}} \Jac{x}{p} \right) \right|_{t_0}^{t_f}
= 0.
\label{eqn:sens:weak-forward-f-2}
\end{equation}
%
At this point in the derivation we decide to restrict the possible set of
functions for $\Lambda$ by forcing $\Lambda$ to satisfy the differential
equation
%
\begin{equation}
\frac{d}{dt}\left( \Lambda^T \Jac{f}{\dot{x}} \right)
-  \Lambda^T \Jac{f}{x} = - \Jac{g}{x}, \; t \in \left[ t_0, t_f \right].
\label{eqn:sens:adj-trans}
\end{equation}
%
All that we have done in (\ref{eqn:sens:adj-trans}) is to make what appears to
be an arbitrary choice to help narrow the weighting function $\Lambda$ from
the infinite set of possible choices.  It will be clear later why this choice
is a convenient one.  Note that the choice in (\ref{eqn:sens:adj-trans}) does
not in and of itself uniquely determine $\Lambda$ since boundary conditions
have yet to be specified.  It will be shown later that the choice of boundary
condition is arbitrary but when we move toward the end of the derivation, a
natural and very convenient choice for a boundary condition that uniquely
specifies $\Lambda$ will be obvious.

{}\noindent\textit{Side Note:} What is critical about the choice for the
adjoint equation in (\ref{eqn:sens:adj-trans}) is that the derivative
$\jac{g}{x}$ appear on the RHS.  What is largely arbitrary, however is the
sign of the given to $\jac{g}{x}$ which of course defines the sign for the
adjoint variables $\Lambda$.  The Petzold papers (e.g.\
{}\cite{adjoint-sens-2003}) and the IDAS code {}\cite{sundials} use the sign
for the adjoint given in (\ref{eqn:sens:adj-trans}).  However, the CVODES code
{}\cite{cvodes,sundials} uses the opposite sign for $\jac{g}{x}$ in the RHS
and therefore gives the opposite sign for the adjoint solution $\Lambda$.
With respect the the computation of the reduced response derivative, the
choice for the sign of $\Lambda$ is arbitrary.  However, if one is going to
directly interpret the adjoint solution $\Lambda$ as we do in Section
{}\ref{sec:adjoint-interpretation}, then the sign of the adjoint becomes very
important and it is critical that the user of any adjoint code know the
significance of the sign of the adjoint and understand its meaning.

The derivation continues by substituting
(\ref{eqn:sens:adj-trans}) into
(\ref{eqn:sens:weak-forward-f-2}) and rearranging which yields
%
\begin{eqnarray}
\int_{t_0}^{t_f} \left( \Jac{g}{x} \Jac{x}{p} \right) dt
& = & 
- \int_{t_0}^{t_f} \left( \Lambda^T \Jac{f}{p} \right) dt
- \left. \left( \Lambda^T \Jac{f}{\dot{x}} \Jac{x}{p} \right) \right|^{t_f}_{t_0}.
\label{eqn:sens:weak-forward-f-3}
\end{eqnarray}

Finally, substituting (\ref{eqn:sens:weak-forward-f-3}) into
(\ref{eqn:sens:d_d_hat_d_p}), dropping terms with $\jac{g}{\dot{x}}$ and
$\jac{h}{\dot{x}}$ since we are assuming there are zero in this derivation,
and rearranging gives
%
\begin{eqnarray}
\Jac{\hat{d}}{p} 
& = &
\int_{t_0}^{t_f} \left(\Jac{g}{p} -  \Lambda^T \Jac{f}{p} \right) dt
+ \left. \left( \Jac{h}{p} \right) \right|_{t=t_f}
+ \left. \left( \Lambda^T \Jac{f}{\dot{x}} \Jac{x}{p} \right) \right|_{t=t_0}
\nonumber \\
& &
+ \left. \left[ \left(
    \Jac{h}{x} - \Lambda^T \Jac{f}{\dot{x}}
  \right)  \Jac{x}{p} \right] \right|_{t=t_f}.
\label{eqn:sens:d_d_hat_d_p-3}
\end{eqnarray}
%
We are not finished yet since, as we stated earlier, the choice for the
weighting functions $\Lambda$ have not yet been uniquely specified.  The first
thing to consider is that, in general, the adjoint DAE in
(\ref{eqn:sens:adj-trans}) is only stable if integrated backwards
in time from $t_f$ to $t_0$ {}\cite{adjoint-sens-2003}.  However, as described
in the following theorem, we are free to pick almost any final condition for
$\Lambda(t_f)$ that is consistent with the adjoint equation at $t_f$ and then
evaluate the terms in (\ref{eqn:sens:d_d_hat_d_p-3}) involving
the resulting adjoint solution.  This is established in the following theorem.

\begin{theorem}
If $\Lambda_1$ and $\Lambda_2$ are any two particular bounded solutions to the
adjoint equation (\ref{eqn:sens:adj-trans}) (each associated with
a different bounded consistent final condition on $\Lambda(t_f)$) then
%
\begin{equation}
\left.\Jac{\hat{d}}{p}\right|_{\Lambda=\Lambda_1} = \left.\Jac{\hat{d}}{p}\right|_{\Lambda=\Lambda_2}
\label{eqn:sens:d_d_hat_d_p-same}
\end{equation}
%
where $\left.(\jac{\hat{d}}{p})\right|_{\Lambda=\Lambda_1}$ and
$\left.(\jac{\hat{d}}{p})\right|_{\Lambda=\Lambda_1}$ represent the reduced
derivative in (\ref{eqn:sens:d_d_hat_d_p-3}) evaluated using the
adjoint solutions $\Lambda_1$ and $\Lambda_2$ respectively.
\end{theorem}

\textbf{Proof}

Here we prove (\ref{eqn:sens:d_d_hat_d_p-same}) by proving that
%
\begin{eqnarray}
D_2 - D_1
& = &
\left.\Jac{\hat{d}}{p}\right|_{\Lambda=\Lambda_2} - \left.\Jac{\hat{d}}{p}\right|_{\Lambda=\Lambda_1}
\nonumber \\
& = &
\int_{t_0}^{t_f} \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{f}{p} dt
+ \left.\left[ \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{f}{\dot{x}} \Jac{x}{p} \right]\right|_{t_0}^{t_f}
= 0
\label{eqn:sens:ta:d_hat_d_d_p-diff-zero}
\end{eqnarray}
%
for any two particular solutions $\Lambda_1$ and $\Lambda_2$ to the adjoint
equation (\ref{eqn:sens:adj-trans}).  Above, the expression in
(\ref{eqn:sens:ta:d_hat_d_d_p-diff-zero}) comes from substituting $\Lambda_1$
and $\Lambda_2$ into (\ref{eqn:sens:d_d_hat_d_p-3}), subtracting the two
expressions, dropping out zero terms, and rearranging.

We start the proof by substituting (\ref{eqn:sens:forward-f}) into
(\ref{eqn:sens:ta:d_hat_d_d_p-diff-zero}) which yields
%
\begin{equation}
D_2 - D_1 = 
\int_{t_0}^{t_f} \left( \Lambda_2 - \Lambda_1 \right)^T \left[
  - \Jac{f}{\dot{x}} \left(\frac{d}{dt} \Jac{x}{p} \right) - \Jac{f}{x} \left(\Jac{x}{p}\right) \right] dt
+ \left.\left[ \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{f}{\dot{x}} \Jac{x}{p} \right]\right|_{t_0}^{t_f}.
\label{eqn:sens:ta:d_hat_d_d_p-diff-zero-2}
\end{equation}
%
Substituting the integration by parts
%
\[
- \int_{t_0}^{t_f} \left[ \left( \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{f}{\dot{x}} \right) \frac{d}{dt}\left( \Jac{x}{p} \right) \right] dt
= \int_{t_0}^{t_f} \left[ \frac{d}{dt}\left( \left( \Lambda_2 - \Lambda_1 \right)^T  \Jac{f}{\dot{x}} \right) \Jac{x}{p} \right] dt
- \left. \left[ \left( \Lambda_2 - \Lambda_1 \right)^T  \Jac{f}{\dot{x}} \Jac{x}{p} \right] \right|_{t_0}^{t_f}
\]
%
into (\ref{eqn:sens:ta:d_hat_d_d_p-diff-zero-2}), canceling
terms, and rearranging yields
%
\begin{equation}
D_2 - D_1 = 
\int_{t_0}^{t_f} \left[ \frac{d}{dt} \left( \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{f}{\dot{x}} \right)
- \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{f}{x} \right] \left(\Jac{x}{p}\right) dt.
\label{eqn:sens:ta:d_hat_d_d_p-diff-zero-3}
\end{equation}
%
Next, substituting the solutions $\Lambda_1$ and $\Lambda_2$ into
(\ref{eqn:sens:adj-trans}) and subtracting these two adjoint
equations yields
%
\begin{equation}
- \frac{d}{dt} \left( \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{f}{\dot{x}} \right)
+ \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{f}{x} = 0, \; t \in [t_0,t_f].
\label{eqn:sens:ta:adj-trans-diff}
\end{equation}
%
Finally, substituting (\ref{eqn:sens:ta:adj-trans-diff}) into
(\ref{eqn:sens:ta:d_hat_d_d_p-diff-zero-3}) yields
%
\begin{equation}
D_2 - D_1 = 0
\label{eqn:sens:ta:d_hat_d_d_p-diff-zero-final}
\end{equation}
%
which completes the proof. $\;\Box$

The above theorem establishes that there are an infinite number of choices for
the final condition for the adjoint solution that all yield the same reduced
derivative $\jac{\hat{d}}{p}$.  While a wide variety of final conditions for
$\Lambda(t_f)$ can be chosen to close the adjoint equation, rather than
picking just any final condition, selecting the final condition
%
\begin{equation}
\left. \left(
  \Jac{h}{x} - \Lambda^T \Jac{f}{\dot{x}}
\right) \right|_{t=t_f}
 = 0,
\label{eqn:sens:adj-trans:fc}
\end{equation}
%
is rather convenient since it zeros out the last term in
(\ref{eqn:sens:d_d_hat_d_p-3}) and allows us to avoid the computation of
$\jac{x}{p}$ at $t=t_f$ which we do not readly have with an adjoint method.
This specification of the final condition closes the adjoint equation and
uniquely defines the adjoint $\Lambda$ in all of the cases of state DAEs what
we will consider here\footnote{A uniquely defined adjoint $\Lambda$ requires
that certain regularity conditions are satisfied by the state DAE (see
{}\cite{adjoint-sens-2003}) but for the purposes of this derivation we assume
that such a condition is always satisfied.}.

Finally, substituting (\ref{eqn:sens:adj-trans:fc}) and
$\jac{x_0}{p}$ for $\jac{x}{p}$ at $t=t_0$ in
(\ref{eqn:sens:d_d_hat_d_p-3}) we arrive the final expression for
reduced derivative
%
\begin{equation}
\Jac{\hat{d}}{p} =
\int_{t_0}^{t_f} \left(
    \Jac{g}{p}
    - \Lambda^T \Jac{f}{p}
  \right) dt
  + \left. \left( \Jac{h}{p} \right) \right|_{t=t_f}
  + \left. \left( \Lambda^T \Jac{f}{\dot{x}} \right) \right|_{t=t_0} \Jac{x_0}{p}.
\label{eqn:sens:d_d_hat_d_p_final}
\end{equation}

In conclusion, (\ref{eqn:sens:adj-trans}) and (\ref{eqn:sens:adj-trans:fc})
define the set of adjoint DAE equations that specify a unique adjoint solution
$\Lambda$ which are integrated backwards in time and
(\ref{eqn:sens:d_d_hat_d_p_final}) gives the expression for the computation of
$\jac{\hat{d}}{p}$ in terms of the computed adjoint $\Lambda$.  Note that the
integral in (\ref{eqn:sens:d_d_hat_d_p_final}) can be evaluated at the same
time that the adjoint is being solved backward in time.  Therefore, no storage
of the adjoint is needed beyond what is needed for the time stepping
algorithm.

\subsection{Derivation of adjoint sensitivities for transient parameters}

Here we consider the derivation of the reduced sensitivity with respect to
transient parameters
%
\begin{equation}
\Jac{\hat{d}}{v(t)} \in \mathcal{G}|\mathcal{V}, \; \mbox{for} \; t\in[t_0,t_f].
\end{equation}
%
Our derivation for these sensitivities for the transient parameters $v$ will
be a little different than for the steady-state parameters $p$.  The primary
reason for using a different derivation is that the infinite-dimensional
transient parameters $v$ are fundamentally different than finite-dimensional
steady-state parameters $p$.  Because of the infinite-dimensional nature of
$v$, we use a different tool of functional analysis.  The tool we use is the
application of the infinite-dimensional operator $\jac{\hat{d}}{v}$ to the
infinite-dimensional perturbation $\delta v$ where we will then use to
identify $\jac{\hat{d}}{v(t)}$ in the integrand of
%
\begin{equation}
\Jac{\hat{d}}{v} \delta v = \int_{t_0}^{t_f} \Jac{\hat{d}}{v(t)} \delta v(t) dt.
\label{eqn:sens:d_d_hat_d_v_delta_v}
\end{equation}
%
The integral in (\ref{eqn:sens:d_d_hat_d_v_delta_v}) is the definition the
infinite-dimensional operator application $(\jac{\hat{d}}{v}) {}\delta v$.

We begin the derivation with the variational derivative of $\hat{d}(v)$ with
respect to $v$ along the variation $\delta v$ which is simply
%
\begin{equation}
\Jac{\hat{d}}{v} \delta v
= \int_{t_0}^{t_f} \left( \Jac{g}{x} \Jac{x}{v} \delta v +  \Jac{g}{v} \delta v \right) dt
+ \left. \left(  \Jac{h}{x} \Jac{x}{v} \delta_v \right) \right|_{t=t_f}.
\label{eqn:sens:d_d_hat_d_v_delta_v-2}
\end{equation}
%
Next, we write the weak form of the linearization of the state equation (about
a solution for $f(\dot{x},x,p,v,t)=0$) with respect to $v$ along the variation
$\delta v$ which is
%
\begin{equation}
%
\int_{t_0}^{t_f} \Lambda^T \left[
\Jac{f}{\dot{x}} \left(\frac{d}{dt} \Jac{x}{v} \right) \delta v + \Jac{f}{x} \Jac{x}{v} \delta v
+ \Jac{f}{v} \delta_v \right] dt = 0
\label{eqn:sens:weak-forward-f-v}
\end{equation}
%
and has initial condition $\left.(\jac{x}{v})\delta v\right|_{t=t_0} = 0$.
The sensitivity equation that is used in the weak form in
(\ref{eqn:sens:weak-forward-f-v}) can be derived using a simple
Taylor expansion of the state equation about a solution $f(\dot{x},x,p,v,t)=0$
in the variation $\delta v$ and then dropping off the $O(||\delta v||^2)$ and
higher terms.

From here the derivation proceeds almost identically to the case for the
steady-state parameters $p$ described in the previous section.

Substituting the integration by parts
%
\begin{equation}
\int_{t_0}^{t_f} \left[ \left( \Lambda^T \Jac{f}{\dot{x}} \right) \frac{d}{dt}\left( \Jac{x}{v} \delta v \right) \right] dt
= \left. \left( \Lambda^T \Jac{f}{\dot{x}} \Jac{x}{v} \delta v \right) \right|_{t_0}^{t_f}
- \int_{t_0}^{t_f} \left[ \frac{d}{dt}\left( \Lambda^T \Jac{f}{\dot{x}} \right) \Jac{x}{v} \delta v \right] dt
\end{equation}
%
into (\ref{eqn:sens:weak-forward-f-v}) and rearranging yields
%
\begin{equation}
\int_{t_0}^{t_f} \left( \Lambda^T \Jac{f}{v} \delta v \right) dt
+ \int_{t_0}^{t_f} \left[
    - \frac{d}{dt}\left( \Lambda^T \Jac{f}{\dot{x}} \right)
    + \Lambda^T \Jac{f}{x}
  \right] \Jac{x}{v} \delta v dt
+ \left. \left( \Lambda^T \Jac{f}{\dot{x}} \Jac{x}{v} \delta v \right) \right|_{t_0}^{t_f}
= 0.
\label{eqn:sens:weak-forward-f-v-2}
\end{equation}
%
Substituting the choice for the adjoint equation
(\ref{eqn:sens:adj-trans}) into
(\ref{eqn:sens:weak-forward-f-v-2}) and rearranging yields
%
\begin{eqnarray}
\int_{t_0}^{t_f} \left( \Jac{g}{x} \Jac{x}{v} \delta v \right) dt
& = &
- \int_{t_0}^{t_f} \left( \Lambda^T \Jac{f}{v} \delta v \right) dt
- \left. \left( \Lambda^T \Jac{f}{\dot{x}} \Jac{x}{v} \delta v \right) \right|^{t_f}_{t_0}.
\label{eqn:sens:weak-forward-f-v-3}
\end{eqnarray}
%
Substituting (\ref{eqn:sens:weak-forward-f-v-3}) into
(\ref{eqn:sens:d_d_hat_d_v_delta_v-2}) and rearranging
yields
%
\begin{eqnarray}
\Jac{\hat{d}}{v} \delta v 
& = &
\int_{t_0}^{t_f} \left( \Jac{g}{v} -  \Lambda^T \Jac{f}{v} \right) \delta v dt
+ \left. \left( \Lambda^T \Jac{f}{\dot{x}} \Jac{x}{v} \delta v \right) \right|_{t=t_0}
\nonumber \\
& &
+ \left. \left[ \left(
    \Jac{h}{x} - \Lambda^T \Jac{f}{\dot{x}}
  \right)  \Jac{x}{v} \delta v \right] \right|_{t=t_f}.
\label{eqn:sens:d_d_hat_d_v_delta_v-3}
\end{eqnarray}
%
Using the choice for the final condition in (\ref{eqn:sens:adj-trans:fc}) and
noting that $\left.(\jac{x}{v})\delta v\right|_{t=t_0} = 0$ (since $v$ can not
affect $x$ at $t=t_0$ because $v$ only appears in the DAE RHS, not the initial
condition), then (\ref{eqn:sens:d_d_hat_d_v_delta_v-3}) reduces to
%
\begin{equation}
\Jac{\hat{d}}{v} \delta v 
 = \int_{t_0}^{t_f} \left( \Jac{g}{v} - \Lambda^T \Jac{f}{v} \right) \delta v dt.
\label{eqn:sens:d_d_hat_d_v_delta_v-final}
\end{equation}

Finally, by comparing (\ref{eqn:sens:d_d_hat_d_v_delta_v-final})
with (\ref{eqn:sens:d_d_hat_d_v_delta_v}) it is clear that
%
\begin{equation}
\Jac{\hat{d}}{v(t)} = \Jac{g}{v(t)} - \Lambda(t)^T \Jac{f}{v(t)}, \; t\in[t_0,t_f]
\label{eqn:sens:d_d_hat_d_v_t-final}
\end{equation}
%
is the reduced derivative object that we are seeking.


%
\section{Significance and interpretation of the adjoint}
\label{sec:adjoint-interpretation}
%

In this section we consider another reason for choosing
(\ref{eqn:sens:adj-trans:fc}) as the final condition to close the
adjoint equation (\ref{eqn:sens:adj-trans}).  This choice of the
final condition makes the adjoint variables $\Lambda$ become the first-order
sensitivity of the auxiliary response function $d(x)$ with respect the
perturbations in the constraints through the state solution.  This is easy to
see by simply considering the case where $g(x,v,t) = g(x)$, $h(x) = h(x)$, and
$f(\dot{x},x,v,t) = f(\dot{x},x,t) - v$ which, when substituted into
(\ref{eqn:sens:d_d_hat_d_v_t-final}), gives
%
\begin{equation}
\Jac{\hat{d}}{v(t)} = \Lambda(t)^T , \; t\in[t_0,t_f],
\label{eqn:sens:adjoint_interpretation}
\end{equation}
%
where $v(t)\in\mathcal{F}$ are the perturbations in the state constraints
$f(\dot{x},x,t)$.

The knowledge that this selection for the adjoint $\Lambda$ is the sensitivity
of a functional of interest $d(x)$ with respect to the changes in the
constraints makes the adjoint a useful quantity in and of itself.  This is why
the adjoint is useful in more contexts other than just computing reduced
derivatives.  Examples of other areas where adjoints are useful are
sensitivity analysis and error estimation
{}\cite{SensitivityTechnologiesSandReport}

Note that the sign given the adjoint in the choice for the adjoint equation
(\ref{eqn:sens:adj-trans}) determines the interpretation for the adjoint in
this analysis.  If the opposite sign for the adjoint would have been chosen,
then we must interpret the perturbation to the state equation as
$f(\dot{x},x,v,t) = f(\dot{x},x,t) + v$.  The only issue here is whether
adding a constant to a state equation will increase or decrease the reduced
response function $\hat{d}(p)$.  However, since most people just look at the
magnitude of this sensitivity inherent in the adjoint, the sign of the adjoint
again becomes unimportant.


%
\section{The adjoint, the reduced gradient and the augmented adjoint}
%

We now restate the adjoint in (\ref{eqn:sens:adj-trans}) and
(\ref{eqn:sens:adj-trans:fc}), and the reduced sensitivities in
(\ref{eqn:sens:d_d_hat_d_p_final}) and (\ref{eqn:sens:d_d_hat_d_v_t-final}) in
standard column-wise multi-vector form for $\Lambda$ which yields the adjoint
DAE equations
%
\begin{eqnarray}
\frac{d}{dt}\left( \Jac{f}{\dot{x}}^T \Lambda \right)
- \Jac{f}{x}^T \Lambda + \Jac{g}{x}^T
& = & 0, \; t \in \left[ t_0, t_f \right],
\label{eqn:sens:adj} \\
\left.\left( \Jac{f}{\dot{x}}^T \Lambda \right)\right|_{t=t_f}
& = & \left. \Jac{h}{x}^T \right|_{x=x(t_f)},
\label{eqn:sens:adj:fc}
\end{eqnarray}
%
and the reduced gradient expressions
%
\begin{equation}
\Jac{\hat{d}}{p}^T =
\int_{t_0}^{t_f} \left(
    \Jac{g}{p}^T - \Jac{f}{p}^T \Lambda
  \right) dt
  + \left. \Jac{h}{p}^T \right|_{t=t_f}
  +\Jac{x_0}{p}^T \left. \left( \Jac{f}{\dot{x}}^T \Lambda \right) \right|_{t=t_0}.
\label{eqn:sens:d_d_hat_d_p_final-2}
\end{equation}
%
and
%
\begin{equation}
\Jac{\hat{d}}{v(t)}^T =  \Jac{g}{v(t)}^T - \Jac{f}{v(t)}^T \Lambda(t), \; t\in[t_0,t_f]
\label{eqn:sens:d_d_hat_d_v_t-final-2}
\end{equation}
%

As described in {}\cite{adjoint-sens-2003}, for some classes of DAEs
(including some implicit ODEs), the form of the adjoint in
(\ref{eqn:sens:adj})--(\ref{eqn:sens:adj:fc}) may be unstable while the
following {}\textit{augmented adjoint DAE system} (created by defining the
augmented adjoint varaibles $\hat{\Lambda} = \jac{f}{\dot{x}} {}\Lambda$)
%
\begin{eqnarray}
\frac{d}{dt}\left( \hat{\Lambda} \right)
-  \Jac{f}{x}^T \Lambda + \Jac{g}{x}^T
& = & 0, \; t \in \left[ t_0, t_f \right],
\label{eqn:sens:aug-adj-de} \\
\hat{\Lambda} - \Jac{f}{\dot{x}}^T \Lambda
& = & 0, \; t \in \left[ t_0, t_f \right],
\label{eqn:sens:aug-adj-ae} \\
\left. \hat{\Lambda} \right|_{t=t_f}
& = & \left. \Jac{h}{x}^T \right|_{x=x(t_f)},
\label{eqn:sens:aug-adj:fc}
\end{eqnarray}
%
generally is stable when the forward DAE (up to index 2) is stable.  In
addition, when considering time-stepping algorithms for state and adjoint
equations, only the augmented adjoint DAE is guaranteed to be stable for the
same time-step used by the forward DAE {}\cite{adjoint-sens-2003}.  Therefore,
the augmented adjoint DAE system is to be preferred in a software
implementation and only imparts a small additional space/time cost.


% ---------------------------------------------------------------------- %
% References
%
\clearpage
\bibliographystyle{plain}
\bibliography{references}
\addcontentsline{toc}{section}{References}


\begin{SANDdistribution}[NM]
% \SANDdistCRADA	% If this report is about CRADA work
% \SANDdistPatent	% If this report has a Patent Caution or Patent Interest
% \SANDdistLDRD	% If this report is about LDRD work
% External Address Format: {num copies}{Address}
%\SANDdistExternal{}{}
%\bigskip
%% The following MUST BE between the external and internal distributions!
%\SANDdistClassified % If this report is classified
% Internal Address Format: {num copies}{Mail stop}{Name}{Org}
%\SANDdistInternal{}{}{}{}
% Mail Channel Address Format: {num copies}{Mail Channel}{Name}{Org}
%\SANDdistInternalM{}{}{}{}
%\SANDdistInternal{2}{MS 9018}{Central Technical Files}{8944}
%\SANDdistInternal{2}{MS 0899}{Technical Library}{4536}
\end{SANDdistribution}

\end{document}

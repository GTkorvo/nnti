Greg,

I understand that you are willing to look at important kernels for TFLOPS.  I have developed
a test set for some of the most important kernels used in Aztec and hope that you will have
a chance to look at it.  Specifically, I wrote a test driver and a new set of routines for
the variable block row matrix-matrix multipy (VBRMM) kernel.

The performance of the kernel depends largely on how well DGEMM performs, but also should
probably be written to handle a few special cases where the block sizes are too small to be 
handle by calls to DGEMM, although you might be able to make DGEMM fast enough that this is
not an issue.

In writing the test routine, I have set it up to call a close approximation to the new 
standard for sparse BLAS.  What is different about the new sparse BLAS from the stuff 
currently used by Aztec is that we now have a set up routine that preprocesses the user's 
matrix and creates a handle for the matrix.  Subsequent calls to a sparse BLAS kernel use 
this handle.  This is very helpful for performance improvements because we can analyze the 
matrix and do some one-time stuff that would be too expensive if done each time the kernel 
is called.  I have already taken a stab at doing some optimization steps that I think would
be helpful.

I have makefile support for pclinux, Dec Alpha, Sun Ultra Sparc and SGI MIPS and have tested 
on a variety of machines.  Performance using the new VBR is generally better than the current 
way Aztec calls the routine because of the advantage that the set up routine gives us.  I 
can get 100+ MFLOPS/per PE on my linux boxes for block sizes around 20 and 5-6 rhs, which 
promises to give us good overall parallel performance.

As I mentioned, for large block sizes, the performance of the VBR kernel is entirely dependent 
on how well DGEMM performs.  Specifically, it depends on how DGEMM works for matrices that 
have small outer dimensions and a generally larger inner dimension, i.e., if the operation 
is C = A*B with dim(C) = (m,n), dim(A) = (m,k) and dim(B) = (k,n) then

1< m < ndof
1<= n <= nrhs
1< k = 27*ndof

where ndof is the number of degrees of freedom per node and nrhs is the number of rhs 
(in a block iterative method)

A picture (this could be really messy depending on the font used by your mailer):

----------        --------------------------------------       --------
|    C   |   =   |                        A             |  *   |   B  |
----------        --------------------------------------       |      |
                                                               |      |
                                                               |      |
                                                               |      |
                                                               |      |
                                                               |      |
                                                               --------

It has been my experience that most DGEMM implementations do not perform optimally for 
these shapes of matrix multiplies.  In fact, I have seen that the reference implementation,
which only uses square block entries, i.e., k = m = ndof, often works much better at larger
block sizes.  In principle this should never happen.

In addition to a test routine, I have set up a run script that tests all the relevant
problem dimensions.  It is called tvbrmm.run.

The number of unknowns at a FE node can range from 1 to about 64 at the higher end. Specific 
sizes of interest are listed below.


unknowns                size
---------------------------------------
R,I                     2  - Real/imag parts for real-equivalent form of complex valued 
                             linear systems.
U,V,P                   3  - 2D fluids
U,V,W,P                 4  - 3D fluids
U,V,W,P,T               5  - 3D fluids with heat transfer
U,V,W,P,T,Yi            8  - 3D fluids with heat transfer, 3 species
                        12 - - 3D fluids with heat transfer, 7 species
                        16 - 3D fluids with heat transfer, 11 species
                        20 - 3D fluids with heat transfer, 15 species
                        24
                        32
                        36
                         .
                         .
                        64


I have attached a gzip'ed compressed tar file that contains the entire directory.  Please
contact me if you have questions or comments.

Mike

(320) 845-7695
(320) 845-7846
mheroux@cs.sandia.gov
